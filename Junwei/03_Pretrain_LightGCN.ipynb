{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wcks13589/.local/lib/python3.6/site-packages/numba/core/errors.py:154: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "\n",
    "from time import time\n",
    "from LightGCN import LightGCN\n",
    "from utils import make_edges_symmetry, column_idx, train_test_split_edges\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "shop_col = 'stonc_6_label'\n",
    "#shop_col = 'mcc'\n",
    "#shop_col = 'stonc_label'\n",
    "#shop_col = 'stonc_10_label'\n",
    "\n",
    "load_edges = True\n",
    "edges_path = './edges_stonc6.pkl'\n",
    "pretrain_weights = './weights/LightGCNencoder_stonc6'\n",
    "\n",
    "epochs = 400\n",
    "batch_size = 2048\n",
    "learning_rate = 0.001\n",
    "\n",
    "embedding_size = 64\n",
    "n_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_path = './data/sample'\n",
    "\n",
    "chid_dict_file_name = 'sample_50k_idx_map.npy'\n",
    "cdtx_file_name = 'sample_50k_cdtx.csv'\n",
    "cust_file_name = 'sample_50k_cust.csv'\n",
    "\n",
    "sample_chid_dict = os.path.join(sample_data_path, chid_dict_file_name)\n",
    "sample_cdtx_file = os.path.join(sample_data_path, cdtx_file_name)\n",
    "sample_cust_file = os.path.join(sample_data_path, cust_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx = pd.read_csv(sample_cdtx_file)\n",
    "df_cdtx.sort_values('csmdt')\n",
    "\n",
    "df_cust = pd.read_csv(sample_cust_file)\n",
    "df_cust.drop_duplicates(ignore_index=True, inplace=True)\n",
    "\n",
    "idx_map = np.load(sample_chid_dict, allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(idx_map)\n",
    "for i , j in enumerate(sorted(df_cdtx[shop_col].unique())):\n",
    "    idx_map[j] = i+l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx.chid = df_cdtx.chid.map(idx_map)\n",
    "df_cdtx[shop_col] = df_cdtx[shop_col].map(idx_map)\n",
    "\n",
    "df_cust.chid = df_cust.chid.map(idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx.csmdt = df_cdtx.csmdt.apply(lambda x: x[:8]+'01')\n",
    "df_cdtx.objam = df_cdtx.objam.apply(lambda x: int(x))\n",
    "\n",
    "df_cust.data_dt = df_cust.data_dt.apply(lambda x: x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_cols = ['chid', 'data_dt']\n",
    "category_cols = ['masts', 'educd', 'naty', 'trdtp', 'poscd', 'cuorg']\n",
    "numeric_cols = sorted(set(df_cust.columns) - set(category_cols) - set(ignore_cols)) + ['objam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {col: {value: index for index, value in enumerate(sorted(df_cust[col].unique()))} \n",
    "          for col in category_cols}\n",
    "\n",
    "df_cust[category_cols] = df_cust[category_cols].apply(lambda x: x.map(mapper[x.name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx = df_cdtx[df_cdtx.csmdt < '2019-01-01']\n",
    "df_cust = df_cust[df_cust.data_dt == '2018-12-01'].sort_values(by=['chid'])\n",
    "\n",
    "df_cust['objam'] = np.ma.log(df_cdtx.groupby(['chid']).sum().objam.values/12).filled(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_pairs = df_cdtx[['chid', shop_col]].copy()\n",
    "edge_pairs.drop_duplicates(ignore_index=True, inplace=True)\n",
    "edge_pairs = edge_pairs.to_numpy().T\n",
    "\n",
    "edge_pairs = make_edges_symmetry(edge_pairs)\n",
    "edge_pairs = torch.LongTensor(edge_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler()\n",
    "df_cust[numeric_cols] = x_scaler.fit_transform(df_cust[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_ = df_cust[category_cols+numeric_cols]\n",
    "\n",
    "cust_feature = torch.Tensor(df_cust_.to_numpy())\n",
    "shop_feature = torch.zeros(len(idx_map)-cust_feature.shape[0], cust_feature.shape[1])\n",
    "x_feature = torch.cat([cust_feature, shop_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_edges:\n",
    "    with open(edges_path, 'rb') as f:\n",
    "        import pickle\n",
    "        data = pickle.load(f)\n",
    "else:\n",
    "    data = Data(x=x_feature, edge_index=edge_pairs)\n",
    "    data = train_test_split_edges(data, cust_feature.shape[0])\n",
    "    with open(edges_path, 'wb') as output:\n",
    "        import pickle\n",
    "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dims = {col_name : len(uni)\n",
    "                 for col_name, uni in mapper.items()}\n",
    "\n",
    "category_dict = column_idx(df_cust_, category_cols)\n",
    "numeric_dict = column_idx(df_cust_, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(data.train_pos_edge_index[0],\n",
    "                              data.train_pos_edge_index[1],\n",
    "                              data.train_pos_edge_weight.view(-1,1))\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset()\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(category_dict)*embedding_size + len(numeric_dict)\n",
    "\n",
    "layer_dims = [input_dim, 256, 128, 1]\n",
    "\n",
    "n_users = cust_feature.shape[0]\n",
    "n_shops = shop_feature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wcks13589/玉山銀行/20210215/LightGCN.py:53: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -0.5).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costing 223.4960036277771s, saved norm_mat...\n",
      "don't split the matrix\n"
     ]
    }
   ],
   "source": [
    "model = GAE(LightGCN(embedding_size, n_users, n_shops, n_layers, data.train_pos_edge_index[:,:int(data.train_pos_edge_index.shape[1]/2)])).to(device)\n",
    "train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(train_pos_edge_index)\n",
    "        \n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss:1.3168, AUC: 0.5039, AP: 0.5145\n",
      "Epoch: 002, Train Loss:1.3161, AUC: 0.5046, AP: 0.5154\n",
      "Epoch: 003, Train Loss:1.3151, AUC: 0.5053, AP: 0.5164\n",
      "Epoch: 004, Train Loss:1.3136, AUC: 0.5061, AP: 0.5175\n",
      "Epoch: 005, Train Loss:1.3129, AUC: 0.5070, AP: 0.5186\n",
      "Epoch: 006, Train Loss:1.3118, AUC: 0.5080, AP: 0.5199\n",
      "Epoch: 007, Train Loss:1.3108, AUC: 0.5090, AP: 0.5213\n",
      "Epoch: 008, Train Loss:1.3098, AUC: 0.5101, AP: 0.5229\n",
      "Epoch: 009, Train Loss:1.3084, AUC: 0.5113, AP: 0.5246\n",
      "Epoch: 010, Train Loss:1.3070, AUC: 0.5127, AP: 0.5264\n",
      "Epoch: 011, Train Loss:1.3058, AUC: 0.5141, AP: 0.5284\n",
      "Epoch: 012, Train Loss:1.3041, AUC: 0.5156, AP: 0.5305\n",
      "Epoch: 013, Train Loss:1.3023, AUC: 0.5172, AP: 0.5328\n",
      "Epoch: 014, Train Loss:1.3008, AUC: 0.5189, AP: 0.5352\n",
      "Epoch: 015, Train Loss:1.2985, AUC: 0.5207, AP: 0.5378\n",
      "Epoch: 016, Train Loss:1.2970, AUC: 0.5226, AP: 0.5406\n",
      "Epoch: 017, Train Loss:1.2947, AUC: 0.5247, AP: 0.5435\n",
      "Epoch: 018, Train Loss:1.2930, AUC: 0.5268, AP: 0.5467\n",
      "Epoch: 019, Train Loss:1.2904, AUC: 0.5291, AP: 0.5499\n",
      "Epoch: 020, Train Loss:1.2884, AUC: 0.5315, AP: 0.5534\n",
      "Epoch: 021, Train Loss:1.2854, AUC: 0.5340, AP: 0.5569\n",
      "Epoch: 022, Train Loss:1.2837, AUC: 0.5366, AP: 0.5606\n",
      "Epoch: 023, Train Loss:1.2809, AUC: 0.5393, AP: 0.5645\n",
      "Epoch: 024, Train Loss:1.2780, AUC: 0.5421, AP: 0.5685\n",
      "Epoch: 025, Train Loss:1.2750, AUC: 0.5451, AP: 0.5726\n",
      "Epoch: 026, Train Loss:1.2720, AUC: 0.5481, AP: 0.5769\n",
      "Epoch: 027, Train Loss:1.2683, AUC: 0.5513, AP: 0.5812\n",
      "Epoch: 028, Train Loss:1.2649, AUC: 0.5546, AP: 0.5857\n",
      "Epoch: 029, Train Loss:1.2625, AUC: 0.5580, AP: 0.5902\n",
      "Epoch: 030, Train Loss:1.2584, AUC: 0.5614, AP: 0.5949\n",
      "Epoch: 031, Train Loss:1.2546, AUC: 0.5650, AP: 0.5996\n",
      "Epoch: 032, Train Loss:1.2514, AUC: 0.5687, AP: 0.6043\n",
      "Epoch: 033, Train Loss:1.2469, AUC: 0.5724, AP: 0.6092\n",
      "Epoch: 034, Train Loss:1.2435, AUC: 0.5763, AP: 0.6141\n",
      "Epoch: 035, Train Loss:1.2395, AUC: 0.5802, AP: 0.6190\n",
      "Epoch: 036, Train Loss:1.2348, AUC: 0.5843, AP: 0.6240\n",
      "Epoch: 037, Train Loss:1.2307, AUC: 0.5883, AP: 0.6290\n",
      "Epoch: 038, Train Loss:1.2263, AUC: 0.5925, AP: 0.6341\n",
      "Epoch: 039, Train Loss:1.2223, AUC: 0.5967, AP: 0.6392\n",
      "Epoch: 040, Train Loss:1.2179, AUC: 0.6010, AP: 0.6443\n",
      "Epoch: 041, Train Loss:1.2131, AUC: 0.6053, AP: 0.6494\n",
      "Epoch: 042, Train Loss:1.2080, AUC: 0.6097, AP: 0.6545\n",
      "Epoch: 043, Train Loss:1.2038, AUC: 0.6141, AP: 0.6596\n",
      "Epoch: 044, Train Loss:1.1992, AUC: 0.6185, AP: 0.6647\n",
      "Epoch: 045, Train Loss:1.1945, AUC: 0.6229, AP: 0.6697\n",
      "Epoch: 046, Train Loss:1.1892, AUC: 0.6274, AP: 0.6748\n",
      "Epoch: 047, Train Loss:1.1847, AUC: 0.6319, AP: 0.6798\n",
      "Epoch: 048, Train Loss:1.1793, AUC: 0.6364, AP: 0.6847\n",
      "Epoch: 049, Train Loss:1.1748, AUC: 0.6408, AP: 0.6896\n",
      "Epoch: 050, Train Loss:1.1702, AUC: 0.6453, AP: 0.6945\n",
      "Epoch: 051, Train Loss:1.1654, AUC: 0.6498, AP: 0.6993\n",
      "Epoch: 052, Train Loss:1.1603, AUC: 0.6542, AP: 0.7041\n",
      "Epoch: 053, Train Loss:1.1550, AUC: 0.6587, AP: 0.7088\n",
      "Epoch: 054, Train Loss:1.1503, AUC: 0.6631, AP: 0.7134\n",
      "Epoch: 055, Train Loss:1.1458, AUC: 0.6675, AP: 0.7180\n",
      "Epoch: 056, Train Loss:1.1416, AUC: 0.6718, AP: 0.7224\n",
      "Epoch: 057, Train Loss:1.1365, AUC: 0.6761, AP: 0.7269\n",
      "Epoch: 058, Train Loss:1.1314, AUC: 0.6803, AP: 0.7312\n",
      "Epoch: 059, Train Loss:1.1273, AUC: 0.6845, AP: 0.7355\n",
      "Epoch: 060, Train Loss:1.1224, AUC: 0.6887, AP: 0.7396\n",
      "Epoch: 061, Train Loss:1.1183, AUC: 0.6928, AP: 0.7437\n",
      "Epoch: 062, Train Loss:1.1135, AUC: 0.6968, AP: 0.7477\n",
      "Epoch: 063, Train Loss:1.1098, AUC: 0.7007, AP: 0.7516\n",
      "Epoch: 064, Train Loss:1.1052, AUC: 0.7046, AP: 0.7555\n",
      "Epoch: 065, Train Loss:1.1009, AUC: 0.7085, AP: 0.7592\n",
      "Epoch: 066, Train Loss:1.0966, AUC: 0.7122, AP: 0.7629\n",
      "Epoch: 067, Train Loss:1.0929, AUC: 0.7159, AP: 0.7665\n",
      "Epoch: 068, Train Loss:1.0886, AUC: 0.7195, AP: 0.7699\n",
      "Epoch: 069, Train Loss:1.0854, AUC: 0.7231, AP: 0.7733\n",
      "Epoch: 070, Train Loss:1.0811, AUC: 0.7266, AP: 0.7766\n",
      "Epoch: 071, Train Loss:1.0776, AUC: 0.7300, AP: 0.7799\n",
      "Epoch: 072, Train Loss:1.0739, AUC: 0.7333, AP: 0.7830\n",
      "Epoch: 073, Train Loss:1.0706, AUC: 0.7365, AP: 0.7860\n",
      "Epoch: 074, Train Loss:1.0667, AUC: 0.7397, AP: 0.7890\n",
      "Epoch: 075, Train Loss:1.0635, AUC: 0.7428, AP: 0.7919\n",
      "Epoch: 076, Train Loss:1.0603, AUC: 0.7459, AP: 0.7947\n",
      "Epoch: 077, Train Loss:1.0570, AUC: 0.7488, AP: 0.7974\n",
      "Epoch: 078, Train Loss:1.0544, AUC: 0.7517, AP: 0.8001\n",
      "Epoch: 079, Train Loss:1.0508, AUC: 0.7545, AP: 0.8026\n",
      "Epoch: 080, Train Loss:1.0484, AUC: 0.7573, AP: 0.8051\n",
      "Epoch: 081, Train Loss:1.0459, AUC: 0.7600, AP: 0.8076\n",
      "Epoch: 082, Train Loss:1.0428, AUC: 0.7626, AP: 0.8099\n",
      "Epoch: 083, Train Loss:1.0401, AUC: 0.7652, AP: 0.8122\n",
      "Epoch: 084, Train Loss:1.0369, AUC: 0.7677, AP: 0.8144\n",
      "Epoch: 085, Train Loss:1.0353, AUC: 0.7701, AP: 0.8166\n",
      "Epoch: 086, Train Loss:1.0326, AUC: 0.7725, AP: 0.8187\n",
      "Epoch: 087, Train Loss:1.0308, AUC: 0.7748, AP: 0.8207\n",
      "Epoch: 088, Train Loss:1.0280, AUC: 0.7770, AP: 0.8227\n",
      "Epoch: 089, Train Loss:1.0257, AUC: 0.7792, AP: 0.8246\n",
      "Epoch: 090, Train Loss:1.0237, AUC: 0.7813, AP: 0.8265\n",
      "Epoch: 091, Train Loss:1.0212, AUC: 0.7834, AP: 0.8283\n",
      "Epoch: 092, Train Loss:1.0190, AUC: 0.7854, AP: 0.8301\n",
      "Epoch: 093, Train Loss:1.0176, AUC: 0.7874, AP: 0.8318\n",
      "Epoch: 094, Train Loss:1.0155, AUC: 0.7893, AP: 0.8334\n",
      "Epoch: 095, Train Loss:1.0138, AUC: 0.7911, AP: 0.8350\n",
      "Epoch: 096, Train Loss:1.0118, AUC: 0.7929, AP: 0.8366\n",
      "Epoch: 097, Train Loss:1.0107, AUC: 0.7947, AP: 0.8381\n",
      "Epoch: 098, Train Loss:1.0088, AUC: 0.7964, AP: 0.8396\n",
      "Epoch: 099, Train Loss:1.0068, AUC: 0.7981, AP: 0.8410\n",
      "Epoch: 100, Train Loss:1.0053, AUC: 0.7997, AP: 0.8424\n",
      "Epoch: 101, Train Loss:1.0038, AUC: 0.8012, AP: 0.8437\n",
      "Epoch: 102, Train Loss:1.0027, AUC: 0.8028, AP: 0.8450\n",
      "Epoch: 103, Train Loss:1.0013, AUC: 0.8042, AP: 0.8463\n",
      "Epoch: 104, Train Loss:0.9996, AUC: 0.8057, AP: 0.8475\n",
      "Epoch: 105, Train Loss:0.9982, AUC: 0.8071, AP: 0.8487\n",
      "Epoch: 106, Train Loss:0.9969, AUC: 0.8085, AP: 0.8498\n",
      "Epoch: 107, Train Loss:0.9960, AUC: 0.8098, AP: 0.8510\n",
      "Epoch: 108, Train Loss:0.9944, AUC: 0.8111, AP: 0.8521\n",
      "Epoch: 109, Train Loss:0.9927, AUC: 0.8123, AP: 0.8531\n",
      "Epoch: 110, Train Loss:0.9918, AUC: 0.8135, AP: 0.8541\n",
      "Epoch: 111, Train Loss:0.9916, AUC: 0.8147, AP: 0.8551\n",
      "Epoch: 112, Train Loss:0.9899, AUC: 0.8159, AP: 0.8561\n",
      "Epoch: 113, Train Loss:0.9885, AUC: 0.8170, AP: 0.8571\n",
      "Epoch: 114, Train Loss:0.9878, AUC: 0.8181, AP: 0.8580\n",
      "Epoch: 115, Train Loss:0.9868, AUC: 0.8191, AP: 0.8589\n",
      "Epoch: 116, Train Loss:0.9857, AUC: 0.8202, AP: 0.8597\n",
      "Epoch: 117, Train Loss:0.9847, AUC: 0.8212, AP: 0.8606\n",
      "Epoch: 118, Train Loss:0.9840, AUC: 0.8221, AP: 0.8614\n",
      "Epoch: 119, Train Loss:0.9826, AUC: 0.8231, AP: 0.8622\n",
      "Epoch: 120, Train Loss:0.9818, AUC: 0.8240, AP: 0.8629\n",
      "Epoch: 121, Train Loss:0.9810, AUC: 0.8249, AP: 0.8637\n",
      "Epoch: 122, Train Loss:0.9804, AUC: 0.8258, AP: 0.8644\n",
      "Epoch: 123, Train Loss:0.9797, AUC: 0.8266, AP: 0.8651\n",
      "Epoch: 124, Train Loss:0.9787, AUC: 0.8275, AP: 0.8658\n",
      "Epoch: 125, Train Loss:0.9778, AUC: 0.8283, AP: 0.8665\n",
      "Epoch: 126, Train Loss:0.9772, AUC: 0.8291, AP: 0.8672\n",
      "Epoch: 127, Train Loss:0.9758, AUC: 0.8298, AP: 0.8678\n",
      "Epoch: 128, Train Loss:0.9753, AUC: 0.8306, AP: 0.8684\n",
      "Epoch: 129, Train Loss:0.9744, AUC: 0.8313, AP: 0.8690\n",
      "Epoch: 130, Train Loss:0.9732, AUC: 0.8320, AP: 0.8696\n",
      "Epoch: 131, Train Loss:0.9732, AUC: 0.8327, AP: 0.8702\n",
      "Epoch: 132, Train Loss:0.9725, AUC: 0.8334, AP: 0.8708\n",
      "Epoch: 133, Train Loss:0.9718, AUC: 0.8340, AP: 0.8713\n",
      "Epoch: 134, Train Loss:0.9714, AUC: 0.8347, AP: 0.8718\n",
      "Epoch: 135, Train Loss:0.9708, AUC: 0.8353, AP: 0.8724\n",
      "Epoch: 136, Train Loss:0.9692, AUC: 0.8359, AP: 0.8729\n",
      "Epoch: 137, Train Loss:0.9685, AUC: 0.8365, AP: 0.8734\n",
      "Epoch: 138, Train Loss:0.9684, AUC: 0.8371, AP: 0.8738\n",
      "Epoch: 139, Train Loss:0.9682, AUC: 0.8377, AP: 0.8743\n",
      "Epoch: 140, Train Loss:0.9674, AUC: 0.8382, AP: 0.8748\n",
      "Epoch: 141, Train Loss:0.9663, AUC: 0.8388, AP: 0.8752\n",
      "Epoch: 142, Train Loss:0.9657, AUC: 0.8393, AP: 0.8756\n",
      "Epoch: 143, Train Loss:0.9654, AUC: 0.8398, AP: 0.8761\n",
      "Epoch: 144, Train Loss:0.9644, AUC: 0.8403, AP: 0.8765\n",
      "Epoch: 145, Train Loss:0.9642, AUC: 0.8408, AP: 0.8769\n",
      "Epoch: 146, Train Loss:0.9635, AUC: 0.8413, AP: 0.8773\n",
      "Epoch: 147, Train Loss:0.9633, AUC: 0.8417, AP: 0.8777\n",
      "Epoch: 148, Train Loss:0.9619, AUC: 0.8422, AP: 0.8781\n",
      "Epoch: 149, Train Loss:0.9617, AUC: 0.8426, AP: 0.8784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Train Loss:0.9608, AUC: 0.8431, AP: 0.8788\n",
      "Epoch: 151, Train Loss:0.9604, AUC: 0.8435, AP: 0.8791\n",
      "Epoch: 152, Train Loss:0.9603, AUC: 0.8439, AP: 0.8795\n",
      "Epoch: 153, Train Loss:0.9601, AUC: 0.8443, AP: 0.8798\n",
      "Epoch: 154, Train Loss:0.9595, AUC: 0.8447, AP: 0.8802\n",
      "Epoch: 155, Train Loss:0.9593, AUC: 0.8451, AP: 0.8805\n",
      "Epoch: 156, Train Loss:0.9582, AUC: 0.8455, AP: 0.8808\n",
      "Epoch: 157, Train Loss:0.9580, AUC: 0.8459, AP: 0.8811\n",
      "Epoch: 158, Train Loss:0.9571, AUC: 0.8462, AP: 0.8814\n",
      "Epoch: 159, Train Loss:0.9574, AUC: 0.8466, AP: 0.8817\n",
      "Epoch: 160, Train Loss:0.9564, AUC: 0.8469, AP: 0.8820\n",
      "Epoch: 161, Train Loss:0.9556, AUC: 0.8473, AP: 0.8823\n",
      "Epoch: 162, Train Loss:0.9552, AUC: 0.8476, AP: 0.8826\n",
      "Epoch: 163, Train Loss:0.9546, AUC: 0.8479, AP: 0.8828\n",
      "Epoch: 164, Train Loss:0.9543, AUC: 0.8483, AP: 0.8831\n",
      "Epoch: 165, Train Loss:0.9540, AUC: 0.8486, AP: 0.8834\n",
      "Epoch: 166, Train Loss:0.9534, AUC: 0.8489, AP: 0.8836\n",
      "Epoch: 167, Train Loss:0.9526, AUC: 0.8492, AP: 0.8839\n",
      "Epoch: 168, Train Loss:0.9529, AUC: 0.8495, AP: 0.8841\n",
      "Epoch: 169, Train Loss:0.9519, AUC: 0.8498, AP: 0.8844\n",
      "Epoch: 170, Train Loss:0.9510, AUC: 0.8500, AP: 0.8846\n",
      "Epoch: 171, Train Loss:0.9508, AUC: 0.8503, AP: 0.8848\n",
      "Epoch: 172, Train Loss:0.9510, AUC: 0.8506, AP: 0.8850\n",
      "Epoch: 173, Train Loss:0.9502, AUC: 0.8508, AP: 0.8853\n",
      "Epoch: 174, Train Loss:0.9495, AUC: 0.8511, AP: 0.8855\n",
      "Epoch: 175, Train Loss:0.9493, AUC: 0.8514, AP: 0.8857\n",
      "Epoch: 176, Train Loss:0.9494, AUC: 0.8516, AP: 0.8859\n",
      "Epoch: 177, Train Loss:0.9493, AUC: 0.8519, AP: 0.8861\n",
      "Epoch: 178, Train Loss:0.9477, AUC: 0.8521, AP: 0.8863\n",
      "Epoch: 179, Train Loss:0.9481, AUC: 0.8523, AP: 0.8865\n",
      "Epoch: 180, Train Loss:0.9469, AUC: 0.8526, AP: 0.8867\n",
      "Epoch: 181, Train Loss:0.9464, AUC: 0.8528, AP: 0.8869\n",
      "Epoch: 182, Train Loss:0.9464, AUC: 0.8530, AP: 0.8871\n",
      "Epoch: 183, Train Loss:0.9465, AUC: 0.8532, AP: 0.8873\n",
      "Epoch: 184, Train Loss:0.9448, AUC: 0.8534, AP: 0.8875\n",
      "Epoch: 185, Train Loss:0.9457, AUC: 0.8536, AP: 0.8876\n",
      "Epoch: 186, Train Loss:0.9451, AUC: 0.8538, AP: 0.8878\n",
      "Epoch: 187, Train Loss:0.9445, AUC: 0.8540, AP: 0.8880\n",
      "Epoch: 188, Train Loss:0.9442, AUC: 0.8542, AP: 0.8882\n",
      "Epoch: 189, Train Loss:0.9436, AUC: 0.8544, AP: 0.8883\n",
      "Epoch: 190, Train Loss:0.9430, AUC: 0.8546, AP: 0.8885\n",
      "Epoch: 191, Train Loss:0.9431, AUC: 0.8548, AP: 0.8887\n",
      "Epoch: 192, Train Loss:0.9423, AUC: 0.8550, AP: 0.8888\n",
      "Epoch: 193, Train Loss:0.9413, AUC: 0.8552, AP: 0.8890\n",
      "Epoch: 194, Train Loss:0.9415, AUC: 0.8554, AP: 0.8891\n",
      "Epoch: 195, Train Loss:0.9412, AUC: 0.8555, AP: 0.8893\n",
      "Epoch: 196, Train Loss:0.9408, AUC: 0.8557, AP: 0.8894\n",
      "Epoch: 197, Train Loss:0.9397, AUC: 0.8559, AP: 0.8896\n",
      "Epoch: 198, Train Loss:0.9392, AUC: 0.8561, AP: 0.8897\n",
      "Epoch: 199, Train Loss:0.9393, AUC: 0.8562, AP: 0.8899\n",
      "Epoch: 200, Train Loss:0.9392, AUC: 0.8564, AP: 0.8900\n",
      "Epoch: 201, Train Loss:0.9388, AUC: 0.8565, AP: 0.8902\n",
      "Epoch: 202, Train Loss:0.9387, AUC: 0.8567, AP: 0.8903\n",
      "Epoch: 203, Train Loss:0.9383, AUC: 0.8569, AP: 0.8904\n",
      "Epoch: 204, Train Loss:0.9380, AUC: 0.8570, AP: 0.8906\n",
      "Epoch: 205, Train Loss:0.9370, AUC: 0.8572, AP: 0.8907\n",
      "Epoch: 206, Train Loss:0.9374, AUC: 0.8573, AP: 0.8908\n",
      "Epoch: 207, Train Loss:0.9363, AUC: 0.8575, AP: 0.8910\n",
      "Epoch: 208, Train Loss:0.9351, AUC: 0.8576, AP: 0.8911\n",
      "Epoch: 209, Train Loss:0.9361, AUC: 0.8577, AP: 0.8912\n",
      "Epoch: 210, Train Loss:0.9354, AUC: 0.8579, AP: 0.8913\n",
      "Epoch: 211, Train Loss:0.9345, AUC: 0.8580, AP: 0.8915\n",
      "Epoch: 212, Train Loss:0.9340, AUC: 0.8582, AP: 0.8916\n",
      "Epoch: 213, Train Loss:0.9336, AUC: 0.8583, AP: 0.8917\n",
      "Epoch: 214, Train Loss:0.9347, AUC: 0.8584, AP: 0.8918\n",
      "Epoch: 215, Train Loss:0.9335, AUC: 0.8586, AP: 0.8919\n",
      "Epoch: 216, Train Loss:0.9323, AUC: 0.8587, AP: 0.8921\n",
      "Epoch: 217, Train Loss:0.9323, AUC: 0.8588, AP: 0.8922\n",
      "Epoch: 218, Train Loss:0.9326, AUC: 0.8589, AP: 0.8923\n",
      "Epoch: 219, Train Loss:0.9319, AUC: 0.8591, AP: 0.8924\n",
      "Epoch: 220, Train Loss:0.9315, AUC: 0.8592, AP: 0.8925\n",
      "Epoch: 221, Train Loss:0.9311, AUC: 0.8593, AP: 0.8926\n",
      "Epoch: 222, Train Loss:0.9308, AUC: 0.8594, AP: 0.8927\n",
      "Epoch: 223, Train Loss:0.9307, AUC: 0.8595, AP: 0.8928\n",
      "Epoch: 224, Train Loss:0.9304, AUC: 0.8597, AP: 0.8929\n",
      "Epoch: 225, Train Loss:0.9305, AUC: 0.8598, AP: 0.8930\n",
      "Epoch: 226, Train Loss:0.9294, AUC: 0.8599, AP: 0.8931\n",
      "Epoch: 227, Train Loss:0.9291, AUC: 0.8600, AP: 0.8932\n",
      "Epoch: 228, Train Loss:0.9289, AUC: 0.8601, AP: 0.8933\n",
      "Epoch: 229, Train Loss:0.9282, AUC: 0.8602, AP: 0.8934\n",
      "Epoch: 230, Train Loss:0.9290, AUC: 0.8603, AP: 0.8935\n",
      "Epoch: 231, Train Loss:0.9275, AUC: 0.8604, AP: 0.8936\n",
      "Epoch: 232, Train Loss:0.9278, AUC: 0.8605, AP: 0.8937\n",
      "Epoch: 233, Train Loss:0.9275, AUC: 0.8606, AP: 0.8938\n",
      "Epoch: 234, Train Loss:0.9271, AUC: 0.8607, AP: 0.8939\n",
      "Epoch: 235, Train Loss:0.9267, AUC: 0.8608, AP: 0.8940\n",
      "Epoch: 236, Train Loss:0.9261, AUC: 0.8609, AP: 0.8941\n",
      "Epoch: 237, Train Loss:0.9254, AUC: 0.8610, AP: 0.8942\n",
      "Epoch: 238, Train Loss:0.9255, AUC: 0.8611, AP: 0.8943\n",
      "Epoch: 239, Train Loss:0.9253, AUC: 0.8612, AP: 0.8944\n",
      "Epoch: 240, Train Loss:0.9240, AUC: 0.8613, AP: 0.8944\n",
      "Epoch: 241, Train Loss:0.9248, AUC: 0.8614, AP: 0.8945\n",
      "Epoch: 242, Train Loss:0.9243, AUC: 0.8615, AP: 0.8946\n",
      "Epoch: 243, Train Loss:0.9242, AUC: 0.8616, AP: 0.8947\n",
      "Epoch: 244, Train Loss:0.9238, AUC: 0.8617, AP: 0.8948\n",
      "Epoch: 245, Train Loss:0.9236, AUC: 0.8618, AP: 0.8949\n",
      "Epoch: 246, Train Loss:0.9233, AUC: 0.8619, AP: 0.8950\n",
      "Epoch: 247, Train Loss:0.9227, AUC: 0.8620, AP: 0.8950\n",
      "Epoch: 248, Train Loss:0.9224, AUC: 0.8620, AP: 0.8951\n",
      "Epoch: 249, Train Loss:0.9221, AUC: 0.8621, AP: 0.8952\n",
      "Epoch: 250, Train Loss:0.9219, AUC: 0.8622, AP: 0.8953\n",
      "Epoch: 251, Train Loss:0.9209, AUC: 0.8623, AP: 0.8954\n",
      "Epoch: 252, Train Loss:0.9213, AUC: 0.8624, AP: 0.8954\n",
      "Epoch: 253, Train Loss:0.9207, AUC: 0.8625, AP: 0.8955\n",
      "Epoch: 254, Train Loss:0.9207, AUC: 0.8625, AP: 0.8956\n",
      "Epoch: 255, Train Loss:0.9200, AUC: 0.8626, AP: 0.8957\n",
      "Epoch: 256, Train Loss:0.9209, AUC: 0.8627, AP: 0.8957\n",
      "Epoch: 257, Train Loss:0.9205, AUC: 0.8628, AP: 0.8958\n",
      "Epoch: 258, Train Loss:0.9191, AUC: 0.8629, AP: 0.8959\n",
      "Epoch: 259, Train Loss:0.9197, AUC: 0.8629, AP: 0.8960\n",
      "Epoch: 260, Train Loss:0.9177, AUC: 0.8630, AP: 0.8960\n",
      "Epoch: 261, Train Loss:0.9177, AUC: 0.8631, AP: 0.8961\n",
      "Epoch: 262, Train Loss:0.9178, AUC: 0.8632, AP: 0.8962\n",
      "Epoch: 263, Train Loss:0.9166, AUC: 0.8632, AP: 0.8962\n",
      "Epoch: 264, Train Loss:0.9176, AUC: 0.8633, AP: 0.8963\n",
      "Epoch: 265, Train Loss:0.9175, AUC: 0.8634, AP: 0.8964\n",
      "Epoch: 266, Train Loss:0.9165, AUC: 0.8635, AP: 0.8964\n",
      "Epoch: 267, Train Loss:0.9167, AUC: 0.8635, AP: 0.8965\n",
      "Epoch: 268, Train Loss:0.9157, AUC: 0.8636, AP: 0.8966\n",
      "Epoch: 269, Train Loss:0.9157, AUC: 0.8637, AP: 0.8967\n",
      "Epoch: 270, Train Loss:0.9159, AUC: 0.8637, AP: 0.8967\n",
      "Epoch: 271, Train Loss:0.9161, AUC: 0.8638, AP: 0.8968\n",
      "Epoch: 272, Train Loss:0.9160, AUC: 0.8639, AP: 0.8968\n",
      "Epoch: 273, Train Loss:0.9146, AUC: 0.8639, AP: 0.8969\n",
      "Epoch: 274, Train Loss:0.9146, AUC: 0.8640, AP: 0.8970\n",
      "Epoch: 275, Train Loss:0.9147, AUC: 0.8641, AP: 0.8970\n",
      "Epoch: 276, Train Loss:0.9145, AUC: 0.8641, AP: 0.8971\n",
      "Epoch: 277, Train Loss:0.9138, AUC: 0.8642, AP: 0.8972\n",
      "Epoch: 278, Train Loss:0.9138, AUC: 0.8643, AP: 0.8972\n",
      "Epoch: 279, Train Loss:0.9128, AUC: 0.8643, AP: 0.8973\n",
      "Epoch: 280, Train Loss:0.9132, AUC: 0.8644, AP: 0.8974\n",
      "Epoch: 281, Train Loss:0.9127, AUC: 0.8644, AP: 0.8974\n",
      "Epoch: 282, Train Loss:0.9119, AUC: 0.8645, AP: 0.8975\n",
      "Epoch: 283, Train Loss:0.9119, AUC: 0.8646, AP: 0.8975\n",
      "Epoch: 284, Train Loss:0.9121, AUC: 0.8646, AP: 0.8976\n",
      "Epoch: 285, Train Loss:0.9115, AUC: 0.8647, AP: 0.8977\n",
      "Epoch: 286, Train Loss:0.9118, AUC: 0.8647, AP: 0.8977\n",
      "Epoch: 287, Train Loss:0.9104, AUC: 0.8648, AP: 0.8978\n",
      "Epoch: 288, Train Loss:0.9107, AUC: 0.8649, AP: 0.8978\n",
      "Epoch: 289, Train Loss:0.9105, AUC: 0.8649, AP: 0.8979\n",
      "Epoch: 290, Train Loss:0.9095, AUC: 0.8650, AP: 0.8979\n",
      "Epoch: 291, Train Loss:0.9099, AUC: 0.8650, AP: 0.8980\n",
      "Epoch: 292, Train Loss:0.9092, AUC: 0.8651, AP: 0.8981\n",
      "Epoch: 293, Train Loss:0.9090, AUC: 0.8652, AP: 0.8981\n",
      "Epoch: 294, Train Loss:0.9087, AUC: 0.8652, AP: 0.8982\n",
      "Epoch: 295, Train Loss:0.9083, AUC: 0.8653, AP: 0.8982\n",
      "Epoch: 296, Train Loss:0.9093, AUC: 0.8653, AP: 0.8983\n",
      "Epoch: 297, Train Loss:0.9083, AUC: 0.8654, AP: 0.8983\n",
      "Epoch: 298, Train Loss:0.9081, AUC: 0.8654, AP: 0.8984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299, Train Loss:0.9075, AUC: 0.8655, AP: 0.8985\n",
      "Epoch: 300, Train Loss:0.9068, AUC: 0.8656, AP: 0.8985\n",
      "Epoch: 301, Train Loss:0.9074, AUC: 0.8656, AP: 0.8986\n",
      "Epoch: 302, Train Loss:0.9067, AUC: 0.8657, AP: 0.8986\n",
      "Epoch: 303, Train Loss:0.9062, AUC: 0.8657, AP: 0.8987\n",
      "Epoch: 304, Train Loss:0.9061, AUC: 0.8658, AP: 0.8987\n",
      "Epoch: 305, Train Loss:0.9066, AUC: 0.8658, AP: 0.8988\n",
      "Epoch: 306, Train Loss:0.9057, AUC: 0.8659, AP: 0.8988\n",
      "Epoch: 307, Train Loss:0.9056, AUC: 0.8659, AP: 0.8989\n",
      "Epoch: 308, Train Loss:0.9053, AUC: 0.8660, AP: 0.8989\n",
      "Epoch: 309, Train Loss:0.9048, AUC: 0.8660, AP: 0.8990\n",
      "Epoch: 310, Train Loss:0.9052, AUC: 0.8661, AP: 0.8990\n",
      "Epoch: 311, Train Loss:0.9040, AUC: 0.8661, AP: 0.8991\n",
      "Epoch: 312, Train Loss:0.9048, AUC: 0.8662, AP: 0.8991\n",
      "Epoch: 313, Train Loss:0.9041, AUC: 0.8662, AP: 0.8992\n",
      "Epoch: 314, Train Loss:0.9039, AUC: 0.8663, AP: 0.8992\n",
      "Epoch: 315, Train Loss:0.9036, AUC: 0.8664, AP: 0.8993\n",
      "Epoch: 316, Train Loss:0.9035, AUC: 0.8664, AP: 0.8993\n",
      "Epoch: 317, Train Loss:0.9028, AUC: 0.8665, AP: 0.8994\n",
      "Epoch: 318, Train Loss:0.9023, AUC: 0.8665, AP: 0.8994\n",
      "Epoch: 319, Train Loss:0.9020, AUC: 0.8665, AP: 0.8995\n",
      "Epoch: 320, Train Loss:0.9028, AUC: 0.8666, AP: 0.8995\n",
      "Epoch: 321, Train Loss:0.9026, AUC: 0.8666, AP: 0.8996\n",
      "Epoch: 322, Train Loss:0.9019, AUC: 0.8667, AP: 0.8996\n",
      "Epoch: 323, Train Loss:0.9013, AUC: 0.8667, AP: 0.8997\n",
      "Epoch: 324, Train Loss:0.9017, AUC: 0.8668, AP: 0.8997\n",
      "Epoch: 325, Train Loss:0.9009, AUC: 0.8668, AP: 0.8998\n",
      "Epoch: 326, Train Loss:0.9010, AUC: 0.8669, AP: 0.8998\n",
      "Epoch: 327, Train Loss:0.9002, AUC: 0.8669, AP: 0.8999\n",
      "Epoch: 328, Train Loss:0.9009, AUC: 0.8670, AP: 0.8999\n",
      "Epoch: 329, Train Loss:0.9002, AUC: 0.8670, AP: 0.9000\n",
      "Epoch: 330, Train Loss:0.9002, AUC: 0.8671, AP: 0.9000\n",
      "Epoch: 331, Train Loss:0.8997, AUC: 0.8671, AP: 0.9001\n",
      "Epoch: 332, Train Loss:0.8993, AUC: 0.8671, AP: 0.9001\n",
      "Epoch: 333, Train Loss:0.8999, AUC: 0.8672, AP: 0.9001\n",
      "Epoch: 334, Train Loss:0.8989, AUC: 0.8672, AP: 0.9002\n",
      "Epoch: 335, Train Loss:0.8988, AUC: 0.8673, AP: 0.9002\n",
      "Epoch: 336, Train Loss:0.8988, AUC: 0.8673, AP: 0.9003\n",
      "Epoch: 337, Train Loss:0.8983, AUC: 0.8674, AP: 0.9003\n",
      "Epoch: 338, Train Loss:0.8983, AUC: 0.8674, AP: 0.9004\n",
      "Epoch: 339, Train Loss:0.8983, AUC: 0.8674, AP: 0.9004\n",
      "Epoch: 340, Train Loss:0.8975, AUC: 0.8675, AP: 0.9004\n",
      "Epoch: 341, Train Loss:0.8970, AUC: 0.8675, AP: 0.9005\n",
      "Epoch: 342, Train Loss:0.8975, AUC: 0.8676, AP: 0.9005\n",
      "Epoch: 343, Train Loss:0.8977, AUC: 0.8676, AP: 0.9006\n",
      "Epoch: 344, Train Loss:0.8961, AUC: 0.8677, AP: 0.9006\n",
      "Epoch: 345, Train Loss:0.8959, AUC: 0.8677, AP: 0.9007\n",
      "Epoch: 346, Train Loss:0.8968, AUC: 0.8677, AP: 0.9007\n",
      "Epoch: 347, Train Loss:0.8961, AUC: 0.8678, AP: 0.9007\n",
      "Epoch: 348, Train Loss:0.8957, AUC: 0.8678, AP: 0.9008\n",
      "Epoch: 349, Train Loss:0.8953, AUC: 0.8679, AP: 0.9008\n",
      "Epoch: 350, Train Loss:0.8952, AUC: 0.8679, AP: 0.9009\n",
      "Epoch: 351, Train Loss:0.8951, AUC: 0.8679, AP: 0.9009\n",
      "Epoch: 352, Train Loss:0.8954, AUC: 0.8680, AP: 0.9009\n",
      "Epoch: 353, Train Loss:0.8944, AUC: 0.8680, AP: 0.9010\n",
      "Epoch: 354, Train Loss:0.8952, AUC: 0.8680, AP: 0.9010\n",
      "Epoch: 355, Train Loss:0.8937, AUC: 0.8681, AP: 0.9011\n",
      "Epoch: 356, Train Loss:0.8936, AUC: 0.8681, AP: 0.9011\n",
      "Epoch: 357, Train Loss:0.8943, AUC: 0.8682, AP: 0.9011\n",
      "Epoch: 358, Train Loss:0.8933, AUC: 0.8682, AP: 0.9012\n",
      "Epoch: 359, Train Loss:0.8934, AUC: 0.8682, AP: 0.9012\n",
      "Epoch: 360, Train Loss:0.8931, AUC: 0.8683, AP: 0.9013\n",
      "Epoch: 361, Train Loss:0.8940, AUC: 0.8683, AP: 0.9013\n",
      "Epoch: 362, Train Loss:0.8931, AUC: 0.8684, AP: 0.9013\n",
      "Epoch: 363, Train Loss:0.8924, AUC: 0.8684, AP: 0.9014\n",
      "Epoch: 364, Train Loss:0.8918, AUC: 0.8684, AP: 0.9014\n",
      "Epoch: 365, Train Loss:0.8918, AUC: 0.8685, AP: 0.9014\n",
      "Epoch: 366, Train Loss:0.8918, AUC: 0.8685, AP: 0.9015\n",
      "Epoch: 367, Train Loss:0.8924, AUC: 0.8685, AP: 0.9015\n",
      "Epoch: 368, Train Loss:0.8917, AUC: 0.8686, AP: 0.9016\n",
      "Epoch: 369, Train Loss:0.8905, AUC: 0.8686, AP: 0.9016\n",
      "Epoch: 370, Train Loss:0.8909, AUC: 0.8686, AP: 0.9016\n",
      "Epoch: 371, Train Loss:0.8910, AUC: 0.8687, AP: 0.9017\n",
      "Epoch: 372, Train Loss:0.8905, AUC: 0.8687, AP: 0.9017\n",
      "Epoch: 373, Train Loss:0.8902, AUC: 0.8687, AP: 0.9017\n",
      "Epoch: 374, Train Loss:0.8905, AUC: 0.8688, AP: 0.9018\n",
      "Epoch: 375, Train Loss:0.8897, AUC: 0.8688, AP: 0.9018\n",
      "Epoch: 376, Train Loss:0.8898, AUC: 0.8688, AP: 0.9018\n",
      "Epoch: 377, Train Loss:0.8887, AUC: 0.8689, AP: 0.9019\n",
      "Epoch: 378, Train Loss:0.8897, AUC: 0.8689, AP: 0.9019\n",
      "Epoch: 379, Train Loss:0.8891, AUC: 0.8689, AP: 0.9019\n",
      "Epoch: 380, Train Loss:0.8888, AUC: 0.8690, AP: 0.9020\n",
      "Epoch: 381, Train Loss:0.8884, AUC: 0.8690, AP: 0.9020\n",
      "Epoch: 382, Train Loss:0.8877, AUC: 0.8690, AP: 0.9020\n",
      "Epoch: 383, Train Loss:0.8883, AUC: 0.8691, AP: 0.9021\n",
      "Epoch: 384, Train Loss:0.8874, AUC: 0.8691, AP: 0.9021\n",
      "Epoch: 385, Train Loss:0.8884, AUC: 0.8691, AP: 0.9021\n",
      "Epoch: 386, Train Loss:0.8878, AUC: 0.8692, AP: 0.9022\n",
      "Epoch: 387, Train Loss:0.8878, AUC: 0.8692, AP: 0.9022\n",
      "Epoch: 388, Train Loss:0.8875, AUC: 0.8692, AP: 0.9022\n",
      "Epoch: 389, Train Loss:0.8878, AUC: 0.8693, AP: 0.9023\n",
      "Epoch: 390, Train Loss:0.8869, AUC: 0.8693, AP: 0.9023\n",
      "Epoch: 391, Train Loss:0.8868, AUC: 0.8693, AP: 0.9023\n",
      "Epoch: 392, Train Loss:0.8861, AUC: 0.8694, AP: 0.9024\n",
      "Epoch: 393, Train Loss:0.8862, AUC: 0.8694, AP: 0.9024\n",
      "Epoch: 394, Train Loss:0.8864, AUC: 0.8694, AP: 0.9024\n",
      "Epoch: 395, Train Loss:0.8857, AUC: 0.8695, AP: 0.9025\n",
      "Epoch: 396, Train Loss:0.8855, AUC: 0.8695, AP: 0.9025\n",
      "Epoch: 397, Train Loss:0.8852, AUC: 0.8695, AP: 0.9025\n",
      "Epoch: 398, Train Loss:0.8853, AUC: 0.8696, AP: 0.9026\n",
      "Epoch: 399, Train Loss:0.8850, AUC: 0.8696, AP: 0.9026\n",
      "Epoch: 400, Train Loss:0.8846, AUC: 0.8696, AP: 0.9026\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(400):\n",
    "    loss = train()\n",
    "    \n",
    "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, Train Loss:{:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch+1, loss, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), pretrain_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
