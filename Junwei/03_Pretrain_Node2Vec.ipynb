{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wcks13589/.local/lib/python3.6/site-packages/numba/core/errors.py:154: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from utils import make_edges_symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "shop_col = 'stonc_6_label'\n",
    "#shop_col = 'mcc'\n",
    "##shop_col = 'stonc_label'\n",
    "#shop_col = 'stonc_10_label'\n",
    "\n",
    "epoch = 400\n",
    "batch_size = 512\n",
    "embedding_size = 64\n",
    "learning_rate = 0.01\n",
    "pretrain_weights = './weights/node2vec_weights_stonc6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_path = './data/sample'\n",
    "\n",
    "chid_dict_file_name = 'sample_50k_idx_map.npy'\n",
    "cdtx_file_name = 'sample_50k_cdtx.csv'\n",
    "\n",
    "sample_chid_dict = os.path.join(sample_data_path, chid_dict_file_name)\n",
    "sample_cdtx_file = os.path.join(sample_data_path, cdtx_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx = pd.read_csv(sample_cdtx_file)\n",
    "df_cdtx.sort_values('csmdt')\n",
    "\n",
    "# Load dict\n",
    "idx_map = np.load(sample_chid_dict, allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(idx_map)\n",
    "for i , j in enumerate(sorted(df_cdtx[shop_col].unique())):\n",
    "    idx_map[j] = i+l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx.chid = df_cdtx.chid.map(idx_map)\n",
    "df_cdtx[shop_col] = df_cdtx[shop_col].map(idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx.csmdt = df_cdtx.csmdt.apply(lambda x: x[:8]+'01')\n",
    "df_cdtx = df_cdtx[df_cdtx.csmdt < '2019-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_pairs = df_cdtx[['chid', shop_col]].copy()\n",
    "edge_pairs.drop_duplicates(ignore_index=True, inplace=True)\n",
    "edge_pairs = edge_pairs.to_numpy().T\n",
    "\n",
    "edge_pairs = make_edges_symmetry(edge_pairs)\n",
    "edge_pairs = torch.LongTensor(edge_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Node2Vec(edge_pairs, embedding_dim=embedding_size, walk_length=2,\n",
    "                 context_size=2, walks_per_node=10,\n",
    "                 num_negative_samples=1, p=1, q=1, sparse=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = model.loader(batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 5.8908\n",
      "Epoch: 02, Loss: 4.3920\n",
      "Epoch: 03, Loss: 3.6607\n",
      "Epoch: 04, Loss: 3.3022\n",
      "Epoch: 05, Loss: 3.0457\n",
      "Epoch: 06, Loss: 2.8363\n",
      "Epoch: 07, Loss: 2.6629\n",
      "Epoch: 08, Loss: 2.5185\n",
      "Epoch: 09, Loss: 2.3903\n",
      "Epoch: 10, Loss: 2.2657\n",
      "Epoch: 11, Loss: 2.1680\n",
      "Epoch: 12, Loss: 2.0686\n",
      "Epoch: 13, Loss: 1.9859\n",
      "Epoch: 14, Loss: 1.9048\n",
      "Epoch: 15, Loss: 1.8303\n",
      "Epoch: 16, Loss: 1.7587\n",
      "Epoch: 17, Loss: 1.6936\n",
      "Epoch: 18, Loss: 1.6375\n",
      "Epoch: 19, Loss: 1.5802\n",
      "Epoch: 20, Loss: 1.5277\n",
      "Epoch: 21, Loss: 1.4790\n",
      "Epoch: 22, Loss: 1.4344\n",
      "Epoch: 23, Loss: 1.3920\n",
      "Epoch: 24, Loss: 1.3535\n",
      "Epoch: 25, Loss: 1.3178\n",
      "Epoch: 26, Loss: 1.2847\n",
      "Epoch: 27, Loss: 1.2557\n",
      "Epoch: 28, Loss: 1.2260\n",
      "Epoch: 29, Loss: 1.2001\n",
      "Epoch: 30, Loss: 1.1740\n",
      "Epoch: 31, Loss: 1.1534\n",
      "Epoch: 32, Loss: 1.1323\n",
      "Epoch: 33, Loss: 1.1134\n",
      "Epoch: 34, Loss: 1.0952\n",
      "Epoch: 35, Loss: 1.0751\n",
      "Epoch: 36, Loss: 1.0582\n",
      "Epoch: 37, Loss: 1.0445\n",
      "Epoch: 38, Loss: 1.0315\n",
      "Epoch: 39, Loss: 1.0171\n",
      "Epoch: 40, Loss: 1.0040\n",
      "Epoch: 41, Loss: 0.9929\n",
      "Epoch: 42, Loss: 0.9837\n",
      "Epoch: 43, Loss: 0.9713\n",
      "Epoch: 44, Loss: 0.9629\n",
      "Epoch: 45, Loss: 0.9533\n",
      "Epoch: 46, Loss: 0.9443\n",
      "Epoch: 47, Loss: 0.9358\n",
      "Epoch: 48, Loss: 0.9281\n",
      "Epoch: 49, Loss: 0.9215\n",
      "Epoch: 50, Loss: 0.9144\n",
      "Epoch: 51, Loss: 0.9093\n",
      "Epoch: 52, Loss: 0.9024\n",
      "Epoch: 53, Loss: 0.8970\n",
      "Epoch: 54, Loss: 0.8923\n",
      "Epoch: 55, Loss: 0.8874\n",
      "Epoch: 56, Loss: 0.8819\n",
      "Epoch: 57, Loss: 0.8768\n",
      "Epoch: 58, Loss: 0.8725\n",
      "Epoch: 59, Loss: 0.8693\n",
      "Epoch: 60, Loss: 0.8642\n",
      "Epoch: 61, Loss: 0.8606\n",
      "Epoch: 62, Loss: 0.8563\n",
      "Epoch: 63, Loss: 0.8536\n",
      "Epoch: 64, Loss: 0.8508\n",
      "Epoch: 65, Loss: 0.8480\n",
      "Epoch: 66, Loss: 0.8460\n",
      "Epoch: 67, Loss: 0.8426\n",
      "Epoch: 68, Loss: 0.8406\n",
      "Epoch: 69, Loss: 0.8365\n",
      "Epoch: 70, Loss: 0.8350\n",
      "Epoch: 71, Loss: 0.8328\n",
      "Epoch: 72, Loss: 0.8303\n",
      "Epoch: 73, Loss: 0.8294\n",
      "Epoch: 74, Loss: 0.8270\n",
      "Epoch: 75, Loss: 0.8256\n",
      "Epoch: 76, Loss: 0.8223\n",
      "Epoch: 77, Loss: 0.8206\n",
      "Epoch: 78, Loss: 0.8205\n",
      "Epoch: 79, Loss: 0.8190\n",
      "Epoch: 80, Loss: 0.8157\n",
      "Epoch: 81, Loss: 0.8159\n",
      "Epoch: 82, Loss: 0.8139\n",
      "Epoch: 83, Loss: 0.8128\n",
      "Epoch: 84, Loss: 0.8107\n",
      "Epoch: 85, Loss: 0.8098\n",
      "Epoch: 86, Loss: 0.8083\n",
      "Epoch: 87, Loss: 0.8069\n",
      "Epoch: 88, Loss: 0.8064\n",
      "Epoch: 89, Loss: 0.8049\n",
      "Epoch: 90, Loss: 0.8047\n",
      "Epoch: 91, Loss: 0.8033\n",
      "Epoch: 92, Loss: 0.8024\n",
      "Epoch: 93, Loss: 0.8019\n",
      "Epoch: 94, Loss: 0.7997\n",
      "Epoch: 95, Loss: 0.7990\n",
      "Epoch: 96, Loss: 0.7981\n",
      "Epoch: 97, Loss: 0.7979\n",
      "Epoch: 98, Loss: 0.7976\n",
      "Epoch: 99, Loss: 0.7961\n",
      "Epoch: 100, Loss: 0.7956\n",
      "Epoch: 101, Loss: 0.7947\n",
      "Epoch: 102, Loss: 0.7941\n",
      "Epoch: 103, Loss: 0.7939\n",
      "Epoch: 104, Loss: 0.7923\n",
      "Epoch: 105, Loss: 0.7920\n",
      "Epoch: 106, Loss: 0.7916\n",
      "Epoch: 107, Loss: 0.7902\n",
      "Epoch: 108, Loss: 0.7900\n",
      "Epoch: 109, Loss: 0.7901\n",
      "Epoch: 110, Loss: 0.7897\n",
      "Epoch: 111, Loss: 0.7885\n",
      "Epoch: 112, Loss: 0.7884\n",
      "Epoch: 113, Loss: 0.7878\n",
      "Epoch: 114, Loss: 0.7877\n",
      "Epoch: 115, Loss: 0.7871\n",
      "Epoch: 116, Loss: 0.7867\n",
      "Epoch: 117, Loss: 0.7860\n",
      "Epoch: 118, Loss: 0.7850\n",
      "Epoch: 119, Loss: 0.7848\n",
      "Epoch: 120, Loss: 0.7843\n",
      "Epoch: 121, Loss: 0.7839\n",
      "Epoch: 122, Loss: 0.7845\n",
      "Epoch: 123, Loss: 0.7831\n",
      "Epoch: 124, Loss: 0.7830\n",
      "Epoch: 125, Loss: 0.7828\n",
      "Epoch: 126, Loss: 0.7822\n",
      "Epoch: 127, Loss: 0.7816\n",
      "Epoch: 128, Loss: 0.7824\n",
      "Epoch: 129, Loss: 0.7816\n",
      "Epoch: 130, Loss: 0.7814\n",
      "Epoch: 131, Loss: 0.7813\n",
      "Epoch: 132, Loss: 0.7803\n",
      "Epoch: 133, Loss: 0.7802\n",
      "Epoch: 134, Loss: 0.7798\n",
      "Epoch: 135, Loss: 0.7799\n",
      "Epoch: 136, Loss: 0.7794\n",
      "Epoch: 137, Loss: 0.7794\n",
      "Epoch: 138, Loss: 0.7793\n",
      "Epoch: 139, Loss: 0.7787\n",
      "Epoch: 140, Loss: 0.7781\n",
      "Epoch: 141, Loss: 0.7782\n",
      "Epoch: 142, Loss: 0.7772\n",
      "Epoch: 143, Loss: 0.7776\n",
      "Epoch: 144, Loss: 0.7773\n",
      "Epoch: 145, Loss: 0.7774\n",
      "Epoch: 146, Loss: 0.7773\n",
      "Epoch: 147, Loss: 0.7774\n",
      "Epoch: 148, Loss: 0.7768\n",
      "Epoch: 149, Loss: 0.7766\n",
      "Epoch: 150, Loss: 0.7768\n",
      "Epoch: 151, Loss: 0.7763\n",
      "Epoch: 152, Loss: 0.7762\n",
      "Epoch: 153, Loss: 0.7758\n",
      "Epoch: 154, Loss: 0.7762\n",
      "Epoch: 155, Loss: 0.7758\n",
      "Epoch: 156, Loss: 0.7752\n",
      "Epoch: 157, Loss: 0.7746\n",
      "Epoch: 158, Loss: 0.7756\n",
      "Epoch: 159, Loss: 0.7757\n",
      "Epoch: 160, Loss: 0.7745\n",
      "Epoch: 161, Loss: 0.7744\n",
      "Epoch: 162, Loss: 0.7742\n",
      "Epoch: 163, Loss: 0.7740\n",
      "Epoch: 164, Loss: 0.7743\n",
      "Epoch: 165, Loss: 0.7747\n",
      "Epoch: 166, Loss: 0.7738\n",
      "Epoch: 167, Loss: 0.7743\n",
      "Epoch: 168, Loss: 0.7743\n",
      "Epoch: 169, Loss: 0.7738\n",
      "Epoch: 170, Loss: 0.7740\n",
      "Epoch: 171, Loss: 0.7733\n",
      "Epoch: 172, Loss: 0.7735\n",
      "Epoch: 173, Loss: 0.7739\n",
      "Epoch: 174, Loss: 0.7732\n",
      "Epoch: 175, Loss: 0.7735\n",
      "Epoch: 176, Loss: 0.7730\n",
      "Epoch: 177, Loss: 0.7727\n",
      "Epoch: 178, Loss: 0.7733\n",
      "Epoch: 179, Loss: 0.7729\n",
      "Epoch: 180, Loss: 0.7731\n",
      "Epoch: 181, Loss: 0.7722\n",
      "Epoch: 182, Loss: 0.7724\n",
      "Epoch: 183, Loss: 0.7736\n",
      "Epoch: 184, Loss: 0.7725\n",
      "Epoch: 185, Loss: 0.7723\n",
      "Epoch: 186, Loss: 0.7721\n",
      "Epoch: 187, Loss: 0.7719\n",
      "Epoch: 188, Loss: 0.7718\n",
      "Epoch: 189, Loss: 0.7722\n",
      "Epoch: 190, Loss: 0.7720\n",
      "Epoch: 191, Loss: 0.7722\n",
      "Epoch: 192, Loss: 0.7722\n",
      "Epoch: 193, Loss: 0.7722\n",
      "Epoch: 194, Loss: 0.7721\n",
      "Epoch: 195, Loss: 0.7714\n",
      "Epoch: 196, Loss: 0.7720\n",
      "Epoch: 197, Loss: 0.7720\n",
      "Epoch: 198, Loss: 0.7713\n",
      "Epoch: 199, Loss: 0.7718\n",
      "Epoch: 200, Loss: 0.7715\n",
      "Epoch: 201, Loss: 0.7723\n",
      "Epoch: 202, Loss: 0.7717\n",
      "Epoch: 203, Loss: 0.7716\n",
      "Epoch: 204, Loss: 0.7712\n",
      "Epoch: 205, Loss: 0.7710\n",
      "Epoch: 206, Loss: 0.7712\n",
      "Epoch: 207, Loss: 0.7711\n",
      "Epoch: 208, Loss: 0.7712\n",
      "Epoch: 209, Loss: 0.7714\n",
      "Epoch: 210, Loss: 0.7714\n",
      "Epoch: 211, Loss: 0.7706\n",
      "Epoch: 212, Loss: 0.7713\n",
      "Epoch: 213, Loss: 0.7715\n",
      "Epoch: 214, Loss: 0.7710\n",
      "Epoch: 215, Loss: 0.7708\n",
      "Epoch: 216, Loss: 0.7715\n",
      "Epoch: 217, Loss: 0.7707\n",
      "Epoch: 218, Loss: 0.7714\n",
      "Epoch: 219, Loss: 0.7714\n",
      "Epoch: 220, Loss: 0.7716\n",
      "Epoch: 221, Loss: 0.7710\n",
      "Epoch: 222, Loss: 0.7705\n",
      "Epoch: 223, Loss: 0.7711\n",
      "Epoch: 224, Loss: 0.7709\n",
      "Epoch: 225, Loss: 0.7712\n",
      "Epoch: 226, Loss: 0.7710\n",
      "Epoch: 227, Loss: 0.7711\n",
      "Epoch: 228, Loss: 0.7711\n",
      "Epoch: 229, Loss: 0.7710\n",
      "Epoch: 230, Loss: 0.7714\n",
      "Epoch: 231, Loss: 0.7709\n",
      "Epoch: 232, Loss: 0.7706\n",
      "Epoch: 233, Loss: 0.7701\n",
      "Epoch: 234, Loss: 0.7705\n",
      "Epoch: 235, Loss: 0.7705\n",
      "Epoch: 236, Loss: 0.7709\n",
      "Epoch: 237, Loss: 0.7707\n",
      "Epoch: 238, Loss: 0.7700\n",
      "Epoch: 239, Loss: 0.7706\n",
      "Epoch: 240, Loss: 0.7705\n",
      "Epoch: 241, Loss: 0.7708\n",
      "Epoch: 242, Loss: 0.7703\n",
      "Epoch: 243, Loss: 0.7705\n",
      "Epoch: 244, Loss: 0.7709\n",
      "Epoch: 245, Loss: 0.7706\n",
      "Epoch: 246, Loss: 0.7703\n",
      "Epoch: 247, Loss: 0.7711\n",
      "Epoch: 248, Loss: 0.7706\n",
      "Epoch: 249, Loss: 0.7702\n",
      "Epoch: 250, Loss: 0.7706\n",
      "Epoch: 251, Loss: 0.7706\n",
      "Epoch: 252, Loss: 0.7707\n",
      "Epoch: 253, Loss: 0.7702\n",
      "Epoch: 254, Loss: 0.7706\n",
      "Epoch: 255, Loss: 0.7702\n",
      "Epoch: 256, Loss: 0.7699\n",
      "Epoch: 257, Loss: 0.7702\n",
      "Epoch: 258, Loss: 0.7703\n",
      "Epoch: 259, Loss: 0.7703\n",
      "Epoch: 260, Loss: 0.7708\n",
      "Epoch: 261, Loss: 0.7697\n",
      "Epoch: 262, Loss: 0.7703\n",
      "Epoch: 263, Loss: 0.7706\n",
      "Epoch: 264, Loss: 0.7710\n",
      "Epoch: 265, Loss: 0.7704\n",
      "Epoch: 266, Loss: 0.7710\n",
      "Epoch: 267, Loss: 0.7701\n",
      "Epoch: 268, Loss: 0.7698\n",
      "Epoch: 269, Loss: 0.7708\n",
      "Epoch: 270, Loss: 0.7703\n",
      "Epoch: 271, Loss: 0.7704\n",
      "Epoch: 272, Loss: 0.7703\n",
      "Epoch: 273, Loss: 0.7700\n",
      "Epoch: 274, Loss: 0.7700\n",
      "Epoch: 275, Loss: 0.7704\n",
      "Epoch: 276, Loss: 0.7702\n",
      "Epoch: 277, Loss: 0.7697\n",
      "Epoch: 278, Loss: 0.7702\n",
      "Epoch: 279, Loss: 0.7703\n",
      "Epoch: 280, Loss: 0.7704\n",
      "Epoch: 281, Loss: 0.7700\n",
      "Epoch: 282, Loss: 0.7702\n",
      "Epoch: 283, Loss: 0.7707\n",
      "Epoch: 284, Loss: 0.7710\n",
      "Epoch: 285, Loss: 0.7706\n",
      "Epoch: 286, Loss: 0.7698\n",
      "Epoch: 287, Loss: 0.7709\n",
      "Epoch: 288, Loss: 0.7711\n",
      "Epoch: 289, Loss: 0.7702\n",
      "Epoch: 290, Loss: 0.7701\n",
      "Epoch: 291, Loss: 0.7708\n",
      "Epoch: 292, Loss: 0.7703\n",
      "Epoch: 293, Loss: 0.7699\n",
      "Epoch: 294, Loss: 0.7707\n",
      "Epoch: 295, Loss: 0.7704\n",
      "Epoch: 296, Loss: 0.7699\n",
      "Epoch: 297, Loss: 0.7703\n",
      "Epoch: 298, Loss: 0.7701\n",
      "Epoch: 299, Loss: 0.7706\n",
      "Epoch: 300, Loss: 0.7713\n",
      "Epoch: 301, Loss: 0.7705\n",
      "Epoch: 302, Loss: 0.7703\n",
      "Epoch: 303, Loss: 0.7703\n",
      "Epoch: 304, Loss: 0.7709\n",
      "Epoch: 305, Loss: 0.7705\n",
      "Epoch: 306, Loss: 0.7704\n",
      "Epoch: 307, Loss: 0.7707\n",
      "Epoch: 308, Loss: 0.7705\n",
      "Epoch: 309, Loss: 0.7704\n",
      "Epoch: 310, Loss: 0.7708\n",
      "Epoch: 311, Loss: 0.7699\n",
      "Epoch: 312, Loss: 0.7704\n",
      "Epoch: 313, Loss: 0.7701\n",
      "Epoch: 314, Loss: 0.7707\n",
      "Epoch: 315, Loss: 0.7706\n",
      "Epoch: 316, Loss: 0.7709\n",
      "Epoch: 317, Loss: 0.7706\n",
      "Epoch: 318, Loss: 0.7700\n",
      "Epoch: 319, Loss: 0.7711\n",
      "Epoch: 320, Loss: 0.7706\n",
      "Epoch: 321, Loss: 0.7701\n",
      "Epoch: 322, Loss: 0.7700\n",
      "Epoch: 323, Loss: 0.7702\n",
      "Epoch: 324, Loss: 0.7702\n",
      "Epoch: 325, Loss: 0.7713\n",
      "Epoch: 326, Loss: 0.7710\n",
      "Epoch: 327, Loss: 0.7705\n",
      "Epoch: 328, Loss: 0.7709\n",
      "Epoch: 329, Loss: 0.7702\n",
      "Epoch: 330, Loss: 0.7699\n",
      "Epoch: 331, Loss: 0.7705\n",
      "Epoch: 332, Loss: 0.7709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 333, Loss: 0.7707\n",
      "Epoch: 334, Loss: 0.7701\n",
      "Epoch: 335, Loss: 0.7705\n",
      "Epoch: 336, Loss: 0.7696\n",
      "Epoch: 337, Loss: 0.7705\n",
      "Epoch: 338, Loss: 0.7697\n",
      "Epoch: 339, Loss: 0.7698\n",
      "Epoch: 340, Loss: 0.7701\n",
      "Epoch: 341, Loss: 0.7702\n",
      "Epoch: 342, Loss: 0.7707\n",
      "Epoch: 343, Loss: 0.7698\n",
      "Epoch: 344, Loss: 0.7697\n",
      "Epoch: 345, Loss: 0.7701\n",
      "Epoch: 346, Loss: 0.7704\n",
      "Epoch: 347, Loss: 0.7698\n",
      "Epoch: 348, Loss: 0.7708\n",
      "Epoch: 349, Loss: 0.7698\n",
      "Epoch: 350, Loss: 0.7703\n",
      "Epoch: 351, Loss: 0.7703\n",
      "Epoch: 352, Loss: 0.7702\n",
      "Epoch: 353, Loss: 0.7703\n",
      "Epoch: 354, Loss: 0.7709\n",
      "Epoch: 355, Loss: 0.7705\n",
      "Epoch: 356, Loss: 0.7702\n",
      "Epoch: 357, Loss: 0.7707\n",
      "Epoch: 358, Loss: 0.7707\n",
      "Epoch: 359, Loss: 0.7701\n",
      "Epoch: 360, Loss: 0.7699\n",
      "Epoch: 361, Loss: 0.7707\n",
      "Epoch: 362, Loss: 0.7706\n",
      "Epoch: 363, Loss: 0.7704\n",
      "Epoch: 364, Loss: 0.7700\n",
      "Epoch: 365, Loss: 0.7698\n",
      "Epoch: 366, Loss: 0.7703\n",
      "Epoch: 367, Loss: 0.7701\n",
      "Epoch: 368, Loss: 0.7695\n",
      "Epoch: 369, Loss: 0.7698\n",
      "Epoch: 370, Loss: 0.7704\n",
      "Epoch: 371, Loss: 0.7704\n",
      "Epoch: 372, Loss: 0.7700\n",
      "Epoch: 373, Loss: 0.7702\n",
      "Epoch: 374, Loss: 0.7700\n",
      "Epoch: 375, Loss: 0.7697\n",
      "Epoch: 376, Loss: 0.7702\n",
      "Epoch: 377, Loss: 0.7701\n",
      "Epoch: 378, Loss: 0.7705\n",
      "Epoch: 379, Loss: 0.7706\n",
      "Epoch: 380, Loss: 0.7706\n",
      "Epoch: 381, Loss: 0.7701\n",
      "Epoch: 382, Loss: 0.7702\n",
      "Epoch: 383, Loss: 0.7710\n",
      "Epoch: 384, Loss: 0.7705\n",
      "Epoch: 385, Loss: 0.7700\n",
      "Epoch: 386, Loss: 0.7710\n",
      "Epoch: 387, Loss: 0.7706\n",
      "Epoch: 388, Loss: 0.7698\n",
      "Epoch: 389, Loss: 0.7706\n",
      "Epoch: 390, Loss: 0.7701\n",
      "Epoch: 391, Loss: 0.7702\n",
      "Epoch: 392, Loss: 0.7706\n",
      "Epoch: 393, Loss: 0.7705\n",
      "Epoch: 394, Loss: 0.7706\n",
      "Epoch: 395, Loss: 0.7696\n",
      "Epoch: 396, Loss: 0.7707\n",
      "Epoch: 397, Loss: 0.7701\n",
      "Epoch: 398, Loss: 0.7705\n",
      "Epoch: 399, Loss: 0.7702\n",
      "Epoch: 400, Loss: 0.7697\n",
      "289.18605375289917\n"
     ]
    }
   ],
   "source": [
    "s_t = time()\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch+1:02d}, Loss: {loss:.4f}')\n",
    "print(time()-s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z = model(torch.arange(edge_pairs.max()+1, device=device))\n",
    "#z = TSNE(n_components=2).fit_transform(z.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./embedding/node2vec_0221.npz',z.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), pretrain_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
