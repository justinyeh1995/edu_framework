{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wcks13589/.local/lib/python3.6/site-packages/numba/core/errors.py:154: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "from Model import MLP_with_pretrain, Whole_model_Node2Vec\n",
    "from utils import column_idx, make_edges_symmetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "shop_col = 'stonc_6_label'\n",
    "#shop_col = 'mcc'\n",
    "#shop_col = 'stonc_label'\n",
    "#shop_col = 'stonc_10_label'\n",
    "\n",
    "embedding_size = 64\n",
    "\n",
    "epochs = 400\n",
    "early_stop = 20\n",
    "batch_size = 2048\n",
    "learning_rate = 0.001\n",
    "\n",
    "pretrain_weights = './weights/node2vec_weights_stonc6'\n",
    "result_path = './result/Node2Vec_stonc6.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_path = './data/sample'\n",
    "\n",
    "chid_dict_file_name = 'sample_50k_idx_map.npy'\n",
    "cdtx_file_name = 'sample_50k_cdtx.csv'\n",
    "\n",
    "sample_chid_dict = os.path.join(sample_data_path, chid_dict_file_name)\n",
    "sample_cdtx_file = os.path.join(sample_data_path, cdtx_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_data_path = './data/downstream'\n",
    "\n",
    "x_train_file_name = 'x_train.csv'\n",
    "x_test_file_name = 'x_test.csv'\n",
    "y_train_file_name = 'y_train.csv'\n",
    "y_test_file_name = 'y_test.csv'\n",
    "\n",
    "x_train_file = os.path.join(downstream_data_path, x_train_file_name)\n",
    "x_test_file = os.path.join(downstream_data_path, x_test_file_name)\n",
    "y_train_file = os.path.join(downstream_data_path, y_train_file_name)\n",
    "y_test_file = os.path.join(downstream_data_path, y_test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx = pd.read_csv(sample_cdtx_file)\n",
    "df_cdtx.sort_values('csmdt')\n",
    "\n",
    "# Load dict\n",
    "idx_map = np.load(sample_chid_dict, allow_pickle=True).tolist()\n",
    "\n",
    "l = len(idx_map)\n",
    "for i , j in enumerate(sorted(df_cdtx[shop_col].unique())):\n",
    "    idx_map[j] = i+l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx.chid = df_cdtx.chid.map(idx_map)\n",
    "df_cdtx[shop_col] = df_cdtx[shop_col].map(idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx.csmdt = df_cdtx.csmdt.apply(lambda x: x[:8]+'01')\n",
    "df_cdtx.objam = df_cdtx.objam.apply(lambda x: int(x))\n",
    "df_cdtx = df_cdtx[df_cdtx.csmdt < '2019-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_pairs = df_cdtx[['chid', shop_col]].copy()\n",
    "edge_pairs.drop_duplicates(ignore_index=True, inplace=True)\n",
    "edge_pairs = edge_pairs.to_numpy().T\n",
    "\n",
    "edge_pairs = make_edges_symmetry(edge_pairs)\n",
    "edge_pairs = torch.LongTensor(edge_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(x_train_file)\n",
    "x_test = pd.read_csv(x_test_file)\n",
    "\n",
    "y_train = pd.read_csv(y_train_file)\n",
    "y_test = pd.read_csv(y_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_cols = ['chid']\n",
    "\n",
    "category_cols = ['masts', 'educd', 'naty', 'trdtp', 'poscd', 'cuorg']\n",
    "    \n",
    "numeric_cols = sorted(set(x_train.columns) - set(category_cols) - set(ignore_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dims = {category_col:pd.concat([x_train[category_col],x_test[category_col]]).nunique() \n",
    "                 for category_col in category_cols}\n",
    "\n",
    "category_dict = column_idx(x_train, category_cols)\n",
    "numeric_dict = column_idx(x_train, numeric_cols)\n",
    "\n",
    "input_dim = len(category_dict)*embedding_size + len(numeric_dict) + embedding_size\n",
    "\n",
    "layer_dims = [input_dim, 256, 128, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(x_train.to_numpy()),\n",
    "                              torch.from_numpy(y_train.to_numpy()))\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(x_test.to_numpy()),\n",
    "                              torch.from_numpy(y_test.to_numpy()))\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_model = Node2Vec(edge_pairs, embedding_dim=embedding_size, walk_length=2,\n",
    "                           context_size=2, walks_per_node=10,\n",
    "                           num_negative_samples=1, p=1, q=1, sparse=True)\n",
    "pre_train_model.load_state_dict(torch.load(pretrain_weights))\n",
    "pre_train_model.train()\n",
    "\n",
    "down_stream_model = MLP_with_pretrain(category_dims, layer_dims, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole_model_Node2Vec(\n",
      "  (pre_train_model): Node2Vec(128560, 64)\n",
      "  (down_stream_model): MLP_with_pretrain(\n",
      "    (model): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=472, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (embedding_dict): ModuleDict(\n",
      "      (masts): Embedding(3, 64)\n",
      "      (educd): Embedding(6, 64)\n",
      "      (naty): Embedding(2, 64)\n",
      "      (trdtp): Embedding(27, 64)\n",
      "      (poscd): Embedding(9, 64)\n",
      "      (cuorg): Embedding(30, 64)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Whole_model_Node2Vec(pre_train_model, down_stream_model).to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "train loss:1018532,test loss:614334\n",
      "train MAE(mean):75661,test MAE(mean):80180\n",
      "train MAE(median):11531, test MAE(median):11034\n",
      "\tBetter!\n",
      "epoch:1\n",
      "train loss:1013964,test loss:598450\n",
      "train MAE(mean):69841,test MAE(mean):71724\n",
      "train MAE(median):9462, test MAE(median):9306\n",
      "\tBetter!\n",
      "epoch:2\n",
      "train loss:1006978,test loss:578812\n",
      "train MAE(mean):63462,test MAE(mean):70125\n",
      "train MAE(median):8547, test MAE(median):10699\n",
      "\tBetter!\n",
      "epoch:3\n",
      "train loss:999126,test loss:564742\n",
      "train MAE(mean):61187,test MAE(mean):70767\n",
      "train MAE(median):8263, test MAE(median):11389\n",
      "\tBetter!\n",
      "epoch:4\n",
      "train loss:995906,test loss:551733\n",
      "train MAE(mean):60544,test MAE(mean):66999\n",
      "train MAE(median):8222, test MAE(median):9493\n",
      "\tBetter!\n",
      "epoch:5\n",
      "train loss:990045,test loss:536815\n",
      "train MAE(mean):59957,test MAE(mean):66673\n",
      "train MAE(median):8096, test MAE(median):9212\n",
      "\tBetter!\n",
      "epoch:6\n",
      "train loss:986846,test loss:534209\n",
      "train MAE(mean):59554,test MAE(mean):66728\n",
      "train MAE(median):8057, test MAE(median):8953\n",
      "\tBetter!\n",
      "epoch:7\n",
      "train loss:985104,test loss:533267\n",
      "train MAE(mean):59705,test MAE(mean):66949\n",
      "train MAE(median):8113, test MAE(median):9190\n",
      "\tBetter!\n",
      "epoch:8\n",
      "train loss:987925,test loss:532030\n",
      "train MAE(mean):59535,test MAE(mean):66722\n",
      "train MAE(median):8011, test MAE(median):9212\n",
      "\tBetter!\n",
      "epoch:9\n",
      "train loss:988320,test loss:508862\n",
      "train MAE(mean):59643,test MAE(mean):68295\n",
      "train MAE(median):8075, test MAE(median):10629\n",
      "\tBetter!\n",
      "epoch:10\n",
      "train loss:985406,test loss:544548\n",
      "train MAE(mean):59239,test MAE(mean):66952\n",
      "train MAE(median):8002, test MAE(median):8637\n",
      "epoch:11\n",
      "train loss:986130,test loss:566035\n",
      "train MAE(mean):59456,test MAE(mean):68779\n",
      "train MAE(median):8013, test MAE(median):8406\n",
      "epoch:12\n",
      "train loss:989247,test loss:545493\n",
      "train MAE(mean):59344,test MAE(mean):67557\n",
      "train MAE(median):7974, test MAE(median):8465\n",
      "epoch:13\n",
      "train loss:987810,test loss:529831\n",
      "train MAE(mean):59329,test MAE(mean):66654\n",
      "train MAE(median):7963, test MAE(median):8773\n",
      "epoch:14\n",
      "train loss:986213,test loss:560956\n",
      "train MAE(mean):59092,test MAE(mean):68298\n",
      "train MAE(median):7959, test MAE(median):8307\n",
      "epoch:15\n",
      "train loss:986966,test loss:514425\n",
      "train MAE(mean):59157,test MAE(mean):66575\n",
      "train MAE(median):7921, test MAE(median):9686\n",
      "epoch:16\n",
      "train loss:987353,test loss:549984\n",
      "train MAE(mean):59074,test MAE(mean):67902\n",
      "train MAE(median):7901, test MAE(median):8536\n",
      "epoch:17\n",
      "train loss:984754,test loss:531412\n",
      "train MAE(mean):59285,test MAE(mean):66547\n",
      "train MAE(median):7961, test MAE(median):8795\n",
      "epoch:18\n",
      "train loss:989579,test loss:535999\n",
      "train MAE(mean):59074,test MAE(mean):66874\n",
      "train MAE(median):7922, test MAE(median):8610\n",
      "epoch:19\n",
      "train loss:985436,test loss:513651\n",
      "train MAE(mean):58875,test MAE(mean):66383\n",
      "train MAE(median):7878, test MAE(median):9363\n",
      "epoch:20\n",
      "train loss:984045,test loss:504651\n",
      "train MAE(mean):58754,test MAE(mean):67555\n",
      "train MAE(median):7852, test MAE(median):10092\n",
      "\tBetter!\n",
      "epoch:21\n",
      "train loss:987951,test loss:547686\n",
      "train MAE(mean):59025,test MAE(mean):67930\n",
      "train MAE(median):7896, test MAE(median):8388\n",
      "epoch:22\n",
      "train loss:986461,test loss:514967\n",
      "train MAE(mean):58747,test MAE(mean):66544\n",
      "train MAE(median):7840, test MAE(median):9574\n",
      "epoch:23\n",
      "train loss:986987,test loss:538395\n",
      "train MAE(mean):58716,test MAE(mean):66725\n",
      "train MAE(median):7817, test MAE(median):8587\n",
      "epoch:24\n",
      "train loss:990623,test loss:554940\n",
      "train MAE(mean):58920,test MAE(mean):68204\n",
      "train MAE(median):7897, test MAE(median):8279\n",
      "epoch:25\n",
      "train loss:987240,test loss:524425\n",
      "train MAE(mean):59186,test MAE(mean):67672\n",
      "train MAE(median):7880, test MAE(median):10038\n",
      "epoch:26\n",
      "train loss:988558,test loss:525874\n",
      "train MAE(mean):59179,test MAE(mean):66174\n",
      "train MAE(median):7911, test MAE(median):8903\n",
      "epoch:27\n",
      "train loss:985548,test loss:528605\n",
      "train MAE(mean):58654,test MAE(mean):66600\n",
      "train MAE(median):7813, test MAE(median):9278\n",
      "epoch:28\n",
      "train loss:987544,test loss:515086\n",
      "train MAE(mean):58629,test MAE(mean):66858\n",
      "train MAE(median):7836, test MAE(median):9505\n",
      "epoch:29\n",
      "train loss:984578,test loss:536614\n",
      "train MAE(mean):58816,test MAE(mean):66581\n",
      "train MAE(median):7866, test MAE(median):9115\n",
      "epoch:30\n",
      "train loss:985838,test loss:511750\n",
      "train MAE(mean):58673,test MAE(mean):66212\n",
      "train MAE(median):7814, test MAE(median):9081\n",
      "epoch:31\n",
      "train loss:988654,test loss:529053\n",
      "train MAE(mean):58697,test MAE(mean):66510\n",
      "train MAE(median):7805, test MAE(median):8609\n",
      "epoch:32\n",
      "train loss:986278,test loss:510158\n",
      "train MAE(mean):58533,test MAE(mean):66443\n",
      "train MAE(median):7814, test MAE(median):9327\n",
      "epoch:33\n",
      "train loss:989230,test loss:525376\n",
      "train MAE(mean):58463,test MAE(mean):66426\n",
      "train MAE(median):7796, test MAE(median):9203\n",
      "epoch:34\n",
      "train loss:984838,test loss:533653\n",
      "train MAE(mean):58471,test MAE(mean):66379\n",
      "train MAE(median):7797, test MAE(median):8904\n",
      "epoch:35\n",
      "train loss:986860,test loss:550565\n",
      "train MAE(mean):58635,test MAE(mean):66802\n",
      "train MAE(median):7842, test MAE(median):8625\n",
      "epoch:36\n",
      "train loss:984247,test loss:524917\n",
      "train MAE(mean):58413,test MAE(mean):66581\n",
      "train MAE(median):7793, test MAE(median):9287\n",
      "epoch:37\n",
      "train loss:986515,test loss:518338\n",
      "train MAE(mean):58384,test MAE(mean):66620\n",
      "train MAE(median):7745, test MAE(median):9423\n",
      "epoch:38\n",
      "train loss:986607,test loss:517619\n",
      "train MAE(mean):58487,test MAE(mean):66403\n",
      "train MAE(median):7774, test MAE(median):8874\n",
      "epoch:39\n",
      "train loss:986473,test loss:528329\n",
      "train MAE(mean):58537,test MAE(mean):66560\n",
      "train MAE(median):7768, test MAE(median):8757\n",
      "epoch:40\n",
      "train loss:983433,test loss:545328\n",
      "train MAE(mean):58365,test MAE(mean):67024\n",
      "train MAE(median):7775, test MAE(median):8470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = 1e10\n",
    "early_cnt = 0\n",
    "RMSE = []\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_output = np.array([])\n",
    "    train_y = np.array([])\n",
    "    test_output = np.array([])\n",
    "    test_y = np.array([])\n",
    "    \n",
    "    for x , y in train_loader:\n",
    "        x, y = x.float().to(device), y.float().to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(x, category_dict, numeric_dict)\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "        train_loss += loss.item()\n",
    "        train_output = np.concatenate([train_output,output.cpu().detach().numpy().reshape(-1)])\n",
    "        train_y = np.concatenate([train_y,y.cpu().detach().numpy().reshape(-1)])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    for x , y in test_loader:\n",
    "        x, y = x.float().to(device), y.float().to(device)\n",
    "        model.eval()        \n",
    "        output = model(x, category_dict, numeric_dict)\n",
    "        loss = criterion(output, y)\n",
    "        test_loss += loss.item()\n",
    "        test_output = np.concatenate([test_output,output.cpu().detach().numpy().reshape(-1)])\n",
    "        test_y = np.concatenate([test_y,y.cpu().detach().numpy().reshape(-1)])\n",
    "    \n",
    "    #train_loss = np.sqrt(train_loss/len(train_loader))\n",
    "    #test_loss = np.sqrt(test_loss/len(test_loader))\n",
    "    \n",
    "    train_output, train_y = np.e**train_output, np.e**train_y\n",
    "    train_RMSE = mean_squared_error(train_output, train_y, squared=False)\n",
    "    train_mean = mean_absolute_error(train_output, train_y)\n",
    "    train_median = median_absolute_error(train_output, train_y)\n",
    "    \n",
    "    test_output, test_y = np.e**test_output, np.e**test_y\n",
    "    test_RMSE = mean_squared_error(test_output, test_y, squared=False)\n",
    "    test_mean = mean_absolute_error(test_output, test_y)\n",
    "    test_median = median_absolute_error(test_output, test_y)\n",
    "    \n",
    "    print(f'epoch:{epoch}\\ntrain loss:{train_RMSE:.0f},test loss:{test_RMSE:.0f}\\ntrain MAE(mean):{train_mean:.0f},test MAE(mean):{test_mean:.0f}\\ntrain MAE(median):{train_median:.0f}, test MAE(median):{test_median:.0f}')\n",
    "    \n",
    "    if test_RMSE <= best_loss:\n",
    "        best_model_params = copy.deepcopy(model.state_dict())\n",
    "        best_loss = test_RMSE\n",
    "        print('\\tBetter!')\n",
    "        early_cnt = 0\n",
    "    else:\n",
    "        early_cnt += 1\n",
    "    \n",
    "    if early_cnt >= early_stop:\n",
    "        break\n",
    "\n",
    "model.load_state_dict(best_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [00:05<00:00, 43.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_output = np.array([])\n",
    "train_y = np.array([])\n",
    "test_output = np.array([])\n",
    "test_y = np.array([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for x , y in tqdm(train_loader):\n",
    "    x, y = x.float().to(device), y.float().to(device)\n",
    "    \n",
    "    output = model(x, category_dict, numeric_dict)\n",
    "    train_output = np.concatenate([train_output,output.cpu().detach().numpy().reshape(-1)])\n",
    "    train_y = np.concatenate([train_y,y.cpu().detach().numpy().reshape(-1)])\n",
    "\n",
    "for x , y in test_loader:\n",
    "    x, y = x.float().to(device), y.float().to(device)\n",
    "          \n",
    "    output = model(x, category_dict, numeric_dict)\n",
    "    test_output = np.concatenate([test_output,output.cpu().detach().numpy().reshape(-1)])\n",
    "    test_y = np.concatenate([test_y,y.cpu().detach().numpy().reshape(-1)])\n",
    "\n",
    "train_output, train_y = np.e**train_output, np.e**train_y\n",
    "test_output, test_y = np.e**test_output, np.e**test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\tRMSE: 983030.83 MSE(mean): 58176.82 MSE(median): 8317.89\n",
      "test\tRMSE: 504650.86 MSE(mean): 67555.17 MSE(median): 10092.15\n"
     ]
    }
   ],
   "source": [
    "print('train\\tRMSE: {:.2f} MSE(mean): {:.2f} MSE(median): {:.2f}'.format(\n",
    "    mean_squared_error(train_y, train_output, squared=False), \n",
    "    mean_absolute_error(train_y, train_output), \n",
    "    median_absolute_error(train_y, train_output)\n",
    "))\n",
    "print('test\\tRMSE: {:.2f} MSE(mean): {:.2f} MSE(median): {:.2f}'.format(\n",
    "    mean_squared_error(test_y, test_output, squared=False), \n",
    "    mean_absolute_error(test_y, test_output), \n",
    "    median_absolute_error(test_y, test_output)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chid</th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11755.999087</td>\n",
       "      <td>3.522862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>35519.989944</td>\n",
       "      <td>3.508471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>150494.007197</td>\n",
       "      <td>2031.030697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3671.000531</td>\n",
       "      <td>2724.436818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>62179.981812</td>\n",
       "      <td>44748.776804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>49997</td>\n",
       "      <td>235115.953642</td>\n",
       "      <td>1514.547033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>49998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>139.079169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>49998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>134.432409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>49999</td>\n",
       "      <td>2747.000494</td>\n",
       "      <td>1732.665987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>49999</td>\n",
       "      <td>116310.945179</td>\n",
       "      <td>1710.969005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        chid           true          pred\n",
       "0          0   11755.999087      3.522862\n",
       "1          0   35519.989944      3.508471\n",
       "2          1  150494.007197   2031.030697\n",
       "3          1    3671.000531   2724.436818\n",
       "4          2   62179.981812  44748.776804\n",
       "...      ...            ...           ...\n",
       "99995  49997  235115.953642   1514.547033\n",
       "99996  49998       1.000000    139.079169\n",
       "99997  49998       1.000000    134.432409\n",
       "99998  49999    2747.000494   1732.665987\n",
       "99999  49999  116310.945179   1710.969005\n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out = x_test[['chid']].copy()\n",
    "df_out['true'] = test_y\n",
    "df_out['pred'] = test_output\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv(result_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
