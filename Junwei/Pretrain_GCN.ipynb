{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "\n",
    "from time import time\n",
    "\n",
    "from Model import GCNEncoder\n",
    "from utils import make_edges_symmetry, column_idx, train_test_split_edges\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "shop_col = 'stonc_6_label'\n",
    "#shop_col = 'mcc'\n",
    "#shop_col = 'stonc_label'\n",
    "#shop_col = 'stonc_10_label'\n",
    "\n",
    "\n",
    "load_edges = False\n",
    "edges_path = './edges_stonc6.pkl'\n",
    "pretrain_weights = './weights/GCNencoder_stonc6'\n",
    "\n",
    "embedding_size = 64\n",
    "epochs = 400\n",
    "batch_size = 2048\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_path = './data/sample'\n",
    "\n",
    "chid_dict_file_name = 'sample_50k_idx_map.npy'\n",
    "cdtx_file_name = 'sample_50k_cdtx.csv'\n",
    "cust_file_name = 'sample_50k_cust.csv'\n",
    "\n",
    "sample_chid_dict = os.path.join(sample_data_path, chid_dict_file_name)\n",
    "sample_cdtx_file = os.path.join(sample_data_path, cdtx_file_name)\n",
    "sample_cust_file = os.path.join(sample_data_path, cust_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx = pd.read_csv(sample_cdtx_file)\n",
    "df_cdtx.sort_values('csmdt')\n",
    "\n",
    "df_cust = pd.read_csv(sample_cust_file)\n",
    "df_cust.drop_duplicates(ignore_index=True, inplace=True)\n",
    "\n",
    "idx_map = np.load(sample_chid_dict, allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(idx_map)\n",
    "for i , j in enumerate(sorted(df_cdtx[shop_col].unique())):\n",
    "    idx_map[j] = i+l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx.chid = df_cdtx.chid.map(idx_map)\n",
    "df_cdtx[shop_col] = df_cdtx[shop_col].map(idx_map)\n",
    "\n",
    "df_cust.chid = df_cust.chid.map(idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx.csmdt = df_cdtx.csmdt.apply(lambda x: x[:8]+'01')\n",
    "df_cdtx.objam = df_cdtx.objam.apply(lambda x: int(x))\n",
    "\n",
    "df_cust.data_dt = df_cust.data_dt.apply(lambda x: x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_cols = ['chid', 'data_dt']\n",
    "category_cols = ['masts', 'educd', 'naty', 'trdtp', 'poscd', 'cuorg']\n",
    "numeric_cols = sorted(set(df_cust.columns) - set(category_cols) - set(ignore_cols)) + ['objam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {col: {value: index for index, value in enumerate(sorted(df_cust[col].unique()))} \n",
    "          for col in category_cols}\n",
    "\n",
    "df_cust[category_cols] = df_cust[category_cols].apply(lambda x: x.map(mapper[x.name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdtx = df_cdtx[df_cdtx.csmdt < '2019-01-01']\n",
    "df_cust = df_cust[df_cust.data_dt == '2018-12-01'].sort_values(by=['chid'])\n",
    "\n",
    "df_cust['objam'] = np.ma.log(df_cdtx.groupby(['chid']).sum().objam.values/12).filled(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_pairs = df_cdtx[['chid', shop_col]].copy()\n",
    "edge_pairs.drop_duplicates(ignore_index=True, inplace=True)\n",
    "edge_pairs = edge_pairs.to_numpy().T\n",
    "\n",
    "edge_pairs = make_edges_symmetry(edge_pairs)\n",
    "edge_pairs = torch.LongTensor(edge_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler()\n",
    "df_cust[numeric_cols] = x_scaler.fit_transform(df_cust[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_ = df_cust[category_cols+numeric_cols]\n",
    "\n",
    "cust_feature = torch.Tensor(df_cust_.to_numpy())\n",
    "shop_feature = torch.zeros(len(idx_map)-cust_feature.shape[0], cust_feature.shape[1])\n",
    "x_feature = torch.cat([cust_feature, shop_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_edges:\n",
    "    with open(edges_path, 'rb') as f:\n",
    "        import pickle\n",
    "        data = pickle.load(f)\n",
    "else:\n",
    "    data = Data(x=x_feature, edge_index=edge_pairs)\n",
    "    data = train_test_split_edges(data, cust_feature.shape[0])\n",
    "    with open(edges_path, 'wb') as output:\n",
    "        import pickle\n",
    "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dims = {col_name : len(uni)\n",
    "                 for col_name, uni in mapper.items()}\n",
    "\n",
    "category_dict = column_idx(df_cust_, category_cols)\n",
    "numeric_dict = column_idx(df_cust_, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(category_dict)*embedding_size + len(numeric_dict)\n",
    "layer_dims = [input_dim, 256, 128, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAE(GCNEncoder(input_dim, embedding_size, category_dims)).to(device)\n",
    "\n",
    "x = data.x.to(device)\n",
    "train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index, category_dict, numeric_dict)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_pos_edge_index, category_dict, numeric_dict)\n",
    "        \n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss:2.8316, AUC: 0.8668, AP: 0.8837\n",
      "Epoch: 002, Train Loss:1.5028, AUC: 0.8880, AP: 0.9081\n",
      "Epoch: 003, Train Loss:1.2293, AUC: 0.8905, AP: 0.9102\n",
      "Epoch: 004, Train Loss:1.2071, AUC: 0.8901, AP: 0.9098\n",
      "Epoch: 005, Train Loss:1.2464, AUC: 0.8920, AP: 0.9114\n",
      "Epoch: 006, Train Loss:1.2059, AUC: 0.8952, AP: 0.9141\n",
      "Epoch: 007, Train Loss:1.1564, AUC: 0.8965, AP: 0.9153\n",
      "Epoch: 008, Train Loss:1.1523, AUC: 0.8977, AP: 0.9165\n",
      "Epoch: 009, Train Loss:1.1691, AUC: 0.8961, AP: 0.9154\n",
      "Epoch: 010, Train Loss:1.1665, AUC: 0.8938, AP: 0.9133\n",
      "Epoch: 011, Train Loss:1.1483, AUC: 0.8915, AP: 0.9112\n",
      "Epoch: 012, Train Loss:1.1414, AUC: 0.8903, AP: 0.9100\n",
      "Epoch: 013, Train Loss:1.1476, AUC: 0.8897, AP: 0.9095\n",
      "Epoch: 014, Train Loss:1.1466, AUC: 0.8896, AP: 0.9095\n",
      "Epoch: 015, Train Loss:1.1380, AUC: 0.8894, AP: 0.9093\n",
      "Epoch: 016, Train Loss:1.1344, AUC: 0.8895, AP: 0.9095\n",
      "Epoch: 017, Train Loss:1.1373, AUC: 0.8900, AP: 0.9098\n",
      "Epoch: 018, Train Loss:1.1395, AUC: 0.8904, AP: 0.9101\n",
      "Epoch: 019, Train Loss:1.1367, AUC: 0.8909, AP: 0.9103\n",
      "Epoch: 020, Train Loss:1.1333, AUC: 0.8914, AP: 0.9106\n",
      "Epoch: 021, Train Loss:1.1330, AUC: 0.8918, AP: 0.9109\n",
      "Epoch: 022, Train Loss:1.1349, AUC: 0.8925, AP: 0.9114\n",
      "Epoch: 023, Train Loss:1.1347, AUC: 0.8933, AP: 0.9121\n",
      "Epoch: 024, Train Loss:1.1317, AUC: 0.8944, AP: 0.9131\n",
      "Epoch: 025, Train Loss:1.1297, AUC: 0.8958, AP: 0.9143\n",
      "Epoch: 026, Train Loss:1.1300, AUC: 0.8970, AP: 0.9153\n",
      "Epoch: 027, Train Loss:1.1311, AUC: 0.8978, AP: 0.9160\n",
      "Epoch: 028, Train Loss:1.1304, AUC: 0.8981, AP: 0.9161\n",
      "Epoch: 029, Train Loss:1.1283, AUC: 0.8981, AP: 0.9161\n",
      "Epoch: 030, Train Loss:1.1276, AUC: 0.8981, AP: 0.9160\n",
      "Epoch: 031, Train Loss:1.1279, AUC: 0.8982, AP: 0.9160\n",
      "Epoch: 032, Train Loss:1.1277, AUC: 0.8985, AP: 0.9163\n",
      "Epoch: 033, Train Loss:1.1263, AUC: 0.8989, AP: 0.9165\n",
      "Epoch: 034, Train Loss:1.1253, AUC: 0.8994, AP: 0.9169\n",
      "Epoch: 035, Train Loss:1.1256, AUC: 0.9000, AP: 0.9173\n",
      "Epoch: 036, Train Loss:1.1249, AUC: 0.9006, AP: 0.9178\n",
      "Epoch: 037, Train Loss:1.1236, AUC: 0.9013, AP: 0.9184\n",
      "Epoch: 038, Train Loss:1.1230, AUC: 0.9023, AP: 0.9192\n",
      "Epoch: 039, Train Loss:1.1223, AUC: 0.9030, AP: 0.9198\n",
      "Epoch: 040, Train Loss:1.1218, AUC: 0.9039, AP: 0.9205\n",
      "Epoch: 041, Train Loss:1.1209, AUC: 0.9048, AP: 0.9213\n",
      "Epoch: 042, Train Loss:1.1203, AUC: 0.9055, AP: 0.9218\n",
      "Epoch: 043, Train Loss:1.1202, AUC: 0.9057, AP: 0.9220\n",
      "Epoch: 044, Train Loss:1.1198, AUC: 0.9056, AP: 0.9218\n",
      "Epoch: 045, Train Loss:1.1186, AUC: 0.9054, AP: 0.9216\n",
      "Epoch: 046, Train Loss:1.1183, AUC: 0.9054, AP: 0.9215\n",
      "Epoch: 047, Train Loss:1.1180, AUC: 0.9055, AP: 0.9216\n",
      "Epoch: 048, Train Loss:1.1169, AUC: 0.9058, AP: 0.9218\n",
      "Epoch: 049, Train Loss:1.1167, AUC: 0.9062, AP: 0.9222\n",
      "Epoch: 050, Train Loss:1.1163, AUC: 0.9069, AP: 0.9227\n",
      "Epoch: 051, Train Loss:1.1153, AUC: 0.9074, AP: 0.9231\n",
      "Epoch: 052, Train Loss:1.1152, AUC: 0.9084, AP: 0.9240\n",
      "Epoch: 053, Train Loss:1.1148, AUC: 0.9091, AP: 0.9245\n",
      "Epoch: 054, Train Loss:1.1140, AUC: 0.9094, AP: 0.9248\n",
      "Epoch: 055, Train Loss:1.1133, AUC: 0.9095, AP: 0.9249\n",
      "Epoch: 056, Train Loss:1.1128, AUC: 0.9098, AP: 0.9251\n",
      "Epoch: 057, Train Loss:1.1122, AUC: 0.9100, AP: 0.9252\n",
      "Epoch: 058, Train Loss:1.1119, AUC: 0.9101, AP: 0.9253\n",
      "Epoch: 059, Train Loss:1.1113, AUC: 0.9102, AP: 0.9254\n",
      "Epoch: 060, Train Loss:1.1106, AUC: 0.9105, AP: 0.9256\n",
      "Epoch: 061, Train Loss:1.1103, AUC: 0.9111, AP: 0.9261\n",
      "Epoch: 062, Train Loss:1.1094, AUC: 0.9121, AP: 0.9268\n",
      "Epoch: 063, Train Loss:1.1088, AUC: 0.9114, AP: 0.9263\n",
      "Epoch: 064, Train Loss:1.1096, AUC: 0.9124, AP: 0.9271\n",
      "Epoch: 065, Train Loss:1.1083, AUC: 0.9137, AP: 0.9281\n",
      "Epoch: 066, Train Loss:1.1076, AUC: 0.9141, AP: 0.9284\n",
      "Epoch: 067, Train Loss:1.1071, AUC: 0.9142, AP: 0.9285\n",
      "Epoch: 068, Train Loss:1.1065, AUC: 0.9138, AP: 0.9282\n",
      "Epoch: 069, Train Loss:1.1060, AUC: 0.9131, AP: 0.9277\n",
      "Epoch: 070, Train Loss:1.1054, AUC: 0.9130, AP: 0.9276\n",
      "Epoch: 071, Train Loss:1.1055, AUC: 0.9139, AP: 0.9283\n",
      "Epoch: 072, Train Loss:1.1045, AUC: 0.9149, AP: 0.9290\n",
      "Epoch: 073, Train Loss:1.1040, AUC: 0.9153, AP: 0.9293\n",
      "Epoch: 074, Train Loss:1.1034, AUC: 0.9154, AP: 0.9295\n",
      "Epoch: 075, Train Loss:1.1034, AUC: 0.9160, AP: 0.9299\n",
      "Epoch: 076, Train Loss:1.1026, AUC: 0.9164, AP: 0.9302\n",
      "Epoch: 077, Train Loss:1.1019, AUC: 0.9160, AP: 0.9299\n",
      "Epoch: 078, Train Loss:1.1013, AUC: 0.9156, AP: 0.9296\n",
      "Epoch: 079, Train Loss:1.1008, AUC: 0.9161, AP: 0.9299\n",
      "Epoch: 080, Train Loss:1.1004, AUC: 0.9167, AP: 0.9303\n",
      "Epoch: 081, Train Loss:1.1002, AUC: 0.9169, AP: 0.9304\n",
      "Epoch: 082, Train Loss:1.0997, AUC: 0.9174, AP: 0.9308\n",
      "Epoch: 083, Train Loss:1.0992, AUC: 0.9185, AP: 0.9315\n",
      "Epoch: 084, Train Loss:1.0989, AUC: 0.9185, AP: 0.9316\n",
      "Epoch: 085, Train Loss:1.0981, AUC: 0.9182, AP: 0.9313\n",
      "Epoch: 086, Train Loss:1.0979, AUC: 0.9185, AP: 0.9316\n",
      "Epoch: 087, Train Loss:1.0974, AUC: 0.9187, AP: 0.9316\n",
      "Epoch: 088, Train Loss:1.0967, AUC: 0.9186, AP: 0.9315\n",
      "Epoch: 089, Train Loss:1.0966, AUC: 0.9188, AP: 0.9316\n",
      "Epoch: 090, Train Loss:1.0959, AUC: 0.9196, AP: 0.9322\n",
      "Epoch: 091, Train Loss:1.0956, AUC: 0.9198, AP: 0.9323\n",
      "Epoch: 092, Train Loss:1.0951, AUC: 0.9199, AP: 0.9323\n",
      "Epoch: 093, Train Loss:1.0947, AUC: 0.9205, AP: 0.9327\n",
      "Epoch: 094, Train Loss:1.0945, AUC: 0.9204, AP: 0.9326\n",
      "Epoch: 095, Train Loss:1.0941, AUC: 0.9195, AP: 0.9320\n",
      "Epoch: 096, Train Loss:1.0939, AUC: 0.9203, AP: 0.9326\n",
      "Epoch: 097, Train Loss:1.0930, AUC: 0.9206, AP: 0.9327\n",
      "Epoch: 098, Train Loss:1.0925, AUC: 0.9204, AP: 0.9326\n",
      "Epoch: 099, Train Loss:1.0923, AUC: 0.9208, AP: 0.9328\n",
      "Epoch: 100, Train Loss:1.0915, AUC: 0.9213, AP: 0.9331\n",
      "Epoch: 101, Train Loss:1.0914, AUC: 0.9210, AP: 0.9328\n",
      "Epoch: 102, Train Loss:1.0906, AUC: 0.9212, AP: 0.9329\n",
      "Epoch: 103, Train Loss:1.0902, AUC: 0.9216, AP: 0.9332\n",
      "Epoch: 104, Train Loss:1.0898, AUC: 0.9212, AP: 0.9328\n",
      "Epoch: 105, Train Loss:1.0898, AUC: 0.9212, AP: 0.9328\n",
      "Epoch: 106, Train Loss:1.0893, AUC: 0.9220, AP: 0.9333\n",
      "Epoch: 107, Train Loss:1.0891, AUC: 0.9214, AP: 0.9327\n",
      "Epoch: 108, Train Loss:1.0889, AUC: 0.9221, AP: 0.9332\n",
      "Epoch: 109, Train Loss:1.0880, AUC: 0.9224, AP: 0.9333\n",
      "Epoch: 110, Train Loss:1.0875, AUC: 0.9220, AP: 0.9330\n",
      "Epoch: 111, Train Loss:1.0870, AUC: 0.9221, AP: 0.9330\n",
      "Epoch: 112, Train Loss:1.0870, AUC: 0.9215, AP: 0.9325\n",
      "Epoch: 113, Train Loss:1.0858, AUC: 0.9205, AP: 0.9318\n",
      "Epoch: 114, Train Loss:1.0849, AUC: 0.9196, AP: 0.9311\n",
      "Epoch: 115, Train Loss:1.0842, AUC: 0.9195, AP: 0.9309\n",
      "Epoch: 116, Train Loss:1.0834, AUC: 0.9204, AP: 0.9315\n",
      "Epoch: 117, Train Loss:1.0836, AUC: 0.9191, AP: 0.9304\n",
      "Epoch: 118, Train Loss:1.0834, AUC: 0.9209, AP: 0.9319\n",
      "Epoch: 119, Train Loss:1.0832, AUC: 0.9196, AP: 0.9305\n",
      "Epoch: 120, Train Loss:1.0825, AUC: 0.9229, AP: 0.9329\n",
      "Epoch: 121, Train Loss:1.0812, AUC: 0.9236, AP: 0.9334\n",
      "Epoch: 122, Train Loss:1.0809, AUC: 0.9220, AP: 0.9320\n",
      "Epoch: 123, Train Loss:1.0808, AUC: 0.9226, AP: 0.9327\n",
      "Epoch: 124, Train Loss:1.0798, AUC: 0.9227, AP: 0.9330\n",
      "Epoch: 125, Train Loss:1.0796, AUC: 0.9216, AP: 0.9319\n",
      "Epoch: 126, Train Loss:1.0790, AUC: 0.9225, AP: 0.9325\n",
      "Epoch: 127, Train Loss:1.0783, AUC: 0.9239, AP: 0.9333\n",
      "Epoch: 128, Train Loss:1.0778, AUC: 0.9234, AP: 0.9328\n",
      "Epoch: 129, Train Loss:1.0768, AUC: 0.9228, AP: 0.9324\n",
      "Epoch: 130, Train Loss:1.0768, AUC: 0.9226, AP: 0.9325\n",
      "Epoch: 131, Train Loss:1.0760, AUC: 0.9219, AP: 0.9319\n",
      "Epoch: 132, Train Loss:1.0758, AUC: 0.9219, AP: 0.9319\n",
      "Epoch: 133, Train Loss:1.0753, AUC: 0.9229, AP: 0.9326\n",
      "Epoch: 134, Train Loss:1.0746, AUC: 0.9226, AP: 0.9323\n",
      "Epoch: 135, Train Loss:1.0738, AUC: 0.9224, AP: 0.9321\n",
      "Epoch: 136, Train Loss:1.0736, AUC: 0.9230, AP: 0.9326\n",
      "Epoch: 137, Train Loss:1.0734, AUC: 0.9228, AP: 0.9323\n",
      "Epoch: 138, Train Loss:1.0724, AUC: 0.9230, AP: 0.9323\n",
      "Epoch: 139, Train Loss:1.0721, AUC: 0.9235, AP: 0.9327\n",
      "Epoch: 140, Train Loss:1.0716, AUC: 0.9229, AP: 0.9322\n",
      "Epoch: 141, Train Loss:1.0711, AUC: 0.9224, AP: 0.9320\n",
      "Epoch: 142, Train Loss:1.0706, AUC: 0.9226, AP: 0.9323\n",
      "Epoch: 143, Train Loss:1.0703, AUC: 0.9223, AP: 0.9319\n",
      "Epoch: 144, Train Loss:1.0696, AUC: 0.9226, AP: 0.9321\n",
      "Epoch: 145, Train Loss:1.0689, AUC: 0.9229, AP: 0.9322\n",
      "Epoch: 146, Train Loss:1.0687, AUC: 0.9223, AP: 0.9316\n",
      "Epoch: 147, Train Loss:1.0683, AUC: 0.9225, AP: 0.9325\n",
      "Epoch: 148, Train Loss:1.0693, AUC: 0.9195, AP: 0.9303\n",
      "Epoch: 149, Train Loss:1.0708, AUC: 0.9221, AP: 0.9325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Train Loss:1.0700, AUC: 0.9224, AP: 0.9321\n",
      "Epoch: 151, Train Loss:1.0684, AUC: 0.9224, AP: 0.9315\n",
      "Epoch: 152, Train Loss:1.0680, AUC: 0.9229, AP: 0.9320\n",
      "Epoch: 153, Train Loss:1.0720, AUC: 0.9241, AP: 0.9331\n",
      "Epoch: 154, Train Loss:1.0660, AUC: 0.9205, AP: 0.9305\n",
      "Epoch: 155, Train Loss:1.0664, AUC: 0.9218, AP: 0.9321\n",
      "Epoch: 156, Train Loss:1.0676, AUC: 0.9228, AP: 0.9329\n",
      "Epoch: 157, Train Loss:1.0656, AUC: 0.9207, AP: 0.9305\n",
      "Epoch: 158, Train Loss:1.0653, AUC: 0.9223, AP: 0.9318\n",
      "Epoch: 159, Train Loss:1.0655, AUC: 0.9250, AP: 0.9342\n",
      "Epoch: 160, Train Loss:1.0645, AUC: 0.9219, AP: 0.9316\n",
      "Epoch: 161, Train Loss:1.0627, AUC: 0.9219, AP: 0.9315\n",
      "Epoch: 162, Train Loss:1.0629, AUC: 0.9248, AP: 0.9340\n",
      "Epoch: 163, Train Loss:1.0623, AUC: 0.9239, AP: 0.9330\n",
      "Epoch: 164, Train Loss:1.0622, AUC: 0.9221, AP: 0.9315\n",
      "Epoch: 165, Train Loss:1.0616, AUC: 0.9235, AP: 0.9333\n",
      "Epoch: 166, Train Loss:1.0609, AUC: 0.9230, AP: 0.9331\n",
      "Epoch: 167, Train Loss:1.0605, AUC: 0.9221, AP: 0.9320\n",
      "Epoch: 168, Train Loss:1.0609, AUC: 0.9231, AP: 0.9326\n",
      "Epoch: 169, Train Loss:1.0590, AUC: 0.9240, AP: 0.9334\n",
      "Epoch: 170, Train Loss:1.0581, AUC: 0.9222, AP: 0.9322\n",
      "Epoch: 171, Train Loss:1.0608, AUC: 0.9227, AP: 0.9328\n",
      "Epoch: 172, Train Loss:1.0576, AUC: 0.9228, AP: 0.9334\n",
      "Epoch: 173, Train Loss:1.0580, AUC: 0.9227, AP: 0.9331\n",
      "Epoch: 174, Train Loss:1.0568, AUC: 0.9228, AP: 0.9326\n",
      "Epoch: 175, Train Loss:1.0560, AUC: 0.9229, AP: 0.9326\n",
      "Epoch: 176, Train Loss:1.0569, AUC: 0.9246, AP: 0.9340\n",
      "Epoch: 177, Train Loss:1.0565, AUC: 0.9222, AP: 0.9323\n",
      "Epoch: 178, Train Loss:1.0564, AUC: 0.9221, AP: 0.9324\n",
      "Epoch: 179, Train Loss:1.0561, AUC: 0.9244, AP: 0.9344\n",
      "Epoch: 180, Train Loss:1.0561, AUC: 0.9225, AP: 0.9325\n",
      "Epoch: 181, Train Loss:1.0556, AUC: 0.9215, AP: 0.9316\n",
      "Epoch: 182, Train Loss:1.0525, AUC: 0.9244, AP: 0.9346\n",
      "Epoch: 183, Train Loss:1.0553, AUC: 0.9212, AP: 0.9321\n",
      "Epoch: 184, Train Loss:1.0573, AUC: 0.9211, AP: 0.9328\n",
      "Epoch: 185, Train Loss:1.0557, AUC: 0.9202, AP: 0.9321\n",
      "Epoch: 186, Train Loss:1.0558, AUC: 0.9207, AP: 0.9320\n",
      "Epoch: 187, Train Loss:1.0552, AUC: 0.9220, AP: 0.9326\n",
      "Epoch: 188, Train Loss:1.0541, AUC: 0.9237, AP: 0.9340\n",
      "Epoch: 189, Train Loss:1.0514, AUC: 0.9239, AP: 0.9340\n",
      "Epoch: 190, Train Loss:1.0494, AUC: 0.9215, AP: 0.9322\n",
      "Epoch: 191, Train Loss:1.0515, AUC: 0.9222, AP: 0.9338\n",
      "Epoch: 192, Train Loss:1.0532, AUC: 0.9217, AP: 0.9333\n",
      "Epoch: 193, Train Loss:1.0521, AUC: 0.9222, AP: 0.9334\n",
      "Epoch: 194, Train Loss:1.0463, AUC: 0.9224, AP: 0.9343\n",
      "Epoch: 195, Train Loss:1.0504, AUC: 0.9221, AP: 0.9340\n",
      "Epoch: 196, Train Loss:1.0503, AUC: 0.9251, AP: 0.9361\n",
      "Epoch: 197, Train Loss:1.0434, AUC: 0.9242, AP: 0.9358\n",
      "Epoch: 198, Train Loss:1.0461, AUC: 0.9255, AP: 0.9368\n",
      "Epoch: 199, Train Loss:1.0469, AUC: 0.9265, AP: 0.9378\n",
      "Epoch: 200, Train Loss:1.0464, AUC: 0.9254, AP: 0.9365\n",
      "Epoch: 201, Train Loss:1.0447, AUC: 0.9238, AP: 0.9353\n",
      "Epoch: 202, Train Loss:1.0443, AUC: 0.9250, AP: 0.9370\n",
      "Epoch: 203, Train Loss:1.0494, AUC: 0.9224, AP: 0.9349\n",
      "Epoch: 204, Train Loss:1.0449, AUC: 0.9227, AP: 0.9349\n",
      "Epoch: 205, Train Loss:1.0447, AUC: 0.9267, AP: 0.9380\n",
      "Epoch: 206, Train Loss:1.0404, AUC: 0.9263, AP: 0.9378\n",
      "Epoch: 207, Train Loss:1.0425, AUC: 0.9203, AP: 0.9338\n",
      "Epoch: 208, Train Loss:1.0448, AUC: 0.9255, AP: 0.9378\n",
      "Epoch: 209, Train Loss:1.0377, AUC: 0.9219, AP: 0.9352\n",
      "Epoch: 210, Train Loss:1.0425, AUC: 0.9242, AP: 0.9364\n",
      "Epoch: 211, Train Loss:1.0379, AUC: 0.9253, AP: 0.9372\n",
      "Epoch: 212, Train Loss:1.0372, AUC: 0.9249, AP: 0.9368\n",
      "Epoch: 213, Train Loss:1.0439, AUC: 0.9277, AP: 0.9390\n",
      "Epoch: 214, Train Loss:1.0390, AUC: 0.9240, AP: 0.9364\n",
      "Epoch: 215, Train Loss:1.0370, AUC: 0.9224, AP: 0.9353\n",
      "Epoch: 216, Train Loss:1.0403, AUC: 0.9264, AP: 0.9380\n",
      "Epoch: 217, Train Loss:1.0334, AUC: 0.9222, AP: 0.9351\n",
      "Epoch: 218, Train Loss:1.0363, AUC: 0.9246, AP: 0.9367\n",
      "Epoch: 219, Train Loss:1.0336, AUC: 0.9262, AP: 0.9378\n",
      "Epoch: 220, Train Loss:1.0319, AUC: 0.9257, AP: 0.9380\n",
      "Epoch: 221, Train Loss:1.0319, AUC: 0.9259, AP: 0.9383\n",
      "Epoch: 222, Train Loss:1.0321, AUC: 0.9265, AP: 0.9385\n",
      "Epoch: 223, Train Loss:1.0317, AUC: 0.9241, AP: 0.9367\n",
      "Epoch: 224, Train Loss:1.0334, AUC: 0.9285, AP: 0.9397\n",
      "Epoch: 225, Train Loss:1.0289, AUC: 0.9273, AP: 0.9389\n",
      "Epoch: 226, Train Loss:1.0303, AUC: 0.9265, AP: 0.9383\n",
      "Epoch: 227, Train Loss:1.0314, AUC: 0.9278, AP: 0.9394\n",
      "Epoch: 228, Train Loss:1.0284, AUC: 0.9278, AP: 0.9394\n",
      "Epoch: 229, Train Loss:1.0269, AUC: 0.9229, AP: 0.9363\n",
      "Epoch: 230, Train Loss:1.0364, AUC: 0.9257, AP: 0.9382\n",
      "Epoch: 231, Train Loss:1.0292, AUC: 0.9262, AP: 0.9388\n",
      "Epoch: 232, Train Loss:1.0341, AUC: 0.9239, AP: 0.9368\n",
      "Epoch: 233, Train Loss:1.0344, AUC: 0.9255, AP: 0.9380\n",
      "Epoch: 234, Train Loss:1.0315, AUC: 0.9268, AP: 0.9388\n",
      "Epoch: 235, Train Loss:1.0273, AUC: 0.9224, AP: 0.9361\n",
      "Epoch: 236, Train Loss:1.0302, AUC: 0.9271, AP: 0.9390\n",
      "Epoch: 237, Train Loss:1.0269, AUC: 0.9256, AP: 0.9385\n",
      "Epoch: 238, Train Loss:1.0291, AUC: 0.9239, AP: 0.9371\n",
      "Epoch: 239, Train Loss:1.0327, AUC: 0.9279, AP: 0.9399\n",
      "Epoch: 240, Train Loss:1.0244, AUC: 0.9270, AP: 0.9391\n",
      "Epoch: 241, Train Loss:1.0243, AUC: 0.9216, AP: 0.9357\n",
      "Epoch: 242, Train Loss:1.0332, AUC: 0.9255, AP: 0.9382\n",
      "Epoch: 243, Train Loss:1.0246, AUC: 0.9270, AP: 0.9395\n",
      "Epoch: 244, Train Loss:1.0285, AUC: 0.9225, AP: 0.9363\n",
      "Epoch: 245, Train Loss:1.0346, AUC: 0.9245, AP: 0.9381\n",
      "Epoch: 246, Train Loss:1.0348, AUC: 0.9278, AP: 0.9401\n",
      "Epoch: 247, Train Loss:1.0285, AUC: 0.9238, AP: 0.9377\n",
      "Epoch: 248, Train Loss:1.0243, AUC: 0.9277, AP: 0.9396\n",
      "Epoch: 249, Train Loss:1.0291, AUC: 0.9220, AP: 0.9353\n",
      "Epoch: 250, Train Loss:1.0371, AUC: 0.9242, AP: 0.9373\n",
      "Epoch: 251, Train Loss:1.0291, AUC: 0.9271, AP: 0.9393\n",
      "Epoch: 252, Train Loss:1.0249, AUC: 0.9253, AP: 0.9380\n",
      "Epoch: 253, Train Loss:1.0260, AUC: 0.9211, AP: 0.9355\n",
      "Epoch: 254, Train Loss:1.0324, AUC: 0.9267, AP: 0.9394\n",
      "Epoch: 255, Train Loss:1.0311, AUC: 0.9261, AP: 0.9390\n",
      "Epoch: 256, Train Loss:1.0280, AUC: 0.9258, AP: 0.9382\n",
      "Epoch: 257, Train Loss:1.0243, AUC: 0.9237, AP: 0.9369\n",
      "Epoch: 258, Train Loss:1.0214, AUC: 0.9289, AP: 0.9405\n",
      "Epoch: 259, Train Loss:1.0314, AUC: 0.9253, AP: 0.9380\n",
      "Epoch: 260, Train Loss:1.0220, AUC: 0.9261, AP: 0.9386\n",
      "Epoch: 261, Train Loss:1.0196, AUC: 0.9261, AP: 0.9391\n",
      "Epoch: 262, Train Loss:1.0232, AUC: 0.9260, AP: 0.9393\n",
      "Epoch: 263, Train Loss:1.0235, AUC: 0.9262, AP: 0.9392\n",
      "Epoch: 264, Train Loss:1.0190, AUC: 0.9252, AP: 0.9381\n",
      "Epoch: 265, Train Loss:1.0160, AUC: 0.9267, AP: 0.9390\n",
      "Epoch: 266, Train Loss:1.0211, AUC: 0.9268, AP: 0.9392\n",
      "Epoch: 267, Train Loss:1.0208, AUC: 0.9244, AP: 0.9373\n",
      "Epoch: 268, Train Loss:1.0207, AUC: 0.9229, AP: 0.9367\n",
      "Epoch: 269, Train Loss:1.0183, AUC: 0.9245, AP: 0.9380\n",
      "Epoch: 270, Train Loss:1.0161, AUC: 0.9210, AP: 0.9360\n",
      "Epoch: 271, Train Loss:1.0169, AUC: 0.9228, AP: 0.9371\n",
      "Epoch: 272, Train Loss:1.0145, AUC: 0.9272, AP: 0.9399\n",
      "Epoch: 273, Train Loss:1.0191, AUC: 0.9278, AP: 0.9401\n",
      "Epoch: 274, Train Loss:1.0178, AUC: 0.9258, AP: 0.9387\n",
      "Epoch: 275, Train Loss:1.0158, AUC: 0.9176, AP: 0.9331\n",
      "Epoch: 276, Train Loss:1.0194, AUC: 0.9290, AP: 0.9411\n",
      "Epoch: 277, Train Loss:1.0218, AUC: 0.9246, AP: 0.9383\n",
      "Epoch: 278, Train Loss:1.0142, AUC: 0.9277, AP: 0.9400\n",
      "Epoch: 279, Train Loss:1.0120, AUC: 0.9289, AP: 0.9411\n",
      "Epoch: 280, Train Loss:1.0133, AUC: 0.9297, AP: 0.9415\n",
      "Epoch: 281, Train Loss:1.0125, AUC: 0.9245, AP: 0.9383\n",
      "Epoch: 282, Train Loss:1.0109, AUC: 0.9253, AP: 0.9388\n",
      "Epoch: 283, Train Loss:1.0118, AUC: 0.9270, AP: 0.9399\n",
      "Epoch: 284, Train Loss:1.0070, AUC: 0.9264, AP: 0.9396\n",
      "Epoch: 285, Train Loss:1.0122, AUC: 0.9268, AP: 0.9399\n",
      "Epoch: 286, Train Loss:1.0109, AUC: 0.9269, AP: 0.9399\n",
      "Epoch: 287, Train Loss:1.0060, AUC: 0.9228, AP: 0.9373\n",
      "Epoch: 288, Train Loss:1.0120, AUC: 0.9256, AP: 0.9389\n",
      "Epoch: 289, Train Loss:1.0114, AUC: 0.9297, AP: 0.9415\n",
      "Epoch: 290, Train Loss:1.0095, AUC: 0.9282, AP: 0.9405\n",
      "Epoch: 291, Train Loss:1.0102, AUC: 0.9230, AP: 0.9378\n",
      "Epoch: 292, Train Loss:1.0119, AUC: 0.9301, AP: 0.9421\n",
      "Epoch: 293, Train Loss:1.0158, AUC: 0.9285, AP: 0.9409\n",
      "Epoch: 294, Train Loss:1.0086, AUC: 0.9263, AP: 0.9393\n",
      "Epoch: 295, Train Loss:1.0087, AUC: 0.9296, AP: 0.9416\n",
      "Epoch: 296, Train Loss:1.0107, AUC: 0.9264, AP: 0.9397\n",
      "Epoch: 297, Train Loss:1.0069, AUC: 0.9256, AP: 0.9396\n",
      "Epoch: 298, Train Loss:1.0071, AUC: 0.9300, AP: 0.9424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299, Train Loss:1.0133, AUC: 0.9290, AP: 0.9416\n",
      "Epoch: 300, Train Loss:1.0097, AUC: 0.9245, AP: 0.9387\n",
      "Epoch: 301, Train Loss:1.0069, AUC: 0.9283, AP: 0.9409\n",
      "Epoch: 302, Train Loss:1.0099, AUC: 0.9280, AP: 0.9408\n",
      "Epoch: 303, Train Loss:1.0131, AUC: 0.9254, AP: 0.9392\n",
      "Epoch: 304, Train Loss:1.0033, AUC: 0.9247, AP: 0.9387\n",
      "Epoch: 305, Train Loss:1.0088, AUC: 0.9290, AP: 0.9416\n",
      "Epoch: 306, Train Loss:1.0098, AUC: 0.9274, AP: 0.9407\n",
      "Epoch: 307, Train Loss:1.0092, AUC: 0.9289, AP: 0.9415\n",
      "Epoch: 308, Train Loss:1.0063, AUC: 0.9205, AP: 0.9354\n",
      "Epoch: 309, Train Loss:1.0060, AUC: 0.9299, AP: 0.9421\n",
      "Epoch: 310, Train Loss:1.0082, AUC: 0.9288, AP: 0.9414\n",
      "Epoch: 311, Train Loss:1.0069, AUC: 0.9225, AP: 0.9378\n",
      "Epoch: 312, Train Loss:1.0086, AUC: 0.9213, AP: 0.9361\n",
      "Epoch: 313, Train Loss:1.0030, AUC: 0.9314, AP: 0.9434\n",
      "Epoch: 314, Train Loss:1.0164, AUC: 0.9270, AP: 0.9405\n",
      "Epoch: 315, Train Loss:1.0081, AUC: 0.9215, AP: 0.9373\n",
      "Epoch: 316, Train Loss:1.0147, AUC: 0.9310, AP: 0.9427\n",
      "Epoch: 317, Train Loss:1.0059, AUC: 0.9314, AP: 0.9430\n",
      "Epoch: 318, Train Loss:1.0106, AUC: 0.9171, AP: 0.9330\n",
      "Epoch: 319, Train Loss:1.0122, AUC: 0.9217, AP: 0.9370\n",
      "Epoch: 320, Train Loss:1.0116, AUC: 0.9296, AP: 0.9421\n",
      "Epoch: 321, Train Loss:1.0079, AUC: 0.9309, AP: 0.9431\n",
      "Epoch: 322, Train Loss:1.0103, AUC: 0.9258, AP: 0.9397\n",
      "Epoch: 323, Train Loss:1.0073, AUC: 0.9185, AP: 0.9338\n",
      "Epoch: 324, Train Loss:1.0134, AUC: 0.9303, AP: 0.9426\n",
      "Epoch: 325, Train Loss:1.0135, AUC: 0.9309, AP: 0.9430\n",
      "Epoch: 326, Train Loss:1.0143, AUC: 0.9197, AP: 0.9349\n",
      "Epoch: 327, Train Loss:1.0041, AUC: 0.9269, AP: 0.9400\n",
      "Epoch: 328, Train Loss:1.0031, AUC: 0.9275, AP: 0.9408\n",
      "Epoch: 329, Train Loss:1.0182, AUC: 0.9289, AP: 0.9419\n",
      "Epoch: 330, Train Loss:1.0052, AUC: 0.9230, AP: 0.9388\n",
      "Epoch: 331, Train Loss:1.0039, AUC: 0.9268, AP: 0.9409\n",
      "Epoch: 332, Train Loss:0.9991, AUC: 0.9295, AP: 0.9424\n",
      "Epoch: 333, Train Loss:1.0032, AUC: 0.9260, AP: 0.9405\n",
      "Epoch: 334, Train Loss:0.9982, AUC: 0.9278, AP: 0.9412\n",
      "Epoch: 335, Train Loss:0.9985, AUC: 0.9265, AP: 0.9405\n",
      "Epoch: 336, Train Loss:0.9964, AUC: 0.9275, AP: 0.9410\n",
      "Epoch: 337, Train Loss:0.9969, AUC: 0.9265, AP: 0.9405\n",
      "Epoch: 338, Train Loss:0.9978, AUC: 0.9293, AP: 0.9421\n",
      "Epoch: 339, Train Loss:0.9964, AUC: 0.9281, AP: 0.9413\n",
      "Epoch: 340, Train Loss:0.9972, AUC: 0.9235, AP: 0.9387\n",
      "Epoch: 341, Train Loss:1.0008, AUC: 0.9270, AP: 0.9410\n",
      "Epoch: 342, Train Loss:0.9945, AUC: 0.9325, AP: 0.9444\n",
      "Epoch: 343, Train Loss:1.0095, AUC: 0.9223, AP: 0.9370\n",
      "Epoch: 344, Train Loss:0.9983, AUC: 0.9266, AP: 0.9407\n",
      "Epoch: 345, Train Loss:0.9999, AUC: 0.9297, AP: 0.9426\n",
      "Epoch: 346, Train Loss:0.9966, AUC: 0.9278, AP: 0.9416\n",
      "Epoch: 347, Train Loss:0.9960, AUC: 0.9211, AP: 0.9362\n",
      "Epoch: 348, Train Loss:1.0003, AUC: 0.9287, AP: 0.9415\n",
      "Epoch: 349, Train Loss:1.0023, AUC: 0.9296, AP: 0.9421\n",
      "Epoch: 350, Train Loss:1.0001, AUC: 0.9302, AP: 0.9428\n",
      "Epoch: 351, Train Loss:0.9979, AUC: 0.9282, AP: 0.9418\n",
      "Epoch: 352, Train Loss:0.9961, AUC: 0.9240, AP: 0.9398\n",
      "Epoch: 353, Train Loss:0.9961, AUC: 0.9294, AP: 0.9425\n",
      "Epoch: 354, Train Loss:0.9967, AUC: 0.9217, AP: 0.9367\n",
      "Epoch: 355, Train Loss:0.9950, AUC: 0.9294, AP: 0.9425\n",
      "Epoch: 356, Train Loss:0.9977, AUC: 0.9276, AP: 0.9413\n",
      "Epoch: 357, Train Loss:0.9948, AUC: 0.9250, AP: 0.9397\n",
      "Epoch: 358, Train Loss:0.9981, AUC: 0.9287, AP: 0.9418\n",
      "Epoch: 359, Train Loss:0.9971, AUC: 0.9323, AP: 0.9442\n",
      "Epoch: 360, Train Loss:0.9990, AUC: 0.9249, AP: 0.9401\n",
      "Epoch: 361, Train Loss:0.9945, AUC: 0.9256, AP: 0.9402\n",
      "Epoch: 362, Train Loss:0.9947, AUC: 0.9321, AP: 0.9443\n",
      "Epoch: 363, Train Loss:0.9979, AUC: 0.9299, AP: 0.9429\n",
      "Epoch: 364, Train Loss:0.9968, AUC: 0.9200, AP: 0.9356\n",
      "Epoch: 365, Train Loss:1.0016, AUC: 0.9228, AP: 0.9377\n",
      "Epoch: 366, Train Loss:0.9946, AUC: 0.9327, AP: 0.9446\n",
      "Epoch: 367, Train Loss:1.0041, AUC: 0.9305, AP: 0.9429\n",
      "Epoch: 368, Train Loss:0.9944, AUC: 0.9228, AP: 0.9389\n",
      "Epoch: 369, Train Loss:0.9982, AUC: 0.9289, AP: 0.9423\n",
      "Epoch: 370, Train Loss:0.9964, AUC: 0.9301, AP: 0.9430\n",
      "Epoch: 371, Train Loss:0.9995, AUC: 0.9228, AP: 0.9375\n",
      "Epoch: 372, Train Loss:0.9919, AUC: 0.9294, AP: 0.9426\n",
      "Epoch: 373, Train Loss:0.9999, AUC: 0.9296, AP: 0.9427\n",
      "Epoch: 374, Train Loss:1.0009, AUC: 0.9310, AP: 0.9438\n",
      "Epoch: 375, Train Loss:0.9942, AUC: 0.9219, AP: 0.9372\n",
      "Epoch: 376, Train Loss:0.9945, AUC: 0.9281, AP: 0.9421\n",
      "Epoch: 377, Train Loss:0.9950, AUC: 0.9302, AP: 0.9431\n",
      "Epoch: 378, Train Loss:0.9961, AUC: 0.9294, AP: 0.9425\n",
      "Epoch: 379, Train Loss:0.9940, AUC: 0.9309, AP: 0.9437\n",
      "Epoch: 380, Train Loss:0.9933, AUC: 0.9311, AP: 0.9437\n",
      "Epoch: 381, Train Loss:0.9928, AUC: 0.9227, AP: 0.9376\n",
      "Epoch: 382, Train Loss:0.9944, AUC: 0.9284, AP: 0.9422\n",
      "Epoch: 383, Train Loss:0.9925, AUC: 0.9260, AP: 0.9409\n",
      "Epoch: 384, Train Loss:0.9885, AUC: 0.9277, AP: 0.9417\n",
      "Epoch: 385, Train Loss:0.9936, AUC: 0.9293, AP: 0.9427\n",
      "Epoch: 386, Train Loss:0.9948, AUC: 0.9294, AP: 0.9428\n",
      "Epoch: 387, Train Loss:0.9890, AUC: 0.9298, AP: 0.9432\n",
      "Epoch: 388, Train Loss:0.9906, AUC: 0.9277, AP: 0.9419\n",
      "Epoch: 389, Train Loss:0.9914, AUC: 0.9230, AP: 0.9379\n",
      "Epoch: 390, Train Loss:0.9950, AUC: 0.9317, AP: 0.9441\n",
      "Epoch: 391, Train Loss:0.9936, AUC: 0.9276, AP: 0.9417\n",
      "Epoch: 392, Train Loss:0.9914, AUC: 0.9287, AP: 0.9422\n",
      "Epoch: 393, Train Loss:0.9893, AUC: 0.9232, AP: 0.9378\n",
      "Epoch: 394, Train Loss:0.9883, AUC: 0.9317, AP: 0.9441\n",
      "Epoch: 395, Train Loss:0.9971, AUC: 0.9265, AP: 0.9409\n",
      "Epoch: 396, Train Loss:0.9917, AUC: 0.9210, AP: 0.9364\n",
      "Epoch: 397, Train Loss:0.9919, AUC: 0.9279, AP: 0.9424\n",
      "Epoch: 398, Train Loss:0.9868, AUC: 0.9303, AP: 0.9435\n",
      "Epoch: 399, Train Loss:0.9870, AUC: 0.9202, AP: 0.9359\n",
      "Epoch: 400, Train Loss:0.9959, AUC: 0.9254, AP: 0.9405\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(400):\n",
    "    loss = train()\n",
    "    \n",
    "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, Train Loss:{:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch+1, loss, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), pretrain_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
