{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wcks13589/.local/lib/python3.6/site-packages/numba/core/errors.py:154: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GAE, VGAE\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = './data/'\n",
    "chid_dict_file = 'sample_idx_map.npy'\n",
    "cdtx_file = 'sample_zip_if_cca_cdtx0001_hist.csv'\n",
    "cust_f_file = 'sample_zip_if_cca_cust_f.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 (6654938, 10) (1176172, 32)\n"
     ]
    }
   ],
   "source": [
    "idx_map = np.load(os.path.join(sample_path, chid_dict_file), allow_pickle=True).tolist()\n",
    "df_cdtx = pd.read_csv(os.path.join(sample_path, cdtx_file)) # 交易記錄檔\n",
    "df_cust_f = pd.read_csv(os.path.join(sample_path, cust_f_file)) # user feature\n",
    "df_cust_f.drop_duplicates(ignore_index=True, inplace=True)\n",
    "\n",
    "print(len(idx_map), df_cdtx.shape, df_cust_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0491b7c0a71d409e932d459f44e6ba9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "l = len(idx_map)\n",
    "for i, j  in tqdm(enumerate(set(df_cdtx.mcc))):\n",
    "    idx_map[j] = i+l\n",
    "\n",
    "df_cdtx.chid = df_cdtx.chid.map(idx_map)\n",
    "df_cdtx.mcc = df_cdtx.mcc.map(idx_map)\n",
    "\n",
    "df_cust_f.chid = df_cust_f.chid.map(idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = ['masts', 'educd', 'naty', 'trdtp', 'poscd', 'cuorg']\n",
    "\n",
    "numeric_cols = sorted(set(df_cust_f.columns) - set(category_cols) - set(['chid', 'data_ym', 'data_dt']), \n",
    "                      key=list(df_cust_f.columns).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1176172, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chid</th>\n",
       "      <th>data_ym</th>\n",
       "      <th>monin</th>\n",
       "      <th>wrky</th>\n",
       "      <th>first_mob</th>\n",
       "      <th>data_dt</th>\n",
       "      <th>masts</th>\n",
       "      <th>educd</th>\n",
       "      <th>naty</th>\n",
       "      <th>trdtp</th>\n",
       "      <th>...</th>\n",
       "      <th>constant_u2_ind</th>\n",
       "      <th>constant_u3_ind</th>\n",
       "      <th>constant_u4_ind</th>\n",
       "      <th>constant_l2_ind</th>\n",
       "      <th>constant_l3_ind</th>\n",
       "      <th>constant_l4_ind</th>\n",
       "      <th>constant_change</th>\n",
       "      <th>growth_rate</th>\n",
       "      <th>monotone_up</th>\n",
       "      <th>monotone_down</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8477</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>265153</td>\n",
       "      <td>1</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8477</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>265153</td>\n",
       "      <td>1</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   chid     data_ym   monin  wrky  first_mob     data_dt  masts  educd  naty  \\\n",
       "0  8477  2017-12-01  265153     1       94.0  2018-01-01      0      0     0   \n",
       "1  8477  2018-01-01  265153     1       95.0  2018-02-01      0      0     0   \n",
       "\n",
       "   trdtp  ...  constant_u2_ind  constant_u3_ind  constant_u4_ind  \\\n",
       "0      0  ...              0.0              0.0              2.0   \n",
       "1      0  ...              0.0              0.0              1.0   \n",
       "\n",
       "   constant_l2_ind  constant_l3_ind  constant_l4_ind  constant_change  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   growth_rate  monotone_up  monotone_down  \n",
       "0          1.2          2.0            0.0  \n",
       "1          1.1          4.0            0.0  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper = {col: {value: index for index, value in enumerate(df_cust_f[col].unique())} \n",
    "          for col in category_cols}\n",
    "\n",
    "df_cust_f.loc[:,category_cols] = df_cust_f[category_cols].apply(lambda x: x.map(mapper[x.name]))\n",
    "\n",
    "print(df_cust_f.shape)\n",
    "df_cust_f.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_f_pre = df_cust_f[df_cust_f.data_ym > '2017-12-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111718, 2) (111718, 2)\n",
      "(111384, 2) (111384, 2)\n",
      "(108539, 2) (108539, 2)\n",
      "(114384, 2) (114384, 2)\n",
      "(118297, 2) (118297, 2)\n",
      "(112677, 2) (112677, 2)\n",
      "(123385, 2) (123385, 2)\n",
      "(122938, 2) (122938, 2)\n",
      "(118927, 2) (118927, 2)\n",
      "(123481, 2) (123481, 2)\n",
      "(125585, 2) (125585, 2)\n",
      "(134040, 2) (134040, 2)\n",
      "(133746, 2) (133746, 2)\n",
      "(125068, 2) (125068, 2)\n",
      "(129703, 2) (129703, 2)\n",
      "(133705, 2) (133705, 2)\n",
      "(138664, 2) (138664, 2)\n",
      "(136600, 2) (136600, 2)\n",
      "(145034, 2) (145034, 2)\n",
      "(145165, 2) (145165, 2)\n",
      "(141280, 2) (141280, 2)\n",
      "(146976, 2) (146976, 2)\n",
      "(148163, 2) (148163, 2)\n",
      "(151831, 2) (151831, 2)\n",
      "(154337, 2) (154337, 2)\n"
     ]
    }
   ],
   "source": [
    "df_cdtx['month'] = df_cdtx.csmdt.apply(lambda x: x[:-3]+'-01')\n",
    "#df_cust_f_pre['month'] = df_cust_f_pre.data_ym.apply(lambda x: x[:5]+f'{int(x[5:7])-1:02d}'+ x[7:] if x[5:7]!='01' else f'{int(x[:4])-1}'+'-12'+x[7:])\n",
    "\n",
    "def make_edges_symmetry(edge_index):\n",
    "    new_edge = []\n",
    "    for i in edge_index:\n",
    "        new_edge.append(np.array([i[1],i[0]]))\n",
    "    new_edge = np.concatenate([new_edge],0)\n",
    "    print(new_edge.shape, edge_index.shape)\n",
    "    return torch.LongTensor(np.concatenate([edge_index,new_edge], 0).T)\n",
    "\n",
    "edge_dict = {}\n",
    "for i, j  in enumerate(sorted(df_cdtx.month.unique())):\n",
    "    edge_index = df_cdtx[df_cdtx.month==j].iloc[:,[2,5]].drop_duplicates().to_numpy()\n",
    "    edge_index = make_edges_symmetry(edge_index)\n",
    "    \n",
    "    edge_dict[i] = edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 取得整個月的 objam \n",
    "temp_cdtx = df_cdtx.groupby(['chid', 'month']).sum()\n",
    "df_cdtx_objam = pd.DataFrame(list(map(list, temp_cdtx.index)), columns=['chid', 'data_ym'])\n",
    "df_cdtx_objam['objam'] = np.ma.log(temp_cdtx.objam.values).filled(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1129463, 33)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cust_f_pre = df_cust_f_pre.merge(df_cdtx_objam, \n",
    "                                    how='left', \n",
    "                                    left_on=['chid', 'data_ym'], \n",
    "                                    right_on=['chid', 'data_ym']).fillna(0)\n",
    "\n",
    "df_cust_f_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ['data_ym', 'data_dt', 'month'] \n",
      "\n",
      "7 ['chid', 'masts', 'educd', 'naty', 'trdtp', 'poscd', 'cuorg'] \n",
      "\n",
      "24 ['monin', 'wrky', 'first_mob', 'cycam', 'slam', 'sum_area_c', 'sum_u2_ind', 'sum_u3_ind', 'sum_u4_ind', 'sum_l2_ind', 'sum_l3_ind', 'sum_l4_ind', 'constant_area_c', 'constant_u2_ind', 'constant_u3_ind', 'constant_u4_ind', 'constant_l2_ind', 'constant_l3_ind', 'constant_l4_ind', 'constant_change', 'growth_rate', 'monotone_up', 'monotone_down', 'objam']\n"
     ]
    }
   ],
   "source": [
    "ignore_cols = ['data_ym', 'data_dt', 'month']\n",
    "category_cols = ['chid'] + category_cols\n",
    "numeric_cols = sorted(set(df_cust_f_pre.columns) - set(category_cols) - set(ignore_cols), \n",
    "                      key=list(df_cust_f_pre.columns).index)\n",
    "\n",
    "print(len(ignore_cols), ignore_cols, '\\n')\n",
    "print(len(category_cols), category_cols, '\\n')\n",
    "print(len(numeric_cols), numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler()\n",
    "df_cust_f_pre[numeric_cols] = x_scaler.fit_transform(df_cust_f_pre[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature_dict = {}\n",
    "for i, j in enumerate(sorted(df_cust_f_pre.data_ym.unique())):\n",
    "    temp = df_cust_f_pre[df_cust_f_pre.data_ym == j].copy()[category_cols+numeric_cols].to_numpy()\n",
    "    x_feature = np.zeros([len(idx_map), temp.shape[1]])\n",
    "    for k in set(temp[:,0]):\n",
    "        x_feature[int(k)] = temp[np.where(temp[:,0]==k)[0]]\n",
    "\n",
    "    x_feature_dict[i] = torch.Tensor(x_feature[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_index(x, feature_cols):\n",
    "    feature_idx = {}\n",
    "    x_cols = list(x.columns)\n",
    "    for i in feature_cols:\n",
    "        feature_idx[i] = x_cols.index(i)\n",
    "        \n",
    "    return feature_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols.remove('chid')\n",
    "category_dict = feature_index(df_cust_f_pre[category_cols+numeric_cols], category_cols)\n",
    "numeric_dict = feature_index(df_cust_f_pre[category_cols+numeric_cols], numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_chid = sorted(df_cust_f.chid.unique())\n",
    "list_month = sorted(df_cust_f.data_dt.unique())[12:]\n",
    "\n",
    "df_full_y_sum = pd.DataFrame({\n",
    "    'chid': list_chid*len(list_month),\n",
    "}).sort_values(by='chid', ignore_index=True)\n",
    "df_full_y_sum['data_ym'] = list_month*len(list_chid)\n",
    "\n",
    "df_full_y_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## join objam\n",
    "\n",
    "df_full_y_sum = df_full_y_sum.merge(df_cdtx_objam, \n",
    "                                    how='left', \n",
    "                                    left_on=['chid', 'data_ym'], \n",
    "                                    right_on=['chid', 'data_ym']).fillna(0)\n",
    "\n",
    "df_full_y_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_y_sum.sort_values(by=['data_ym','chid'], inplace=True)\n",
    "y_dict = {}\n",
    "for i,j in enumerate(sorted(df_full_y_sum.data_ym.unique())):\n",
    "    temp_y = df_full_y_sum[df_full_y_sum.data_ym == j].copy()\n",
    "    y_dict[i] = torch.from_numpy(temp_y['objam'].to_numpy()).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, layer_dims, category_cols, category_dims, window_size=12):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dict = torch.nn.ModuleDict({category_col:torch.nn.Embedding(category_dim,64)\n",
    "                                                   for category_col, category_dim in zip(category_cols,category_dims)})\n",
    "        self.gcn1_dict = torch.nn.ModuleDict({str(i):GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "                                                   for i in range(window_size)})\n",
    "        self.gcn2_dict = torch.nn.ModuleDict({str(i):GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "                                                   for i in range(window_size)})\n",
    "        \n",
    "        self.rnn = torch.nn.GRU(out_channels, out_channels, 1, batch_first=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x_ = []\n",
    "        numeric_idx = torch.LongTensor(list(numeric_dict.values()))\n",
    "        for i in x:\n",
    "            category_embeddings = [self.embedding_dict[item[0]](i[:,item[1]].long()) for item in category_dict.items()]\n",
    "            category_embeddings = torch.cat(category_embeddings, -1)\n",
    "            x_.append(torch.cat([category_embeddings, i[:,numeric_idx]], -1))\n",
    "        \n",
    "        gcn_embeddings = [self.gcn1_dict[str(i)](x_[i], edge_index[i]).relu() for i in range(self.window_size)]\n",
    "        gcn_embeddings2 = [self.gcn2_dict[str(i)](gcn_embeddings[i], edge_index[i]).unsqueeze(1) for i in range(self.window_size)]\n",
    "        gcn_embeddings2 = torch.cat(gcn_embeddings2, 1)\n",
    "        \n",
    "        _ ,gcn_embeddings2 = self.rnn(gcn_embeddings2)\n",
    "        \n",
    "        return gcn_embeddings2.squeeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dims = [df_cust_f[feat].nunique() for feat in category_cols]\n",
    "\n",
    "embedding_size = 64\n",
    "layer_dims = [256, 128, 1]\n",
    "input_dim = len(category_dict)*64 + len(numeric_dict)\n",
    "\n",
    "epochs = 400\n",
    "batch_size = 2048\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111718, 2) torch.Size([111718, 2])\n",
      "(111384, 2) torch.Size([111384, 2])\n",
      "(108539, 2) torch.Size([108539, 2])\n",
      "(114384, 2) torch.Size([114384, 2])\n",
      "(118297, 2) torch.Size([118297, 2])\n",
      "(112677, 2) torch.Size([112677, 2])\n",
      "(123385, 2) torch.Size([123385, 2])\n",
      "(122938, 2) torch.Size([122938, 2])\n",
      "(118927, 2) torch.Size([118927, 2])\n",
      "(123481, 2) torch.Size([123481, 2])\n",
      "(125585, 2) torch.Size([125585, 2])\n",
      "(134040, 2) torch.Size([134040, 2])\n",
      "(133746, 2) torch.Size([133746, 2])\n",
      "(125068, 2) torch.Size([125068, 2])\n",
      "(129703, 2) torch.Size([129703, 2])\n",
      "(133705, 2) torch.Size([133705, 2])\n",
      "(138664, 2) torch.Size([138664, 2])\n",
      "(136600, 2) torch.Size([136600, 2])\n",
      "(145034, 2) torch.Size([145034, 2])\n",
      "(145165, 2) torch.Size([145165, 2])\n",
      "(141280, 2) torch.Size([141280, 2])\n",
      "(146976, 2) torch.Size([146976, 2])\n",
      "(148163, 2) torch.Size([148163, 2])\n",
      "(151831, 2) torch.Size([151831, 2])\n",
      "(154337, 2) torch.Size([154337, 2])\n"
     ]
    }
   ],
   "source": [
    "def sample_neg_edges(pos_edges, num_nodes, n_user):\n",
    "    row , col = pos_edges\n",
    "    mask = row < col\n",
    "    row, col = row[mask], col[mask]\n",
    "    neg_adj_mask = torch.ones(num_nodes, num_nodes, dtype=torch.uint8)\n",
    "    neg_adj_mask = neg_adj_mask.triu(diagonal=1).to(torch.bool)\n",
    "    neg_adj_mask[row, col] = 0\n",
    "    neg_row, neg_col = neg_adj_mask[:n_user, n_user:].nonzero(as_tuple=False).t()\n",
    "    neg_col = neg_col+n_user\n",
    "    perm = torch.randperm(row.size(0))\n",
    "    neg_row, neg_col = neg_row[perm], neg_col[perm]\n",
    "    neg_edge_index = torch.cat([neg_row.view(1,-1), neg_col.view(1,-1)],0)\n",
    "    \n",
    "    return make_edges_symmetry(neg_edge_index.T)\n",
    "\n",
    "neg_edges_dict = {}\n",
    "for i,j in enumerate(edge_dict.values()):\n",
    "    neg_edges_dict[i] = sample_neg_edges(j, 50502, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = GAE(Encoder(input_dim, embedding_size, layer_dims, category_cols, category_dims)).to(device)\n",
    "x_feature = [i.float().to(device) for i in x_feature_dict.values()]\n",
    "pos_edge_index = [i.to(device) for i in edge_dict.values()]\n",
    "neg_edges_index = [i.to(device) for i in neg_edges_dict.values()]\n",
    "y = [i.to(device) for i in y_dict.values()]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    loss_ = 0\n",
    "    for i in range(9):\n",
    "        optimizer.zero_grad()\n",
    "        x = x_feature[i:i+12]\n",
    "        z = model.encode(x, pos_edge_index[i:i+12])\n",
    "        loss = model.recon_loss(z, pos_edge_index[i+12], neg_edges_index[i+12])\n",
    "        loss_ += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_/9\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    auc , ap = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(9):\n",
    "            x = x_feature[i:i+12]\n",
    "            z = model.encode(x, pos_edge_index[i:i+12])\n",
    "            auc_, ap_ = model.test(z, pos_edge_index[i+12], neg_edge_index[i+12])\n",
    "            auc += auc_\n",
    "            ap += ap_\n",
    "    return auc/9, ap/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.8995879292488098\n",
      "2 0.8900182313389249\n",
      "3 0.5591222180260552\n",
      "4 0.5036832491556803\n",
      "5 0.4693643848101298\n",
      "6 0.44019139144155717\n",
      "7 0.41778499881426495\n",
      "8 0.40596484806802535\n",
      "9 0.38299890359242755\n",
      "10 0.45180339614550274\n",
      "11 0.5066078735722436\n",
      "12 0.42458731267187333\n",
      "13 0.37789811028374565\n",
      "14 0.3484479652510749\n",
      "15 0.3192361427678002\n",
      "16 0.32598010036680436\n",
      "17 0.30375392238299054\n",
      "18 0.2830920186307695\n",
      "19 0.274363163444731\n",
      "20 0.25992855429649353\n",
      "21 0.2620842622386085\n",
      "22 0.23225410448180306\n",
      "23 0.2056346899933285\n",
      "24 0.30887554420365226\n",
      "25 0.24378892613781822\n",
      "26 0.20992038481765324\n",
      "27 0.18104298578368294\n",
      "28 0.20112165974246132\n",
      "29 0.16838762329684365\n",
      "30 0.16405368347962698\n",
      "31 0.14663105871942309\n",
      "32 0.14824527584844166\n",
      "33 0.1269316921631495\n",
      "34 0.19634524981180826\n",
      "35 0.4488311145040724\n",
      "36 0.22234952615367043\n",
      "37 0.18042585915989345\n",
      "38 0.14279177288214365\n",
      "39 0.13157794127861658\n",
      "40 0.11196287473042806\n",
      "41 0.16840918196572197\n",
      "42 0.1386580409275161\n",
      "43 0.11131610307428572\n",
      "44 0.09601622902684742\n",
      "45 0.08694332175784641\n",
      "46 0.08052356044451396\n",
      "47 0.08446298042933147\n",
      "48 0.0832375329401758\n",
      "49 0.08067983057763842\n",
      "50 0.09250146316157447\n",
      "51 0.07895009964704514\n",
      "52 0.06991369649767876\n",
      "53 0.06660917939411269\n",
      "54 0.0861934224764506\n",
      "55 0.07592981805404027\n",
      "56 0.06361681636836794\n",
      "57 0.0613292840619882\n",
      "58 0.056801574097739324\n",
      "59 0.052675988939073354\n",
      "60 0.04861370763844914\n",
      "61 0.06331031314200825\n",
      "62 0.05830246789587869\n",
      "63 0.10757794313960606\n",
      "64 0.09122452305422889\n",
      "65 0.06755128668414222\n",
      "66 0.05622188953889741\n",
      "67 0.04948468506336212\n",
      "68 0.045503744648562536\n",
      "69 0.04281653215487798\n",
      "70 0.043240836097134486\n",
      "71 0.07511896557278103\n",
      "72 0.0934345672527949\n",
      "73 0.07494337194495732\n",
      "74 0.05804205727246073\n",
      "75 0.048237495952182345\n",
      "76 0.042575653642416\n",
      "77 0.03998673417501979\n",
      "78 0.03790315447582139\n",
      "79 0.03615368033448855\n",
      "80 0.035060789022180766\n",
      "81 0.03613693556851811\n",
      "82 0.05371607467532158\n",
      "83 0.04823383647534582\n",
      "84 0.04073019988007016\n",
      "85 0.03799565012256304\n",
      "86 0.03506332553095288\n",
      "87 0.03366998996999529\n",
      "88 0.03343595564365387\n",
      "89 0.031873736530542374\n",
      "90 0.031490452794565096\n",
      "91 0.031224792616234884\n",
      "92 0.03781369990772671\n",
      "93 0.13350962516334322\n",
      "94 0.08073978539970186\n",
      "95 0.060973031653298274\n",
      "96 0.04767353253232108\n",
      "97 0.04052189075284534\n",
      "98 0.03736219513747427\n",
      "99 0.035410878558953605\n",
      "100 0.034194486836592354\n",
      "101 0.03511655579010645\n",
      "102 0.03285659601291021\n",
      "103 0.032716246942679085\n",
      "104 0.031970111860169306\n",
      "105 0.03223169098297755\n",
      "106 0.032815339871578746\n",
      "107 0.03309550570944945\n",
      "108 0.03176778948141469\n",
      "109 0.02946948115196493\n",
      "110 0.02870662199954192\n",
      "111 0.028477884208162624\n",
      "112 0.028363528557949595\n",
      "113 0.027775438088509772\n",
      "114 0.028139314303795498\n",
      "115 0.029902059791816607\n",
      "116 0.06921334813038509\n",
      "117 0.0536223521663083\n",
      "118 0.04179101934035619\n",
      "119 0.03562010866072443\n",
      "120 0.030589073689447507\n",
      "121 0.02916516736149788\n",
      "122 0.027426379836267896\n",
      "123 0.026991056071387395\n",
      "124 0.02752255979511473\n",
      "125 0.027008218069871266\n",
      "126 0.02589750538269679\n",
      "127 0.025486187181539006\n",
      "128 0.024399835823310748\n",
      "129 0.024543748340672918\n",
      "130 0.024649645512302715\n",
      "131 0.025363401613301702\n",
      "132 0.025686259277992778\n",
      "133 0.025924369485841856\n",
      "134 0.02526012145810657\n",
      "135 0.02518020590974225\n",
      "136 0.02479699295428064\n",
      "137 0.02436832007434633\n",
      "138 0.023484364358915225\n",
      "139 0.024954742648535304\n",
      "140 0.023692438585890666\n",
      "141 0.023906935213340655\n",
      "142 0.024662252515554428\n",
      "143 0.02367273862991068\n",
      "144 0.023918787017464638\n",
      "145 0.025419588718149397\n",
      "146 0.024391369687186346\n",
      "147 0.02349768868750996\n",
      "148 0.02245390001270506\n",
      "149 0.02255787265797456\n",
      "150 0.02293683195279704\n",
      "151 0.022768701737125713\n",
      "152 0.023155952493349712\n",
      "153 0.02343801615966691\n",
      "154 0.02308066085808807\n",
      "155 0.02212602210541566\n",
      "156 0.02242334559559822\n",
      "157 0.02199162936045064\n",
      "158 0.02257596639295419\n",
      "159 0.023425809625122283\n",
      "160 0.02291251305076811\n",
      "161 0.022504992369148467\n",
      "162 0.02321296909617053\n",
      "163 0.021657877291242283\n",
      "164 0.021663592300481267\n",
      "165 0.021617618078986805\n",
      "166 0.022141120293074183\n",
      "167 0.02162212857769595\n",
      "168 0.021771346322364278\n",
      "169 0.02135840927561124\n",
      "170 0.021092876378032897\n",
      "171 0.021051592917905912\n",
      "172 0.02139687372578515\n",
      "173 0.021808203102813825\n",
      "174 0.02148935033215417\n",
      "175 0.021901673533850245\n",
      "176 0.023917119950056076\n",
      "177 0.023855669010016654\n",
      "178 0.022292058914899826\n",
      "179 0.022464462866385777\n",
      "180 0.022894021951489978\n",
      "181 0.021658208014236555\n",
      "182 0.024321340231431857\n",
      "183 0.20189672956864038\n",
      "184 1.542851620250278\n",
      "185 1.2478878895441692\n",
      "186 0.44676387641164994\n",
      "187 0.29956725239753723\n",
      "188 0.2593623184495502\n",
      "189 0.23754712442557016\n",
      "190 0.21704109344217512\n",
      "191 0.1562571277221044\n",
      "192 0.19829185803731283\n",
      "193 0.16215701897939047\n",
      "194 0.12939892295334074\n",
      "195 0.10434372557534112\n",
      "196 0.08759334600634044\n",
      "197 0.0828857496380806\n",
      "198 0.08726920270257527\n",
      "199 0.07694279816415575\n",
      "200 0.15519459462828106\n",
      "201 0.09838226437568665\n",
      "202 0.07579445011085933\n",
      "203 0.061298380295435585\n",
      "204 0.051939508981174894\n",
      "205 0.0459506292310026\n",
      "206 0.044816085033946566\n",
      "207 0.04420735935370127\n",
      "208 0.04010456634892358\n",
      "209 0.0388165737191836\n",
      "210 0.035309244568149246\n",
      "211 0.03455007034871313\n",
      "212 0.04033133159908983\n",
      "213 1.8645762238237593\n",
      "214 0.4605143798722161\n",
      "215 0.33124442564116585\n",
      "216 0.2734851903385586\n",
      "217 0.2371817727883657\n",
      "218 0.20553490850660536\n",
      "219 0.18381049070093367\n",
      "220 0.1647658728890949\n",
      "221 0.15254426995913187\n",
      "222 0.13516143792205387\n",
      "223 0.1227931785914633\n",
      "224 0.1133624157971806\n",
      "225 0.10951404025157292\n",
      "226 0.09850630660851796\n",
      "227 0.09007045129934947\n",
      "228 0.07964279833767149\n",
      "229 0.07038771112759908\n",
      "230 0.6759859273831049\n",
      "231 0.3037254247400496\n",
      "232 0.22869332962565952\n",
      "233 0.17052933077017465\n",
      "234 0.14652126530806223\n",
      "235 0.12465613004234102\n",
      "236 0.11051399923033184\n",
      "237 0.10223085681597392\n",
      "238 0.08958256327443653\n",
      "239 0.08201676027642356\n",
      "240 0.07767308420605129\n",
      "241 0.07322763444648848\n",
      "242 0.06933152510060205\n",
      "243 0.07075587204760975\n",
      "244 0.06277745796574487\n",
      "245 0.06093842453426785\n",
      "246 0.06051084275046984\n",
      "247 0.05522606852981779\n",
      "248 0.049127226488457784\n",
      "249 0.05290319605006112\n",
      "250 0.05090638995170593\n",
      "251 0.04476798325777054\n",
      "252 0.044164764798349805\n",
      "253 0.042619061552815966\n",
      "254 0.03949075399173631\n",
      "255 0.037203638090027705\n",
      "256 0.036618864370716944\n",
      "257 0.03600049060251978\n",
      "258 0.03490451764729288\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1e8d550a4970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-9600dc40be25>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_feature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_edge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecon_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_edge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_edges_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch_geometric/nn/models/autoencoder.py\u001b[0m in \u001b[0;36mrecon_loss\u001b[0;34m(self, z, pos_edge_index, neg_edge_index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Do not include self-loops in negative samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mpos_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_self_loops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_edge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mpos_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_self_loops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_edge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneg_edge_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch_geometric/utils/loop.py\u001b[0m in \u001b[0;36mremove_self_loops\u001b[0;34m(edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 400 + 1):\n",
    "    loss = train()\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), 'Dynamic_GCNEncoder_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DownStream Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_index(x, feature_cols):\n",
    "    feature_idx = {}\n",
    "    x_cols = list(x.columns)\n",
    "    for i in feature_cols:\n",
    "        feature_idx[i] = x_cols.index(i)\n",
    "        \n",
    "    return feature_idx\n",
    "\n",
    "def Linear_block(in_dim, out_dim):\n",
    "    block = torch.nn.Sequential(torch.nn.Linear(in_dim, out_dim),\n",
    "                                torch.nn.ReLU())\n",
    "    return block\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, category_cols, category_dims, category_dict, numeric_dict, input_dim, layer_dims, embedding_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.category_dict = category_dict\n",
    "        self.numeric_dict = numeric_dict\n",
    "        \n",
    "        self.out_dims = [input_dim+256, *layer_dims]\n",
    "        Linear_blokcs = [Linear_block(in_dim, out_dim)\n",
    "                         for in_dim, out_dim in zip(self.out_dims, self.out_dims[1:])]\n",
    "        self.model = torch.nn.Sequential(*Linear_blokcs)\n",
    "        self.embedding_dict = torch.nn.ModuleDict({category_col:torch.nn.Embedding(category_dim,\n",
    "                                                                                   embedding_dim)\n",
    "                                                   for category_col, category_dim in zip(category_cols,category_dims)})\n",
    "\n",
    "        self.pretrain_model = Encoder(input_dim, embedding_dim, layer_dims, category_cols, category_dims)\n",
    "        self.pretrain_model.load_state_dict(torch.load('Dynamic_GCNEncoder_2'))\n",
    "        self.pretrain_model.train()\n",
    "\n",
    "        \n",
    "    def forward(self, x, x_, edge_index, index):\n",
    "        \n",
    "        z = self.pretrain_model(x, edge_index)[index:index+1000]\n",
    "        category_embeddings = [self.pretrain_model.embedding_dict[item[0]](x_[:,item[1]].long()) for item in self.category_dict.items()]\n",
    "        category_embeddings = torch.cat(category_embeddings, -1)\n",
    "        \n",
    "        numeric_idx = torch.Tensor(list(self.numeric_dict.values())).long()\n",
    "        \n",
    "        x = torch.cat([z,category_embeddings, x_[:,numeric_idx]], -1)\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLP(category_cols, category_dims, category_dict, numeric_dict, input_dim, layer_dims, embedding_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df390607a79f4212b4cfca710d5132ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "train loss:397067,test loss:418770\n",
      "train MAE(mean):62938,test MAE(mean):65473\n",
      "train MAE(median):12198, test MAE(median):12377\n",
      "\tBetter!\n",
      "epoch:1\n",
      "train loss:510965,test loss:416258\n",
      "train MAE(mean):59568,test MAE(mean):65133\n",
      "train MAE(median):11148, test MAE(median):11135\n",
      "\tBetter!\n",
      "epoch:2\n",
      "train loss:709982,test loss:419378\n",
      "train MAE(mean):59856,test MAE(mean):65298\n",
      "train MAE(median):10969, test MAE(median):11501\n",
      "epoch:3\n",
      "train loss:3277633,test loss:426143\n",
      "train MAE(mean):63774,test MAE(mean):63555\n",
      "train MAE(median):10963, test MAE(median):11377\n",
      "epoch:4\n",
      "train loss:3047643,test loss:412322\n",
      "train MAE(mean):63905,test MAE(mean):63243\n",
      "train MAE(median):10874, test MAE(median):10933\n",
      "\tBetter!\n",
      "epoch:5\n",
      "train loss:4730434,test loss:422308\n",
      "train MAE(mean):66198,test MAE(mean):63231\n",
      "train MAE(median):10898, test MAE(median):11255\n",
      "epoch:6\n",
      "train loss:5032283,test loss:409894\n",
      "train MAE(mean):67540,test MAE(mean):65072\n",
      "train MAE(median):10874, test MAE(median):11227\n",
      "\tBetter!\n",
      "epoch:7\n",
      "train loss:8036836,test loss:397848\n",
      "train MAE(mean):70617,test MAE(mean):63117\n",
      "train MAE(median):10860, test MAE(median):11072\n",
      "\tBetter!\n",
      "epoch:8\n",
      "train loss:7583929,test loss:396254\n",
      "train MAE(mean):70112,test MAE(mean):63508\n",
      "train MAE(median):10915, test MAE(median):11175\n",
      "\tBetter!\n",
      "epoch:9\n",
      "train loss:1532686,test loss:401938\n",
      "train MAE(mean):60752,test MAE(mean):62413\n",
      "train MAE(median):10906, test MAE(median):11292\n",
      "epoch:10\n",
      "train loss:1765174,test loss:394276\n",
      "train MAE(mean):61481,test MAE(mean):62796\n",
      "train MAE(median):10835, test MAE(median):11280\n",
      "\tBetter!\n",
      "epoch:11\n",
      "train loss:1660276,test loss:394910\n",
      "train MAE(mean):60985,test MAE(mean):62911\n",
      "train MAE(median):10901, test MAE(median):11034\n",
      "epoch:12\n",
      "train loss:968829,test loss:413535\n",
      "train MAE(mean):59430,test MAE(mean):63636\n",
      "train MAE(median):10861, test MAE(median):11291\n",
      "epoch:13\n",
      "train loss:794178,test loss:405559\n",
      "train MAE(mean):59492,test MAE(mean):62441\n",
      "train MAE(median):10835, test MAE(median):10899\n",
      "epoch:14\n",
      "train loss:552756,test loss:400539\n",
      "train MAE(mean):58757,test MAE(mean):64711\n",
      "train MAE(median):10794, test MAE(median):11458\n",
      "epoch:15\n",
      "train loss:490192,test loss:400620\n",
      "train MAE(mean):58578,test MAE(mean):63723\n",
      "train MAE(median):10865, test MAE(median):11094\n",
      "epoch:16\n",
      "train loss:426795,test loss:430821\n",
      "train MAE(mean):58182,test MAE(mean):63625\n",
      "train MAE(median):10848, test MAE(median):10958\n",
      "epoch:17\n",
      "train loss:406378,test loss:456701\n",
      "train MAE(mean):57931,test MAE(mean):63307\n",
      "train MAE(median):10812, test MAE(median):10960\n",
      "epoch:18\n",
      "train loss:399430,test loss:405251\n",
      "train MAE(mean):57949,test MAE(mean):63108\n",
      "train MAE(median):10848, test MAE(median):11012\n",
      "epoch:19\n",
      "train loss:401425,test loss:401916\n",
      "train MAE(mean):57814,test MAE(mean):63667\n",
      "train MAE(median):10822, test MAE(median):11347\n",
      "epoch:20\n",
      "train loss:419916,test loss:414679\n",
      "train MAE(mean):58037,test MAE(mean):63666\n",
      "train MAE(median):10783, test MAE(median):11039\n",
      "epoch:21\n",
      "train loss:398377,test loss:417492\n",
      "train MAE(mean):57770,test MAE(mean):63055\n",
      "train MAE(median):10778, test MAE(median):11193\n",
      "epoch:22\n",
      "train loss:425897,test loss:420101\n",
      "train MAE(mean):58093,test MAE(mean):62575\n",
      "train MAE(median):10780, test MAE(median):11004\n",
      "epoch:23\n",
      "train loss:401225,test loss:394997\n",
      "train MAE(mean):57561,test MAE(mean):63489\n",
      "train MAE(median):10798, test MAE(median):11436\n",
      "epoch:24\n",
      "train loss:395421,test loss:464070\n",
      "train MAE(mean):57480,test MAE(mean):63333\n",
      "train MAE(median):10784, test MAE(median):11308\n",
      "epoch:25\n",
      "train loss:415485,test loss:401230\n",
      "train MAE(mean):57649,test MAE(mean):62523\n",
      "train MAE(median):10843, test MAE(median):11077\n",
      "epoch:26\n",
      "train loss:397202,test loss:398225\n",
      "train MAE(mean):57410,test MAE(mean):63098\n",
      "train MAE(median):10775, test MAE(median):11191\n",
      "epoch:27\n",
      "train loss:412615,test loss:392878\n",
      "train MAE(mean):57362,test MAE(mean):62595\n",
      "train MAE(median):10819, test MAE(median):11156\n",
      "\tBetter!\n",
      "epoch:28\n",
      "train loss:395264,test loss:394736\n",
      "train MAE(mean):57254,test MAE(mean):62326\n",
      "train MAE(median):10801, test MAE(median):11327\n",
      "epoch:29\n",
      "train loss:390977,test loss:395486\n",
      "train MAE(mean):57033,test MAE(mean):62587\n",
      "train MAE(median):10802, test MAE(median):11314\n",
      "epoch:30\n",
      "train loss:389256,test loss:392843\n",
      "train MAE(mean):56957,test MAE(mean):61977\n",
      "train MAE(median):10816, test MAE(median):10922\n",
      "\tBetter!\n",
      "epoch:31\n",
      "train loss:399922,test loss:393898\n",
      "train MAE(mean):57106,test MAE(mean):62366\n",
      "train MAE(median):10761, test MAE(median):11169\n",
      "epoch:32\n",
      "train loss:397917,test loss:389490\n",
      "train MAE(mean):56976,test MAE(mean):61929\n",
      "train MAE(median):10828, test MAE(median):11112\n",
      "\tBetter!\n",
      "epoch:33\n",
      "train loss:399314,test loss:397483\n",
      "train MAE(mean):56935,test MAE(mean):62379\n",
      "train MAE(median):10792, test MAE(median):11109\n",
      "epoch:34\n",
      "train loss:396195,test loss:398911\n",
      "train MAE(mean):56848,test MAE(mean):62884\n",
      "train MAE(median):10784, test MAE(median):11117\n",
      "epoch:35\n",
      "train loss:397779,test loss:390691\n",
      "train MAE(mean):56905,test MAE(mean):61321\n",
      "train MAE(median):10823, test MAE(median):10907\n",
      "epoch:36\n",
      "train loss:388445,test loss:390055\n",
      "train MAE(mean):56682,test MAE(mean):62663\n",
      "train MAE(median):10805, test MAE(median):11444\n",
      "epoch:37\n",
      "train loss:395078,test loss:395542\n",
      "train MAE(mean):56741,test MAE(mean):62159\n",
      "train MAE(median):10784, test MAE(median):11074\n",
      "epoch:38\n",
      "train loss:389979,test loss:397162\n",
      "train MAE(mean):56594,test MAE(mean):62910\n",
      "train MAE(median):10777, test MAE(median):11004\n",
      "epoch:39\n",
      "train loss:396089,test loss:397019\n",
      "train MAE(mean):56753,test MAE(mean):61557\n",
      "train MAE(median):10780, test MAE(median):11370\n",
      "epoch:40\n",
      "train loss:392131,test loss:395888\n",
      "train MAE(mean):56629,test MAE(mean):61787\n",
      "train MAE(median):10817, test MAE(median):11272\n",
      "epoch:41\n",
      "train loss:389330,test loss:394945\n",
      "train MAE(mean):56593,test MAE(mean):61930\n",
      "train MAE(median):10784, test MAE(median):11224\n",
      "epoch:42\n",
      "train loss:394329,test loss:391732\n",
      "train MAE(mean):56715,test MAE(mean):62290\n",
      "train MAE(median):10792, test MAE(median):11064\n",
      "epoch:43\n",
      "train loss:396714,test loss:389531\n",
      "train MAE(mean):56625,test MAE(mean):61742\n",
      "train MAE(median):10807, test MAE(median):11347\n",
      "epoch:44\n",
      "train loss:399332,test loss:391249\n",
      "train MAE(mean):56706,test MAE(mean):61525\n",
      "train MAE(median):10767, test MAE(median):11037\n",
      "epoch:45\n",
      "train loss:393359,test loss:396174\n",
      "train MAE(mean):56585,test MAE(mean):63654\n",
      "train MAE(median):10757, test MAE(median):11426\n",
      "epoch:46\n",
      "train loss:397330,test loss:394459\n",
      "train MAE(mean):56624,test MAE(mean):61842\n",
      "train MAE(median):10773, test MAE(median):11130\n",
      "epoch:47\n",
      "train loss:421472,test loss:390483\n",
      "train MAE(mean):56843,test MAE(mean):61880\n",
      "train MAE(median):10764, test MAE(median):11156\n",
      "epoch:48\n",
      "train loss:413238,test loss:393205\n",
      "train MAE(mean):56847,test MAE(mean):61430\n",
      "train MAE(median):10752, test MAE(median):11522\n",
      "epoch:49\n",
      "train loss:403081,test loss:396936\n",
      "train MAE(mean):56706,test MAE(mean):61726\n",
      "train MAE(median):10769, test MAE(median):11176\n",
      "epoch:50\n",
      "train loss:398125,test loss:396319\n",
      "train MAE(mean):56620,test MAE(mean):61581\n",
      "train MAE(median):10795, test MAE(median):11247\n",
      "epoch:51\n",
      "train loss:394428,test loss:391563\n",
      "train MAE(mean):56683,test MAE(mean):61419\n",
      "train MAE(median):10808, test MAE(median):11599\n",
      "epoch:52\n",
      "train loss:427154,test loss:396978\n",
      "train MAE(mean):56940,test MAE(mean):61429\n",
      "train MAE(median):10775, test MAE(median):11760\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = 20\n",
    "\n",
    "best_loss = 1e10\n",
    "early_cnt = 0\n",
    "RMSE = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_output = np.array([])\n",
    "    train_y = np.array([])\n",
    "    test_output = np.array([])\n",
    "    test_y = np.array([])\n",
    "    \n",
    "    for i in range(1,10):\n",
    "        model.train()\n",
    "        \n",
    "        x = x_feature[i:i+12]\n",
    "        train_dataset = TensorDataset(x[-1][:50000],y[i])\n",
    "        train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=1000)\n",
    "        for index , (j, k) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, j, pos_edge_index[i:i+12], index)\n",
    "            loss = criterion(output, k)\n",
    "            train_loss += loss.item()\n",
    "            train_output = np.concatenate([train_output,output.cpu().detach().numpy().reshape(-1)])\n",
    "            train_y = np.concatenate([train_y,k.cpu().detach().numpy().reshape(-1)])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    for i in range(10,12):\n",
    "\n",
    "        model.eval()        \n",
    "        x = x_feature[i:i+12]\n",
    "        test_dataset = TensorDataset(x[-1][:50000],y[i])\n",
    "        test_loader = DataLoader(dataset=test_dataset, shuffle=True, batch_size=1000)\n",
    "        for index , (j, k) in enumerate(test_loader):\n",
    "            \n",
    "            output = model(x, j, pos_edge_index[i:i+12], index)\n",
    "            loss = criterion(output, k)\n",
    "            test_loss += loss.item()\n",
    "            test_output = np.concatenate([test_output,output.cpu().detach().numpy().reshape(-1)])\n",
    "            test_y = np.concatenate([test_y,k.cpu().detach().numpy().reshape(-1)])\n",
    "\n",
    "    #train_loss = np.sqrt(train_loss/len(train_loader))\n",
    "    #test_loss = np.sqrt(test_loss/len(test_loader))\n",
    "    \n",
    "    train_output, train_y = np.e**train_output, np.e**train_y\n",
    "    train_RMSE = mean_squared_error(train_output, train_y, squared=False)\n",
    "    train_mean = mean_absolute_error(train_output, train_y)\n",
    "    train_median = median_absolute_error(train_output, train_y)\n",
    "    \n",
    "    test_output, test_y = np.e**test_output, np.e**test_y\n",
    "    test_RMSE = mean_squared_error(test_output, test_y, squared=False)\n",
    "    test_mean = mean_absolute_error(test_output, test_y)\n",
    "    test_median = median_absolute_error(test_output, test_y)\n",
    "    \n",
    "    print(f'epoch:{epoch}\\ntrain loss:{train_RMSE:.0f},test loss:{test_RMSE:.0f}\\ntrain MAE(mean):{train_mean:.0f},test MAE(mean):{test_mean:.0f}\\ntrain MAE(median):{train_median:.0f}, test MAE(median):{test_median:.0f}')\n",
    "    \n",
    "    if test_RMSE <= best_loss:\n",
    "        best_model_params = copy.deepcopy(model.state_dict())\n",
    "        best_loss = test_RMSE\n",
    "        print('\\tBetter!')\n",
    "        early_cnt = 0\n",
    "    else:\n",
    "        early_cnt += 1\n",
    "    \n",
    "    if early_cnt >= early_stop:\n",
    "        break\n",
    "\n",
    "model.load_state_dict(best_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = np.array([])\n",
    "train_y = np.array([])\n",
    "test_output = np.array([])\n",
    "test_y = np.array([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i in range(1,10):\n",
    "    x = x_feature[i:i+12]\n",
    "    train_dataset = TensorDataset(x[-1][:50000],y[i])\n",
    "    train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=1000)\n",
    "    for index , (j, k) in enumerate(train_loader):\n",
    "        output = model(x, j, pos_edge_index[i:i+12], index)\n",
    "        loss = criterion(output, k)\n",
    "        train_loss += loss.item()\n",
    "        train_output = np.concatenate([train_output,output.cpu().detach().numpy().reshape(-1)])\n",
    "        train_y = np.concatenate([train_y,k.cpu().detach().numpy().reshape(-1)])\n",
    "\n",
    "        \n",
    "for i in range(10,12):     \n",
    "    x = x_feature[i:i+12]\n",
    "    test_dataset = TensorDataset(x[-1][:50000],y[i])\n",
    "    test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=1000)\n",
    "    for index , (j, k) in enumerate(test_loader):\n",
    "        output = model(x, j, pos_edge_index[i:i+12], index)\n",
    "        loss = criterion(output, k)\n",
    "        test_loss += loss.item()\n",
    "        test_output = np.concatenate([test_output,output.cpu().detach().numpy().reshape(-1)])\n",
    "        test_y = np.concatenate([test_y,k.cpu().detach().numpy().reshape(-1)])\n",
    "\n",
    "\n",
    "\n",
    "train_output, train_y = np.e**train_output, np.e**train_y\n",
    "test_output, test_y = np.e**test_output, np.e**test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\tRMSE: 392762 MAE(mean): 56391 MAE(median): 10772\n",
      "test\tRMSE: 389490 MAE(mean): 61931 MAE(median): 11111\n"
     ]
    }
   ],
   "source": [
    "print('train\\tRMSE: {:.0f} MAE(mean): {:.0f} MAE(median): {:.0f}'.format(\n",
    "    mean_squared_error(train_y, train_output, squared=False), \n",
    "    mean_absolute_error(train_y, train_output), \n",
    "    median_absolute_error(train_y, train_output)\n",
    "))\n",
    "print('test\\tRMSE: {:.0f} MAE(mean): {:.0f} MAE(median): {:.0f}'.format(\n",
    "    mean_squared_error(test_y, test_output, squared=False), \n",
    "    mean_absolute_error(test_y, test_output), \n",
    "    median_absolute_error(test_y, test_output)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_full_y_sum[['chid']].iloc[-100000:].copy()\n",
    "df_out['true'] = test_y\n",
    "df_out['pred'] = test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('GCN+RNN_output.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
