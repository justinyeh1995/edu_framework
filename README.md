# ç°¡ä»‹: 

æ­¤ç¨‹å¼æ¡†æ¶çš„ç”¨é€”æ˜¯å¹«åŠ©å¤šä»»å‹™å¯¦é©—çš„å”ä½œèˆ‡é–‹ç™¼ï¼Œæä¾›äº†å¯¦é©—çš„è¨“ç·´ã€æ¸¬è©¦ã€Debugã€å‰è™•ç†ç”¨çš„å…±ç”¨æ¨¡çµ„ï¼Œä¸¦ä¸”æ”¯æ´Checkpointï¼Œè®“æ¯æ¬¡å¯¦é©—ç”¢ç”Ÿçš„æœ€ä½³æ¨¡å‹å¯ä»¥è¢«å„²å­˜ä»¥ä¾›æ¸¬è©¦ä½¿ç”¨ï¼Œä¹Ÿæä¾›loggingçš„æ©Ÿåˆ¶ï¼Œä»¥è®“è¨“ç·´éç¨‹ä¸­çš„æ¨¡å‹çš„æˆæ•ˆå¯ä»¥ç”¨Tensorboardä¾†éš¨æ™‚æª¢è¦–ã€‚

æ–¼æ¯æ¬¡å¯¦é©—ï¼Œæ ¹æ“šæˆ‘å€‘æ‰€åˆ¶å®šçš„è¦ç¯„å‰µç«‹ä¸€å€‹å…¨æ–°çš„å¯¦é©—è¨­å®šè³‡æ–™å¤¾ï¼Œåœ¨å…¶ä¸­å®šç¾©æ¨¡å‹ã€æ¨¡å‹åƒæ•¸ã€å„ä»»å‹™è¡¡é‡æŒ‡æ¨™ã€è³‡æ–™å‰è™•ç†æ–¹æ³•ï¼Œå³å¯èˆ‡æˆ‘å€‘çš„å¯¦é©—å…±ç”¨æ¨¡çµ„é€²è¡Œä¸²æ¥æ•´åˆï¼Œè®“æ¯ä¸€æ¬¡çš„å¯¦é©—éƒ½å¯ä»¥è¢«ç°¡æ˜“åœ°è¤‡è£½ã€è¡¡é‡ã€èª¿æ•´ã€‚

ä»¥ä¸‹å°‡é€²ä¸€æ­¥ä»‹ç´¹æ­¤æ¡†æ¶çš„ 1. å®‰è£æ–¹æ³• 2. è³‡æ–™å¤¾æ¶æ§‹ 3. å¯¦é©—åŸ·è¡Œæ–¹å¼ 3. å¯¦é©—è¨­å®šæ–¹æ³• 4. å°å·¥å…· 5. ç¯„ä¾‹æª”èªªæ˜ 

æ­¤ç¨‹å¼ç‚ºbetaç‰ˆï¼Œè‹¥æ–¼ä½¿ç”¨ä¸­æœ‰ç–‘å•æˆ–å»ºè­°ï¼Œéƒ½å¯ä»¥éš¨æ™‚æä¾›çµ¦æˆ‘å€‘ã€‚

# è³‡æ–™å¤¾æ¶æ§‹ 

ä»¥ä¸‹ç‚ºè³‡æ–™å¤¾æ¶æ§‹ï¼Œæ¨™ä¸Š * çš„æª”æ¡ˆç‚ºå¯¦é©—åŸ·è¡Œå¾Œï¼Œæ‰æœƒç”Ÿæˆçš„æª”æ¡ˆæˆ–è³‡æ–™å¤¾ï¼›æ¨™ä¸Š V çš„ç‚ºç‰¹å®šå¯¦é©—å°ˆå±¬ä¹‹å¯¦é©—æª”æˆ–æª”æ¡ˆå¤¾ï¼›

æ¨™ä¸ŠğŸŸ ç‚ºé ˆç”±å¯¦é©—è€…è‡ªè¡Œå»ºç½®ä¹‹å¯¦é©—æª”æˆ–è³‡æ–™å¤¾ï¼›æ¨™ä¸ŠğŸŸ¢ç‚ºé ˆåŸ·è¡Œä¹‹æª”æ¡ˆï¼›æ¨™ä¸ŠğŸ”µç‚ºé ˆè‡ªè¡Œä¸‹è¼‰ä¹‹æª”æ¡ˆã€‚

```
.
â”œâ”€â”€ data                                                 # å¯¦é©—è³‡æ–™
|    â”œâ”€â”€ source                                            # å­˜æ”¾åŸå§‹è³‡æ–™ 
|    |      â”œâ”€â”€ ğŸŸ¢download_data_from_google_drive.ipynb      # å¾google_dirveä¸‹è¼‰åŸå§‹è³‡æ–™ç”¨
|    |      â”œâ”€â”€ ğŸ”µgoogle_drive.json                          # ä¸²æ¥google_driveç”¨çš„api-keysï¼Œä¸‹è¼‰æ–¹å¼åƒè€ƒ download_data_from_google_drive.ipynb
|    |      â”œâ”€â”€ ğŸ”µsample_chid.txt                            # åŸå§‹è³‡æ–™
|    |      â”œâ”€â”€ ğŸ”µsample_idx_map.npy                         # åŸå§‹è³‡æ–™
|    |      â”œâ”€â”€ ğŸ”µsample_zip_if_cca_cdtx0001_hist.csv        # ...
|    |      â”œâ”€â”€ ğŸ”µsample_zip_if_cca_cust_f.csv               # ...
|    |      â””â”€â”€ ğŸ”µsample_zip_if_cca_y.csv                    # åŸå§‹è³‡æ–™ 
|    | 
|    â”œâ”€â”€ * sample                                        # å­˜æ”¾åŸå§‹è³‡æ–™downsampleå¾Œçš„è³‡æ–™
|    | 
|    â”œâ”€â”€ *V [experiment_group_name]                      # å­˜æ”¾ç‰¹å®šé¡å‹çš„å¯¦é©—(e.g., rnn)æ‰€éœ€ä¹‹è³‡æ–™
|    |      â”œâ”€â”€ * tmp                                      # ä¸­ç¹¼æª”
|    |      â””â”€â”€ * result                                   # çµæœæª”
|    â”œâ”€â”€ *V [experiment_name]                            # å­˜æ”¾ç‰¹å®šå¯¦é©—(e.g., ex1)ç”¨è³‡æ–™
|    |      â”œâ”€â”€ * tmp                                      # ä¸­ç¹¼æª”
|    |      â””â”€â”€ * result                                   # çµæœæª”
|    â”œâ”€â”€ *V [experiment_name]                            
|    â””â”€â”€ ... 
|
â”œâ”€â”€ common                                               # å…±ç”¨æ¨¡çµ„           
|    â”œâ”€â”€ ETLBase.py                                        # å‰è™•ç†ç”¨å°å·¥å…· 
|    â”œâ”€â”€ utils.py                                          
|    â”œâ”€â”€ pl_module.py                                    # å…§å«å¤šä»»å‹™å¯¦é©—å…±ç”¨ä¹‹æŠ½è±¡æ¨¡çµ„ 
|    â””â”€â”€ __init__.py 
|    
â”œâ”€â”€ experiments                                          # V å¯¦é©—æ¨¡çµ„ 
|    â”œâ”€â”€VğŸŸ [experiment_name]                              # V ç‰¹å®šå¯¦é©—ä¹‹å¯¦é©—æ¨¡çµ„ 
|    |      â”œâ”€â”€VğŸŸ experiment_module.py                    # V å¯¦é©—è¨­å®šæ¨¡çµ„                                
|    |      â”œâ”€â”€VğŸŸ model.py                                # V æ¨¡å‹æ¨¡çµ„ 
|    |      â”œâ”€â”€VğŸŸ dataset_builder.py                      # V è³‡æ–™å‰è™•ç†æ¨¡çµ„ 
|    |      â”œâ”€â”€VğŸŸ preprocess.py                           # V è³‡æ–™å‰è™•ç†æ¨¡çµ„ 
|    |      â””â”€â”€VğŸŸ __init__.py                        
|    â””â”€â”€VğŸŸ [experiment_name]
|    â””â”€â”€ ...
|
â”œâ”€â”€ *checkpoint                                          # å„²å­˜æ¨¡å‹æš«å­˜æª” 
|    â”œâ”€â”€ *V [experiment_name]                              # å„²å­˜ç‰¹å®šå¯¦é©—çš„æ¨¡å‹æš«å­˜æª”
|    |      â”œâ”€â”€ *V epoch964-loss0.00.ckpt                    # V æœ€ä½³æ¨¡å‹çš„æš«å­˜æª”
|    |      â””â”€â”€ *V last.ckpt                                 # V æœ€å¾Œä¸€å€‹epochçš„æ¨¡å‹æš«å­˜æª”   
|    â”œâ”€â”€ *V [experiment_name]
|    â””â”€â”€ ... 
|
â”œâ”€â”€ *logs                                                # å­˜æ”¾è¨“ç·´LOG: å„ä»»å‹™by-iterationçš„æˆæ•ˆã€å¯¦é©—åƒæ•¸ã€æ¨¡å‹æ¶æ§‹åœ– 
|    â””â”€â”€ tensorboard 
|           â”œâ”€â”€ *V [experiment_name]                       # å„²å­˜ç‰¹å®šå¯¦é©—çš„LOG 
|           |       â”œâ”€â”€ *V version_0                         # å„²å­˜ç¬¬1æ¬¡å¯¦é©—çš„LOG 
|           |       â”œâ”€â”€ *V version_1                         # å„²å­˜ç¬¬2æ¬¡å¯¦é©—çš„LOG 
|           |       â””â”€â”€ ... 
|           â”œâ”€â”€ *V [experiment_name]                      
|           â””â”€â”€ ... 
â”œâ”€â”€ğŸŸ¢run_project.py                                      # å¯¦é©—åŸ·è¡Œæª” 
â”œâ”€â”€ requirements.txt 
â””â”€â”€ ReadMe.md 
```

# å¯¦é©—åŸ·è¡Œæ–¹æ³• 

## Step 1: å®‰è£dependencies 

é¦–å…ˆå°‡ç›¸é—œå¥—ä»¶é€²è¡Œå®‰è£ã€‚

åŸ·è¡Œ: 
`sh install_packages.sh`

æˆ–: 
```
pip install google-api-python-client==2.5.0
pip install oauth2client==4.1.3

pip install numpy==1.18.5
pip install pandas==1.1.4
pip install tqdm==4.54.1
pip install feather-format==0.4.1
pip install tables==3.6.1

pip install sklearn==0.0
pip install torch==1.8.1

pip install torchmetrics==0.3.2
pip install pytorch-lightning==1.3.2
pip install lightning-bolts==0.3.3

pip install tensorboard==2.4.0
```
## Step 2: ä¸‹è¼‰åŸå§‹è³‡æ–™ 

* æ–¹æ³•ä¸€: è‡³data/sourceåŸ·è¡Œ**download_data_from_google_drive.ipynb**é€²è¡Œä»¥ä¸‹åŸå§‹è³‡æ–™çš„ä¸‹è¼‰
```
ğŸ”µsample_chid.txt                            # åŸå§‹è³‡æ–™
ğŸ”µsample_idx_map.npy                         # åŸå§‹è³‡æ–™
ğŸ”µsample_zip_if_cca_cdtx0001_hist.csv        # ...
ğŸ”µsample_zip_if_cca_cust_f.csv               # ...
ğŸ”µsample_zip_if_cca_y.csv                    # åŸå§‹è³‡æ–™ 
```



* æ–¹æ³•äºŒ: è‡ªè¡Œä¸‹è¼‰ä»¥ä¸Šè³‡æ–™è‡³data/sourceã€‚

è‹¥æ¡ç”¨æ–¹æ³•ä¸€ï¼Œé ˆè‡³google developer platformä¸‹è¼‰ğŸ”µgoogle_drive.jsonï¼Œä¸²æ¥google_driveç”¨çš„api-keysï¼Œä¸‹è¼‰æ–¹å¼åƒè€ƒ download_data_from_google_drive.ipynbã€‚

## Step 3: æ¸¬è©¦å¯¦é©—æ˜¯å¦å¯åŸ·è¡Œ 

åŸ·è¡ŒFastDebug: 
`python run_project.py -m fastdebug -e ex1` 


æ­¤ç¨‹å¼æœƒå°experiments/ex1è³‡æ–™å¤¾æ‰€å®šç¾©ä¹‹å¯¦é©—é€²è¡Œdebugã€‚éç¨‹ä¸­æœƒå°åŸå§‹è³‡æ–™é€²è¡Œå‰è™•ç†ï¼Œä¸¦å°‡çµæœèˆ‡ä½”å­˜æª”å„²å­˜æ–¼`data/sample`ã€`data/rnn/tmp`ã€`data/rnn/result`ã€`data/ex1/tmp`ã€`data/ex1/result`ï¼Œæ¥è‘—ï¼Œè³‡æ–™è™•ç†å®Œå¾Œï¼Œå°±æœƒé€²è¡Œ1å€‹stepçš„trainingå’Œvalidationï¼Œä»¥å¿«é€Ÿé©—è­‰æ¨¡å‹ã€ç¨‹å¼çš„é‹ä½œæ­£å¸¸ã€‚

## Step 4: å»ºæ§‹æ–°å¯¦é©—: 

å¯ä»¥è¤‡è£½ex1è³‡æ–™å¤¾ï¼Œå¿…å°‡å…¶æ”¹ç‚ºå¯¦é©—è€…æ¬²å‘½åçš„å¯¦é©—åç¨±ï¼ˆe.g., ex2)ï¼Œä¸¦ä¿®æ”¹å…¶ä¸­çš„`experiment_module.py`/`model.py`/`dataset_builder.py`/`preprocess.py`ã€‚å…¶ä¸­`experiment_module.py`ç‚ºå¯¦é©—æ¨¡çµ„ï¼Œ`model.py`ç‚ºæ¨¡å‹ï¼Œ`dataset_builder.py`å’Œ`preprocess.py`ç‚ºå‰è™•ç†ç¨‹å¼ã€‚

ä»¥ä¸‹å°‡ä»¥ex1ç‚ºç¯„ä¾‹ï¼Œåˆ†åˆ¥èªªæ˜æ­¤ä¸‰é¡ç¨‹å¼çš„å»ºæ§‹æ–¹å¼: 

### å¯¦é©—æ¨¡çµ„ (`experiment_module.py`)

å¯¦é©—æ¨¡çµ„å¿…é ˆåŒ…å«ä¸‰å€‹Class: `ExperimentConfig`ã€`ExperimentalMultiTaskDataModule`å’Œ`ExperimentalMultiTaskModule`ï¼Œ`ExperimentConfig`å®šç¾©äº†å¯¦é©—åç¨±ã€å¯¦é©—åƒæ•¸ã€ä»¥åŠæœ€ä½³æ¨¡å‹çš„æš«å­˜æª”åç¨±ï¼Œ`ExperimentalMultiTaskDataModule`å®šç¾©äº†è³‡æ–™å‰è™•ç†ã€è¨“ç·´ä»¥åŠæ¸¬è©¦è³‡æ–™ï¼Œ`ExperimentalMultiTaskModule`å®šç¾©äº†å¤šä»»å‹™æ¨¡å‹ã€ä»»å‹™åç¨±ã€ä»»å‹™ç›®æ¨™å‡½æ•¸ä»¥åŠä»»å‹™æˆæ•ˆè¡¡é‡æŒ‡æ¨™ã€‚

ä»¥ä¸‹å°‡é‡å°æ’°å¯«æ–¹å¼é€²ä¸€æ­¥èªªæ˜: 

1. **ExperimentConfig**:
```python 
class ExperimentConfig:
    # @ExperimentDependent 
    best_model_checkpoint = 'epoch07-loss0.00.ckpt' # æœ€ä½³æ¨¡å‹çš„æš«å­˜æª”åç¨±

    name = experiment_name  # å¯¦é©—åç¨±ï¼Œéœ€å’Œè³‡æ–™å¤¾åç¨±ç›¸åŒ(e.g., ex1) 

    experiment_parameters = {                    # å¯¦é©—åƒæ•¸ 
        "model_parameters": {                      # æ¨¡å‹åƒæ•¸: æœƒæ”¾åˆ°model.pyçš„åƒæ•¸ã€‚
            "data_independent":{                     # å’Œè³‡æ–™ç„¡é—œçš„æ¨¡å‹åƒæ•¸ï¼Œ(e.g., hidden_dims ä¸­é–“å±¤ç¶­åº¦, n_layers å±¤æ•¸, ...) 
                'hidden_dims': 64,              
                'n_layers': 2, 
                'cell': 'LSTM', 
                'bi': False
            },
            "data_dependent": {                      # å’Œè³‡æ–™ç›¸é—œä¹‹æ¨¡å‹åƒæ•¸ï¼Œ(e.g., dense_dims è¼¸å…¥ä¹‹æ•¸å€¼å‹ç‰¹å¾µç¶­åº¦, use_chid æ˜¯å¦ä½¿ç”¨é¡§å®¢IDç‚ºç‰¹å¾µ, out_dims æ¨¡å‹è¼¸å‡ºçš„ç¶­åº¦)  
                'dense_dims': dense_dims, 
                'sparse_dims': sparse_dims,
                'use_chid': use_chid, 
                'out_dims': out_dims,
                'class_outputs': class_outputs 
            }
        },
        "training_parameters":{                      # è¨“ç·´åƒæ•¸ï¼Œ(e.g., dropout, warmup_epochs, annealing_cycle_epochs) 
            "dropout": 0.5, 
            "warmup_epochs": 5, 
            "annealing_cycle_epochs": 40
        }
    }
```

* å…¶ä¸­ï¼Œdropout/warmup_epochs/annealing_cycle_epochsè‹¥ä¸æä¾›ï¼Œé æ¸¬å‰‡åˆ†åˆ¥æœƒçµ¦0.5, 5, 40ã€‚warmup_epochså’Œannealing_cycle_epochsæœƒè¼¸å…¥ä»¥ä¸‹learning rate schedulerï¼Œé€²è¡Œlearning rateå‹•æ…‹èª¿æ•´ï¼Œä»¥åŠ é€Ÿæ¨¡å‹è¨“ç·´ã€‚

```python 
LinearWarmupCosineAnnealingLR(optimizer, 
 warmup_epochs=self._warmup_epochs, 
 max_epochs=self._annealing_cycle_epochs 
)
```
2. **ExperimentalMultiTaskDataModule**:


```python 
class ExperimentalMultiTaskDataModule(BaseMultiTaskDataModule):
    # @blockPrinting
    def prepare_data(self):
        self.train_dataset = train_dataset.run()[0] 
        self.test_dataset = test_dataset.run()[0]
```

* æ–¼prepare_dataå®šç¾©train_datasetå’Œtest_datasetã€‚æ­¤å…©å€‹ç‰©ä»¶é ˆç‚ºtorch.utils.dataçš„TensorDatasetç‰©ä»¶ã€‚

3. **ExperimentalMultiTaskModule**:

```python 
import torch.nn.functional as F
class ExperimentalMultiTaskModule(BaseMultiTaskModule):

    def config_model(self, model_parameters, dropout): # æ­¤è™•å¼•å…¥model.pyçš„æœ€top-levelçš„nn.Moduleï¼Œæ­¤nn.Moduleåƒmodel_parameterså’Œdropoutå…©å€‹åƒæ•¸ï¼Œå¾Œé¢å°‡æ–¼model.pyé€²ä¸€æ­¥èªªæ˜å»ºæ§‹æ–¹å¼ã€‚
        return MultiTaskModel(
                model_parameters,
                dropout = dropout
            )
            
    def config_task_names(self):                                 # æ­¤è™•å®šç¾©æ¨¡å‹è¼¸å‡ºæ‰€å°æ‡‰ä¹‹ä»»å‹™åç¨± 
        return ['objmean', 'tscnt', 'label_0']
        
    def config_loss_funcs(self): 
        return [F.mse_loss, F.mse_loss, F.binary_cross_entropy]  # æ­¤è™•å®šç¾©å„ä»»å‹™ä¹‹ç›®æ¨™å‡½æ•¸ 
    
    

    def config_task_metrics(self):                               # æ­¤è™•å®šç¾©å€‹ä»»å‹™ä¹‹è¡¡é‡æŒ‡æ¨™åç¨± 
        return {
            'objmean': ['mse', 'mae'], 
            'tscnt': ['mse', 'mae'], 
            'label_0': ['acc', 'auc']
        }
    
    def config_metric_calculators(self):                         # (optional) å®šç¾©å€‹æŒ‡æ¨™åç¨±æ‰€å°æ‡‰ä¹‹æŒ‡æ¨™è¨ˆç®—å…ƒä»¶ã€‚è‹¥æœ‰æ–°æŒ‡æ¨™(émse/mae/acc/auc)æ‰éœ€å¯¦ä½œæ­¤å‡½æ•¸ã€‚
        from torchmetrics import MeanSquaredError, MeanAbsoluteError, Accuracy, AUROC
        return {
            'mse': lambda: MeanSquaredError(compute_on_step=False), 
            'mae': lambda: MeanAbsoluteError(compute_on_step=False), 
            'acc': lambda: Accuracy(compute_on_step=False),
            'auc': lambda: AUROC(compute_on_step=False, pos_label=1)
        }

```

### æ¨¡å‹ (`model.py`)

### è³‡æ–™å‰è™•ç† (`dataset_builder.py`/`preprocess.py`)


## Step 5: åŸ·è¡ŒFit1Batch & Training: 
`python run_project.py -m fit1batch -e ex1` 

è‹¥è¦é©—è­‰æ¨¡å‹æ¶æ§‹æ˜¯æ­£ç¢ºï¼Œå¯ä»¥åŸ·è¡Œfit1batchï¼Œæ­¤æ™‚æœƒè®“æ¨¡å‹Overfitä¸€å€‹Batchçš„è¨“ç·´è³‡æ–™ï¼Œæ­¤æ™‚æœƒåœ¨

`python run_project.py -m training -e ex1` 


# å°å·¥å…· 


# ç¯„ä¾‹ 

# Old ReadMe: 

## åŸå§‹ç¨‹å¼ç¢¼
```diff
! Under Construction !
```

You can check the latest sources with the command:
```
git clone git@github.com:udothemath/ncku_customer_embedding.git
```

## å®‰è£dependencies: 

```
sh install_packages.sh
```

### ToDo: add package version
```
pip install google-api-python-client==2.5.0
pip install oauth2client==4.1.3

pip install numpy==1.18.5
pip install pandas==1.1.4
pip install tqdm==4.54.1
pip install feather-format==0.4.1
pip install tables==3.6.1

pip install sklearn==0.0
pip install torch==1.8.1

pip install torchmetrics==0.3.2
pip install pytorch-lightning==1.3.2
pip install lightning-bolts==0.3.3

pip install tensorboard==2.4.0
```

## å¦‚ä½•è¨­å®šèˆ‡åŸ·è¡Œ? 

### 1. Download Dataset from Google Drive 
* è‡³../dataåŸ·è¡Œ**download_data_from_google_drive.ipynb**é€²è¡Œè¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™ä¸‹è¼‰

### 2. Preprocessing and Build TensorDataset 

* å…ˆè‡³../esunåº•ä¸‹
* åˆ†æ®µåŸ·è¡Œ
  * `python preprocess.py`: å°‡../dataçš„è³‡æ–™é€²è¡Œdownsamplingå’Œè³‡æ–™è™•ç†è½‰æ›ï¼Œä¸¦å°‡çµæœå„²å­˜æ–¼../esun/data/resultã€‚
  * `python dataset_builder.py`: å°‡preprocess.pyçš„çµæœé€²ä¸€æ­¥è½‰æ›ç‚ºæ¨¡å‹æ‰€éœ€ä¹‹æ ¼å¼(i.e., TensorDataset)ã€‚
* ç›´æ¥åŸ·è¡Œ
  * `python dataset_builder.py`

### 3. å»ºç«‹loggingèˆ‡checkpointè·¯å¾‘

1.  å»ºç«‹**logs/tensorboard**è·¯å¾‘ï¼Œä¸¦æ–¼å…¶ä¸­å»ºç«‹ncku_customer_embeddingè³‡æ–™å¤¾ï¼Œä»¥å„²å­˜å¯¦é©—ç”¢ç”Ÿä¹‹Tensorboard Logsã€‚
2.  å»ºç«‹**checkpoint**è³‡æ–™å¤¾ï¼Œä»¥å„²å­˜æ¨¡å‹æš«å­˜æª”ã€‚
3.  æ‰“é–‹../esunåŸ·è¡Œ**run_project.py**é€²è¡Œç·¨è¼¯ã€‚
    - å°‡TensorBoardLogger('/home/ai/work/logs/tensorboard',...)ä¸­çš„tensorboardè·¯å¾‘æ”¹ç‚ºStep 1æ‰€å‰µå»ºçš„**logs/tensorboardè·¯å¾‘**ã€‚
    - å°‡ModelCheckpoint(... dirpath='./checkpoint',...)ä¸­çš„dirpathè·¯å¾‘æ”¹ç‚ºStep 2çš„**checkpointè·¯å¾‘**

### 4. åŸ·è¡Œæ¨¡å‹è¨“ç·´ã€Debugæˆ–é©—è­‰

* è¨“ç·´: `python run_project.py -m train`
* Debug: 
  - `python run_project.py -m fastdebug` (å¿«é€ŸåŸ·è¡Œä¸€æ¬¡validation_stepå’Œtrain_step)
  - `python run_project.py -m fit1batch` ([è®“æ¨¡å‹overfitä¸€å€‹batch](https://www.youtube.com/watch?v=nAZdK4codMk)) 
* é©—è­‰: 
  - `python run_project.py -m test` (ä½¿ç”¨æ¸¬è©¦è³‡æ–™é€²è¡Œæ¸¬è©¦) 


## å¦‚ä½•ç›£æ§è¨“ç·´ç‹€æ³? 

- æ–¼terminalè¼¸å…¥`tensorboard --logdir [tensorboard/ncku_customer_embeddingè·¯å¾‘]`ï¼Œå³å¯æ–¼ç€è¦½å™¨é–‹å•ŸtensorboardæŸ¥çœ‹è¨“ç·´ç‹€æ³(http://localhost:6006/)ã€‚


## é‡è¦ç¨‹å¼è¨­å®šèªªæ˜ 

### Downsampling: 

ç‚ºäº†åŠ é€Ÿæ¸¬è©¦ï¼Œ**preprocess.py**åšè³‡æ–™è™•ç†éç¨‹ä¸­ï¼Œæœƒé€²ä¸€æ­¥downsampleè‡³500åusersï¼Œå°‡**preprocess.py**ä¸­é€²è¡Œä»¥ä¸‹ä¿®æ”¹ï¼Œå³å¯è€ƒæ…®æ‰€æœ‰(50K)çš„usersã€‚

å°‡
```python
sampled_chids = Sample_chids(
                      'sample_chids', 
                      [chids], 
                      result_dir = os.path.join(sample_path,'sampled_chids.npy'), 
                      n_sample = 500
           ) 
```
æ”¹ç‚º
```python
sampled_chids = Sample_chids(
                      'sample_chids', 
                      [chids], 
                      result_dir = os.path.join(sample_path,'sampled_chids.npy'), 
                      n_sample = None
           ) 
```
### å¦‚ä½•ä¿®æ”¹æ¨¡å‹åƒæ•¸? 

è‡³run_project.pyä¿®æ”¹: 
```python
config = {
           'hidden_dims': 64, 
           'n_layers': 2, 
           'cell': 'LSTM', 
           'bi': False, 
           'dropout': 0.5
}
``` 

è‡³dataset_builder.pyä¿®æ”¹`dense_feat`ã€`sparse_feat`å’Œ`USE_CHID`ä»¥æ±ºå®šæ¨¡å‹æ‰€ä½¿ç”¨çš„**é¡åˆ¥å‹ç‰¹å¾µ**ã€**æ•¸å€¼å‹ç‰¹å¾µ**ä»¥åŠ**æ˜¯å¦ä½¿ç”¨é¡§å®¢idåšç‚ºé¡åˆ¥å‹ç‰¹å¾µ**ã€‚

### å¦‚ä½•ä½¿preprocess.pyèªåˆ¥å…¶ä½¿ç”¨æª”æ¡ˆçš„å„²å­˜è·¯å¾‘ä»¥åŠå…¶ç”¢ç”Ÿçš„æª”æ¡ˆä¹‹å„²å­˜è·¯å¾‘? 

å¯å°‡ä»¥ä¸‹preprocess.pyçš„è·¯å¾‘é€²è¡Œèª¿æ•´ï¼Œ`origin_path`æ˜¯ä¾†æºè³‡æ–™çš„è·¯å¾‘ã€`sample_path`æ˜¯å„²å­˜ä¾†æºè³‡æ–™çš„ä¸€å€‹downsampleçš„ç‰ˆæœ¬çš„è·¯å¾‘ã€`tmp_path`å„²å­˜preprocesséç¨‹ä¸­ä¸­ç¹¼æª”çš„è·¯å¾‘ã€`result_path`å„²å­˜æœ€çµ‚æª”æ¡ˆçš„è·¯å¾‘ã€‚

```python
origin_path = '../data'
sample_path = 'data/sample'
tmp_path = 'data/tmp'
result_path = 'data/result'
chid_file = os.path.join(origin_path, 'sample_chid.txt')
cdtx_file = os.path.join(origin_path, 'sample_zip_if_cca_cdtx0001_hist.csv')
cust_f_file = os.path.join(origin_path, 'sample_zip_if_cca_cust_f.csv')
```

## Test
Add my comment
