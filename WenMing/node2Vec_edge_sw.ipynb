{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#from torch_geometric.nn import Node2Vec\n",
    "from model import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw, pos_val in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device), pos_val.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/sample_50k'\n",
    "\n",
    "data = pd.read_csv(os.path.join(path, 'sample_zip_if_cca_cdtx0001_hist.csv')).sort_values(by=['csmdt'], ignore_index=True)\n",
    "chid_idx_map = np.load(os.path.join(path, 'sample_idx_map.npy'), allow_pickle=True).item()\n",
    "months = sorted(data.csmdt.apply(lambda x: x[:-3]).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = len(chid_idx_map)\n",
    "data = data[data.chid.map(chid_idx_map) < sample_size].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_map = {}\n",
    "\n",
    "for i, (key, value) in zip(range(sample_size), chid_idx_map.items()):\n",
    "    idx_map[key] = value\n",
    "    \n",
    "l = len(idx_map)\n",
    "for i, j  in enumerate(set(data.mcc)):\n",
    "    idx_map[str(j)] = i+l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range: 2018-01 2019-01\n",
      "\tInteraction: (2785843, 3)\n",
      "\tEdge: (518927, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3446\n",
      "\tEpoch: 50, Loss 0.8367\n",
      "\tEpoch: 75, Loss 0.7609\n",
      "\tEpoch: 100, Loss 0.7431\n",
      "\tEpoch: 125, Loss 0.7379\n",
      "\tEpoch: 150, Loss 0.7366\n",
      "\tEpoch: 175, Loss 0.7354\n",
      "\tEpoch: 200, Loss 0.7311\n",
      "\tEpoch: 225, Loss 0.7325\n",
      "\tEpoch: 250, Loss 0.7293\n",
      "\tEpoch: 275, Loss 0.7313\n",
      "\tEpoch: 300, Loss 0.7290\n",
      "\tEpoch: 325, Loss 0.7287\n",
      "\tEpoch: 350, Loss 0.7276\n",
      "\tEpoch: 375, Loss 0.7268\n",
      "\tEpoch: 400, Loss 0.7268\n",
      "\tTraining cost: 591.27\n",
      "\n",
      "Range: 2018-02 2019-02\n",
      "\tInteraction: (2837070, 3)\n",
      "\tEdge: (526835, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3469\n",
      "\tEpoch: 50, Loss 0.8396\n",
      "\tEpoch: 75, Loss 0.7637\n",
      "\tEpoch: 100, Loss 0.7472\n",
      "\tEpoch: 125, Loss 0.7398\n",
      "\tEpoch: 150, Loss 0.7386\n",
      "\tEpoch: 175, Loss 0.7363\n",
      "\tEpoch: 200, Loss 0.7306\n",
      "\tEpoch: 225, Loss 0.7304\n",
      "\tEpoch: 250, Loss 0.7306\n",
      "\tEpoch: 275, Loss 0.7310\n",
      "\tEpoch: 300, Loss 0.7280\n",
      "\tEpoch: 325, Loss 0.7284\n",
      "\tEpoch: 350, Loss 0.7276\n",
      "\tEpoch: 375, Loss 0.7266\n",
      "\tEpoch: 400, Loss 0.7265\n",
      "\tTraining cost: 623.03\n",
      "\n",
      "Range: 2018-03 2019-03\n",
      "\tInteraction: (2867363, 3)\n",
      "\tEdge: (530825, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3443\n",
      "\tEpoch: 50, Loss 0.8416\n",
      "\tEpoch: 75, Loss 0.7665\n",
      "\tEpoch: 100, Loss 0.7499\n",
      "\tEpoch: 125, Loss 0.7402\n",
      "\tEpoch: 150, Loss 0.7367\n",
      "\tEpoch: 175, Loss 0.7364\n",
      "\tEpoch: 200, Loss 0.7318\n",
      "\tEpoch: 225, Loss 0.7311\n",
      "\tEpoch: 250, Loss 0.7319\n",
      "\tEpoch: 275, Loss 0.7304\n",
      "\tEpoch: 300, Loss 0.7301\n",
      "\tEpoch: 325, Loss 0.7290\n",
      "\tEpoch: 350, Loss 0.7282\n",
      "\tEpoch: 375, Loss 0.7277\n",
      "\tEpoch: 400, Loss 0.7274\n",
      "\tTraining cost: 518.86\n",
      "\n",
      "Range: 2018-04 2019-04\n",
      "\tInteraction: (2917893, 3)\n",
      "\tEdge: (536634, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3505\n",
      "\tEpoch: 50, Loss 0.8391\n",
      "\tEpoch: 75, Loss 0.7627\n",
      "\tEpoch: 100, Loss 0.7453\n",
      "\tEpoch: 125, Loss 0.7378\n",
      "\tEpoch: 150, Loss 0.7374\n",
      "\tEpoch: 175, Loss 0.7363\n",
      "\tEpoch: 200, Loss 0.7319\n",
      "\tEpoch: 225, Loss 0.7316\n",
      "\tEpoch: 250, Loss 0.7312\n",
      "\tEpoch: 275, Loss 0.7312\n",
      "\tEpoch: 300, Loss 0.7301\n",
      "\tEpoch: 325, Loss 0.7298\n",
      "\tEpoch: 350, Loss 0.7290\n",
      "\tEpoch: 375, Loss 0.7290\n",
      "\tEpoch: 400, Loss 0.7273\n",
      "\tTraining cost: 542.34\n",
      "\n",
      "Range: 2018-05 2019-05\n",
      "\tInteraction: (2967517, 3)\n",
      "\tEdge: (542162, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3504\n",
      "\tEpoch: 50, Loss 0.8412\n",
      "\tEpoch: 75, Loss 0.7649\n",
      "\tEpoch: 100, Loss 0.7470\n",
      "\tEpoch: 125, Loss 0.7396\n",
      "\tEpoch: 150, Loss 0.7373\n",
      "\tEpoch: 175, Loss 0.7366\n",
      "\tEpoch: 200, Loss 0.7326\n",
      "\tEpoch: 225, Loss 0.7311\n",
      "\tEpoch: 250, Loss 0.7307\n",
      "\tEpoch: 275, Loss 0.7301\n",
      "\tEpoch: 300, Loss 0.7290\n",
      "\tEpoch: 325, Loss 0.7293\n",
      "\tEpoch: 350, Loss 0.7293\n",
      "\tEpoch: 375, Loss 0.7274\n",
      "\tEpoch: 400, Loss 0.7281\n",
      "\tTraining cost: 511.77\n",
      "\n",
      "Range: 2018-06 2019-06\n",
      "\tInteraction: (3020763, 3)\n",
      "\tEdge: (547510, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3464\n",
      "\tEpoch: 50, Loss 0.8378\n",
      "\tEpoch: 75, Loss 0.7635\n",
      "\tEpoch: 100, Loss 0.7472\n",
      "\tEpoch: 125, Loss 0.7406\n",
      "\tEpoch: 150, Loss 0.7378\n",
      "\tEpoch: 175, Loss 0.7361\n",
      "\tEpoch: 200, Loss 0.7330\n",
      "\tEpoch: 225, Loss 0.7319\n",
      "\tEpoch: 250, Loss 0.7302\n",
      "\tEpoch: 275, Loss 0.7302\n",
      "\tEpoch: 300, Loss 0.7293\n",
      "\tEpoch: 325, Loss 0.7296\n",
      "\tEpoch: 350, Loss 0.7290\n",
      "\tEpoch: 375, Loss 0.7289\n",
      "\tEpoch: 400, Loss 0.7284\n",
      "\tTraining cost: 538.73\n",
      "\n",
      "Range: 2018-07 2019-07\n",
      "\tInteraction: (3086395, 3)\n",
      "\tEdge: (553971, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3572\n",
      "\tEpoch: 50, Loss 0.8420\n",
      "\tEpoch: 75, Loss 0.7658\n",
      "\tEpoch: 100, Loss 0.7491\n",
      "\tEpoch: 125, Loss 0.7415\n",
      "\tEpoch: 150, Loss 0.7390\n",
      "\tEpoch: 175, Loss 0.7366\n",
      "\tEpoch: 200, Loss 0.7326\n",
      "\tEpoch: 225, Loss 0.7329\n",
      "\tEpoch: 250, Loss 0.7319\n",
      "\tEpoch: 275, Loss 0.7304\n",
      "\tEpoch: 300, Loss 0.7305\n",
      "\tEpoch: 325, Loss 0.7291\n",
      "\tEpoch: 350, Loss 0.7296\n",
      "\tEpoch: 375, Loss 0.7291\n",
      "\tEpoch: 400, Loss 0.7289\n",
      "\tTraining cost: 525.25\n",
      "\n",
      "Range: 2018-08 2019-08\n",
      "\tInteraction: (3149391, 3)\n",
      "\tEdge: (559629, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3549\n",
      "\tEpoch: 50, Loss 0.8379\n",
      "\tEpoch: 75, Loss 0.7634\n",
      "\tEpoch: 100, Loss 0.7460\n",
      "\tEpoch: 125, Loss 0.7404\n",
      "\tEpoch: 150, Loss 0.7383\n",
      "\tEpoch: 175, Loss 0.7369\n",
      "\tEpoch: 200, Loss 0.7342\n",
      "\tEpoch: 225, Loss 0.7325\n",
      "\tEpoch: 250, Loss 0.7328\n",
      "\tEpoch: 275, Loss 0.7316\n",
      "\tEpoch: 300, Loss 0.7315\n",
      "\tEpoch: 325, Loss 0.7304\n",
      "\tEpoch: 350, Loss 0.7312\n",
      "\tEpoch: 375, Loss 0.7282\n",
      "\tEpoch: 400, Loss 0.7299\n",
      "\tTraining cost: 507.56\n",
      "\n",
      "Range: 2018-09 2019-09\n",
      "\tInteraction: (3220317, 3)\n",
      "\tEdge: (566024, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3509\n",
      "\tEpoch: 50, Loss 0.8421\n",
      "\tEpoch: 75, Loss 0.7682\n",
      "\tEpoch: 100, Loss 0.7515\n",
      "\tEpoch: 125, Loss 0.7410\n",
      "\tEpoch: 150, Loss 0.7372\n",
      "\tEpoch: 175, Loss 0.7362\n",
      "\tEpoch: 200, Loss 0.7336\n",
      "\tEpoch: 225, Loss 0.7329\n",
      "\tEpoch: 250, Loss 0.7325\n",
      "\tEpoch: 275, Loss 0.7311\n",
      "\tEpoch: 300, Loss 0.7300\n",
      "\tEpoch: 325, Loss 0.7308\n",
      "\tEpoch: 350, Loss 0.7295\n",
      "\tEpoch: 375, Loss 0.7285\n",
      "\tEpoch: 400, Loss 0.7286\n",
      "\tTraining cost: 526.32\n",
      "\n",
      "Range: 2018-10 2019-10\n",
      "\tInteraction: (3290484, 3)\n",
      "\tEdge: (572274, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3509\n",
      "\tEpoch: 50, Loss 0.8401\n",
      "\tEpoch: 75, Loss 0.7652\n",
      "\tEpoch: 100, Loss 0.7483\n",
      "\tEpoch: 125, Loss 0.7421\n",
      "\tEpoch: 150, Loss 0.7397\n",
      "\tEpoch: 175, Loss 0.7379\n",
      "\tEpoch: 200, Loss 0.7328\n",
      "\tEpoch: 225, Loss 0.7316\n",
      "\tEpoch: 250, Loss 0.7320\n",
      "\tEpoch: 275, Loss 0.7308\n",
      "\tEpoch: 300, Loss 0.7304\n",
      "\tEpoch: 325, Loss 0.7296\n",
      "\tEpoch: 350, Loss 0.7304\n",
      "\tEpoch: 375, Loss 0.7293\n",
      "\tEpoch: 400, Loss 0.7295\n",
      "\tTraining cost: 517.95\n",
      "\n",
      "Range: 2018-11 2019-11\n",
      "\tInteraction: (3372371, 3)\n",
      "\tEdge: (577904, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3433\n",
      "\tEpoch: 50, Loss 0.8419\n",
      "\tEpoch: 75, Loss 0.7686\n",
      "\tEpoch: 100, Loss 0.7520\n",
      "\tEpoch: 125, Loss 0.7427\n",
      "\tEpoch: 150, Loss 0.7396\n",
      "\tEpoch: 175, Loss 0.7391\n",
      "\tEpoch: 200, Loss 0.7346\n",
      "\tEpoch: 225, Loss 0.7325\n",
      "\tEpoch: 250, Loss 0.7313\n",
      "\tEpoch: 275, Loss 0.7318\n",
      "\tEpoch: 300, Loss 0.7310\n",
      "\tEpoch: 325, Loss 0.7306\n",
      "\tEpoch: 350, Loss 0.7298\n",
      "\tEpoch: 375, Loss 0.7291\n",
      "\tEpoch: 400, Loss 0.7284\n",
      "\tTraining cost: 540.14\n",
      "\n",
      "Range: 2018-12 2019-12\n",
      "\tInteraction: (3452978, 3)\n",
      "\tEdge: (582249, 5)\n",
      "\n",
      "\tTraining start\n",
      "\tEpoch: 25, Loss 1.3428\n",
      "\tEpoch: 50, Loss 0.8392\n",
      "\tEpoch: 75, Loss 0.7651\n",
      "\tEpoch: 100, Loss 0.7483\n",
      "\tEpoch: 125, Loss 0.7413\n",
      "\tEpoch: 150, Loss 0.7373\n",
      "\tEpoch: 175, Loss 0.7356\n",
      "\tEpoch: 200, Loss 0.7338\n",
      "\tEpoch: 225, Loss 0.7319\n",
      "\tEpoch: 250, Loss 0.7325\n",
      "\tEpoch: 275, Loss 0.7314\n",
      "\tEpoch: 300, Loss 0.7309\n",
      "\tEpoch: 325, Loss 0.7317\n",
      "\tEpoch: 350, Loss 0.7304\n",
      "\tEpoch: 375, Loss 0.7303\n",
      "\tEpoch: 400, Loss 0.7289\n",
      "\tTraining cost: 507.93\n",
      "\n",
      "Total cost: 6509.27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ws: window size\n",
    "# sw: sliding window\n",
    "ws = 12\n",
    "batch_size = 2048\n",
    "learning_rate = 1e-2\n",
    "epochs = 400\n",
    "\n",
    "embeds_list = []\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(12):\n",
    "    print('Range:', months[i], months[i+ws])\n",
    "    \n",
    "    mask = data.index[data.csmdt.between(months[i], months[i+ws])]\n",
    "    data_sw = data.loc[mask, ['chid', 'mcc', 'objam']]\n",
    "    \n",
    "    print('\\tInteraction:', data_sw.shape)\n",
    "    \n",
    "    df_group = data_sw.groupby(by=['chid', 'mcc']).mean()\n",
    "\n",
    "    df_edge = pd.DataFrame(list(map(list, df_group.index)), columns=['chid', 'mcc'])\n",
    "    df_edge['value'] = df_group.objam.values\n",
    "    df_edge['log_value'] = np.log1p(df_edge.value)\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_edge['mm_log_value'] = scaler.fit_transform(df_edge.loc[:, ['log_value']])\n",
    "\n",
    "    df_edge.chid = df_edge.chid.map(idx_map)\n",
    "    df_edge.mcc = df_edge.mcc.map(idx_map)\n",
    "    \n",
    "    print('\\tEdge:', df_edge.shape)\n",
    "    \n",
    "    edges = df_edge.loc[:, ['chid', 'mcc', 'mm_log_value']].values\n",
    "    edges = np.append(edges, df_edge.loc[:, ['mcc', 'chid', 'mm_log_value']].values, axis = 0)\n",
    "    \n",
    "    ## build model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = Node2Vec(torch.Tensor(edges.T), embedding_dim=64, num_nodes=len(idx_map), \n",
    "                     walk_length=2, context_size=2, walks_per_node=10,\n",
    "                     num_negative_samples=4, p=1, q=1, sparse=False).to(device)    \n",
    "    \n",
    "    if i > 0:\n",
    "        model.embedding.weight.data.copy_(torch.from_numpy(embeds_list[-1]))\n",
    "    \n",
    "    loader = model.loader(batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    optimizer = torch.optim.AdamW(list(model.parameters()), lr=learning_rate)\n",
    "\n",
    "    print('\\n\\tTraining start')\n",
    "    t1 = time.time()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        loss = train()\n",
    "        \n",
    "        if (ep+1) % 25 == 0:\n",
    "            print('\\tEpoch: {:02d}, Loss {:.4f}'.format(ep+1, loss))\n",
    "    print('\\tTraining cost: {:.2f}\\n'.format(time.time() - t1))\n",
    "    \n",
    "    embeds = model().detach().cpu().numpy()\n",
    "    embeds_list.append(embeds)\n",
    "    \n",
    "print('Total cost: {:.2f}\\n'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = ['#065535', '#bada55']\n",
    "for embeds in embeds_list:\n",
    "    tsne_embeds = TSNE(n_components=2, n_jobs=8, random_state=4036).fit_transform(embeds) \n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(tsne_embeds[:sample_size, 0], tsne_embeds[:sample_size, 1], s=20, color=colors[0], label='user')\n",
    "    plt.scatter(tsne_embeds[sample_size:, 0], tsne_embeds[sample_size:, 1], s=20, color=colors[1], label='item')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (embeds, ms, me) in enumerate(zip(embeds_list, months[:12], months[ws:ws+12])):\n",
    "    path_ = os.path.join(path, 'embedding_ws12_noCP', 'node2vec_50k_{}_{}'.format(ms.replace('-', ''), me.replace('-', '')))\n",
    "    np.save(path_, embeds)\n",
    "    \n",
    "np.save(os.path.join(path, 'embedding_ws12_noCP', 'cust_mcc_idx_map_50k'), idx_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    print('Range:', months[i], months[i+ws])\n",
    "    \n",
    "    mask = data.index[data.csmdt.between(months[i], months[i+ws])]\n",
    "    data_sw = data.loc[mask, ['chid', 'mcc', 'objam']]\n",
    "        \n",
    "    df_group = data_sw.groupby(by=['chid', 'mcc']).mean()\n",
    "\n",
    "    df_edge = pd.DataFrame(list(map(list, df_group.index)), columns=['chid', 'mcc'])\n",
    "    df_edge['value'] = df_group.objam.values\n",
    "    df_edge['log_value'] = np.log1p(df_edge.value)\n",
    "\n",
    "    with sns.axes_style(\"darkgrid\"):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 4), sharey=True)\n",
    "        fig.suptitle('Data in range({}, {})'.format(months[i], months[i+ws]))\n",
    "        \n",
    "        sns.histplot(ax=axes[0], x='value', data=df_edge, kde=True, element='step', stat='probability')\n",
    "        sns.histplot(ax=axes[1], x='log_value', data=df_edge, kde=True, element='step', stat='probability')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
