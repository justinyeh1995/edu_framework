{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ET_Rnn(torch.nn.Module):\n",
    "    def __init__(self, dense_dims, sparse_dims, hidden_dims, n_layers=1, use_chid=True, cell='GRU', bi=False, dropout=0, device='cpu'):\n",
    "        super(ET_Rnn, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.cell = cell\n",
    "        self.use_chid = use_chid\n",
    "        self.device = device\n",
    "        self.bi = bi\n",
    "        \n",
    "        self.embedding_list = nn.ModuleList([nn.Embedding(fd, ed, padding_idx=0) for fd, ed in sparse_dims])\n",
    "        \n",
    "        if use_chid: \n",
    "            rnn_in_dim = dense_dims + sum([ed for fd, ed in sparse_dims[1:]])   \n",
    "            self.out_dim = hidden_dims*(bi+1) + sparse_dims[0][1] # chid embed dim\n",
    "            self.user_layer = nn.Linear(sparse_dims[0][1], sparse_dims[0][1]) \n",
    "            \n",
    "        else:\n",
    "            rnn_in_dim = dense_dims + sum([ed for fd, ed in sparse_dims[:]])\n",
    "            self.out_dim = hidden_dims*(bi+1)\n",
    "        \n",
    "        if self.cell == 'LSTM':\n",
    "            self.rnn = nn.LSTM(rnn_in_dim, hidden_dims, n_layers, batch_first=True, bidirectional=bi, dropout=dropout)\n",
    "        elif self.cell == 'GRU':\n",
    "            self.rnn = nn.GRU(rnn_in_dim, hidden_dims, n_layers, batch_first=True, bidirectional=bi, dropout=dropout)    \n",
    "        \n",
    "        self.init_embedding()\n",
    "        \n",
    "    def init_embedding(self):\n",
    "        for embed in self.embedding_list:\n",
    "            embed.reset_parameters()\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        if self.cell == 'LSTM':\n",
    "            hidden = Variable(torch.zeros(self.n_layers*(self.bi+1), x.size(0), self.hidden_dims)).to(self.device)\n",
    "            context = Variable(torch.zeros(self.n_layers*(self.bi+1), x.size(0), self.hidden_dims)).to(self.device)\n",
    "            ret = (hidden, context)\n",
    "        elif self.cell == 'GRU':\n",
    "            hidden = Variable(torch.zeros(self.n_layers*(self.bi+1), x.size(0), self.hidden_dims)).to(self.device)\n",
    "            ret = hidden\n",
    "        \n",
    "        return ret\n",
    "            \n",
    "    def forward(self, x_dense, x_sparse):\n",
    "        if self.use_chid:\n",
    "            x = torch.cat([x_dense]+[embed(x_sparse[:, :, i+1]) for i, embed in enumerate(self.embedding_list[1:])], dim=-1)            \n",
    "        else:\n",
    "            x = torch.cat([x_dense]+[embed(x_sparse[:, :, i]) for i, embed in enumerate(self.embedding_list[:])], dim=-1)\n",
    "        \n",
    "        self.hidden = self.init_hidden(x)\n",
    "        logits, self.hidden = self.rnn(x, self.hidden)\n",
    "        \n",
    "        if self.use_chid:\n",
    "            user_embed = self.user_layer(self.embedding_list[0](x_sparse[:,0,0]))\n",
    "            last_logits = torch.cat([logits[:, -1], user_embed], dim=-1)\n",
    "        else:\n",
    "            last_logits = logits[:, -1]\n",
    "        \n",
    "        return last_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims=[1], out_dim=1):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_dims = [input_dims] + hidden_dims\n",
    "        \n",
    "        self.layers = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Linear(idim, odim), \n",
    "                nn.ReLU()\n",
    "            ) for idim, odim in zip(hidden_dims[:-1], hidden_dims[1:])\n",
    "        ])\n",
    "        \n",
    "        self.out_layer = nn.Linear(hidden_dims[-1], out_dim)\n",
    "                   \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        out = self.out_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, dense_dims, sparse_dims, hidden_dims, out_dims=[1], n_layers=1, use_chid=True, cell='GRU', bi=False, dropout=0, device='cpu'):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.rnn = ET_Rnn(dense_dims, sparse_dims, hidden_dims, n_layers=n_layers, use_chid=use_chid, \n",
    "                          cell=cell, bi=bi, dropout=dropout, device=device)\n",
    "        \n",
    "        self.mlps = nn.ModuleList([MLP(self.rnn.out_dim, hidden_dims=[self.rnn.out_dim//2], out_dim=od) for od in out_dims])\n",
    "        \n",
    "    def forward(self, x_dense, x_sparse):\n",
    "        logits = self.rnn(x_dense, x_sparse)\n",
    "        outs = [mlp(logits)for mlp in self.mlps]\n",
    "        \n",
    "        return outs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion_reg = nn.MSELoss()\n",
    "        self.criterion_clf = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        \n",
    "    def fit(self, train_loader, test_loader=None, epoch=1, early_stop=-1, scaler=None):\n",
    "        history = {\n",
    "            'train': [], \n",
    "            'test': []\n",
    "        }\n",
    "        \n",
    "        best_eval = 9e9\n",
    "        early_cnt = 0\n",
    "        best_model_params = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "        for ep in tqdm(range(epoch)):\n",
    "            #print('Epoch:{}'.format(ep+1))\n",
    "            \n",
    "            self.model.train()\n",
    "            for batch in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                x_dense, x_sparse, objmean, tscnt, spcnt, label_0 = [b.to(self.device) for b in batch]\n",
    "                \n",
    "                outputs = self.model(x_dense, x_sparse)\n",
    "                \n",
    "                loss = self.criterion_reg(outputs[0], objmean)\n",
    "                if ep >= 20:\n",
    "                    loss += self.criterion_reg(outputs[1], tscnt)\n",
    "                if ep >= 30:\n",
    "                    loss += self.criterion_reg(outputs[2], spcnt)\n",
    "                if ep >= 40:\n",
    "                    loss += self.criterion_clf(outputs[3], label_0)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "            train_result, _, _ = self.evaluate(train_loader)\n",
    "            history['train'].append(train_result)\n",
    "            #print('\\ttrain\\t'+' '.join(['{}:{:.2f}'.format(k, v) for k, v in train_result.items()]))\n",
    "            \n",
    "            if test_loader:\n",
    "                test_result, _, _ = self.evaluate(test_loader)\n",
    "                history['test'].append(test_result)\n",
    "                if ep%5 == 0 or ep == epoch-1:\n",
    "                    print('Epoch:{}'.format(ep+1))\n",
    "                    print('\\ttest\\t'+' '.join(['{}:{:.3f}'.format(k, v) for k, v in test_result.items()]))\n",
    "\n",
    "                if test_result['total_loss'] < best_eval:\n",
    "                    early_cnt = 0\n",
    "                    best_eval = test_result['total_loss']\n",
    "                    best_model_params = copy.deepcopy(self.model.state_dict())\n",
    "                    #print('\\tbetter!')\n",
    "\n",
    "                elif early_stop > 0 and ep >= 40:\n",
    "                    early_cnt += 1\n",
    "\n",
    "            if early_stop > 0 and early_cnt >= early_stop and ep >= 40:\n",
    "                break\n",
    "        \n",
    "        self.model.load_state_dict(best_model_params)\n",
    "        \n",
    "        return history\n",
    "        \n",
    "    def evaluate(self, loader):\n",
    "        true_list = [[], [], [], []]\n",
    "        pred_list = [[], [], [], []]\n",
    "        total_loss = 0\n",
    "        loss_list = [0]*4\n",
    "        \n",
    "        self.model.eval()\n",
    "        for batch in loader:\n",
    "            x_dense, x_sparse, objmean, tscnt, spcnt, label_0 = [b.to(self.device) for b in batch]\n",
    "            outputs = self.model(x_dense, x_sparse)\n",
    "            \n",
    "            for i, (y, output) in enumerate(zip([objmean, tscnt, spcnt, label_0], outputs)):\n",
    "                true_list[i].append(y.cpu().detach().numpy())\n",
    "                pred_list[i].append(output.cpu().detach().numpy())\n",
    "                \n",
    "                if i < 3:\n",
    "                    batch_loss = self.criterion_reg(output, y).cpu().detach().item() * y.shape[0]\n",
    "                else:\n",
    "                    batch_loss = self.criterion_clf(output, y).cpu().detach().item() * y.shape[0]\n",
    "                    \n",
    "                total_loss += batch_loss\n",
    "                loss_list[i] += batch_loss\n",
    "        \n",
    "        true_list[0] = objmean_scaler.inverse_transform(np.concatenate(true_list[0], axis=0).reshape(-1, 1))\n",
    "        pred_list[0] = objmean_scaler.inverse_transform(np.concatenate(pred_list[0], axis=0).reshape(-1, 1))\n",
    "        true_list[0] = np.expm1(true_list[0].flatten())\n",
    "        pred_list[0] = np.expm1(pred_list[0].flatten())\n",
    "        \n",
    "        for i in range(1, 3):\n",
    "            true_list[i] = np.expm1(np.concatenate(true_list[i], axis=0))\n",
    "            pred_list[i] = np.expm1(np.concatenate(pred_list[i], axis=0))        \n",
    "        \n",
    "        true_list[3] = np.concatenate(true_list[3], axis=0).reshape(-1, 1)\n",
    "        pred_list[3] = np.argmax(np.concatenate(pred_list[3], axis=0), axis=1).reshape(-1, 1)        \n",
    "        \n",
    "        result = {\n",
    "            'total_loss': total_loss/len(loader.dataset), \n",
    "            'objmean': mean_squared_error(true_list[0], pred_list[0], squared=False), \n",
    "            'objmean_loss': loss_list[0]/len(loader.dataset), \n",
    "            'tscnt': mean_squared_error(true_list[1], pred_list[1], squared=False), \n",
    "            'tscnt_loss': loss_list[1]/len(loader.dataset), \n",
    "            'spcnt': mean_squared_error(true_list[2], pred_list[2], squared=False),\n",
    "            'spcnt_loss': loss_list[2]/len(loader.dataset), \n",
    "            'label_0': accuracy_score(true_list[3], pred_list[3]),\n",
    "            'label_0_loss': loss_list[3]/len(loader.dataset)\n",
    "        }\n",
    "        \n",
    "        return result, true_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = './data/sample_50k'\n",
    "\n",
    "x_train = np.load(os.path.join(sample_path, 'RNN', 'x_train.npy'), allow_pickle=True)\n",
    "x_test = np.load(os.path.join(sample_path, 'RNN', 'x_test.npy'), allow_pickle=True)\n",
    "\n",
    "#f_train = np.load(os.path.join(sample_path, 'RNN', 'f_train.npy'), allow_pickle=True)\n",
    "#f_test = np.load(os.path.join(sample_path, 'RNN', 'f_test.npy'), allow_pickle=True)\n",
    "\n",
    "Y_train = np.load(os.path.join(sample_path, 'RNN', 'y_train.npy'), allow_pickle=True)\n",
    "Y_test = np.load(os.path.join(sample_path, 'RNN', 'y_test.npy'), allow_pickle=True)\n",
    "\n",
    "chid_mapper = np.load(os.path.join(sample_path, 'sample_50k_chid_idx_map.npy'), allow_pickle=True).item()\n",
    "feat_mapper = np.load(os.path.join(sample_path, 'RNN', 'feature_map.npy'), allow_pickle=True).item()\n",
    "cust_feature_map = np.load(os.path.join(sample_path, 'RNN', 'cust_feature_map.npy'), allow_pickle=True).item()\n",
    "\n",
    "columns = np.load(os.path.join(sample_path, 'RNN', 'columns.npy'), allow_pickle=True).item()\n",
    "\n",
    "print(x_train.shape, x_test.shape, Y_train.shape, Y_test.shape, len(chid_mapper))\n",
    "print([(k, len(v)) for k, v in feat_mapper.items()], [(k, len(v)) for k, v in cust_feature_map.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "category_cols = columns['x_columns'][:-4]\n",
    "numeric_cols = columns['x_columns'][-4:]\n",
    "\n",
    "print(columns['x_columns'])\n",
    "print(category_cols, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns['y_columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regession\n",
    "index = columns['y_columns'].index('objam_sum')\n",
    "train_objsum = Y_train[:, [index]].astype(np.float64)\n",
    "test_objsum = Y_test[:, [index]].astype(np.float64)\n",
    "\n",
    "index = columns['y_columns'].index('objam_mean')\n",
    "train_objmean = Y_train[:, [index]].astype(np.float64)\n",
    "test_objmean = Y_test[:, [index]].astype(np.float64)\n",
    "\n",
    "index = columns['y_columns'].index('trans_count')\n",
    "train_tscnt = Y_train[:, [index]].astype(np.float64)\n",
    "test_tscnt = Y_test[:, [index]].astype(np.float64)\n",
    "\n",
    "index = columns['y_columns'].index('shop_count')\n",
    "train_spcnt = Y_train[:, [index]].astype(np.float64)\n",
    "test_spcnt = Y_test[:, [index]].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform\n",
    "train_objsum = np.log1p(train_objsum)\n",
    "test_objsum = np.log1p(test_objsum)\n",
    "\n",
    "train_objmean = np.log1p(train_objmean)\n",
    "test_objmean = np.log1p(test_objmean)\n",
    "\n",
    "objmean_scaler = MinMaxScaler((0, 1))\n",
    "train_objmean = objmean_scaler.fit_transform(train_objmean)\n",
    "test_objmean = objmean_scaler.transform(test_objmean)\n",
    "\n",
    "train_tscnt = np.log1p(train_tscnt)\n",
    "test_tscnt = np.log1p(test_tscnt)\n",
    "\n",
    "train_spcnt = np.log1p(train_spcnt)\n",
    "test_spcnt = np.log1p(test_spcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classfication\n",
    "bounds = [0]\n",
    "lable_trans = np.vectorize(lambda x: sum([x > bound for bound in bounds]))\n",
    "\n",
    "train_label_0 = lable_trans(train_objsum)\n",
    "test_label_0 = lable_trans(test_objsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_num = [np.sum(train_label_0 == label) for label in sorted(np.unique(train_label_0))]\n",
    "test_label_num = [np.sum(test_label_0 == label) for label in sorted(np.unique(train_label_0))]\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.barplot(x=[0, 1], y=train_label_num)\n",
    "    plt.xticks(range(len(train_label_num)))\n",
    "    plt.show()\n",
    "    \n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.barplot(x=[0, 1], y=train_label_num)\n",
    "    plt.xticks(range(len(test_label_num)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feat = category_cols[:5]#+['stonc_tag', 'stonc_6_label']\n",
    "dense_feat = numeric_cols\n",
    "\n",
    "keys = list(feat_mapper.keys())\n",
    "for key in keys:\n",
    "    if key not in sparse_feat:\n",
    "        del feat_mapper[key]\n",
    "\n",
    "print(sparse_feat, [(k, len(v)) for k, v in feat_mapper.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CHID = True\n",
    "idx_start = 1-int(USE_CHID)\n",
    "sparse_index = [category_cols.index(feat) for feat in sparse_feat][idx_start:]\n",
    "\n",
    "chid_embed_dim = 64\n",
    "feat_embed_dim = 16\n",
    "\n",
    "dense_dims = len(dense_feat) # number of dense feature\n",
    "feat_dims = np.array([len(chid_mapper)] + [len(v) for v in feat_mapper.values()])+1 # 0 is padding index, so add 1 dims\n",
    "embed_dims = [chid_embed_dim]+[feat_embed_dim]*len(feat_mapper) # dims of chid and other sparse feature\n",
    "\n",
    "sparse_dims = [(fd, ed) for fd, ed in zip(feat_dims[idx_start:], embed_dims[idx_start:])]\n",
    "\n",
    "dense_dims, sparse_dims, sparse_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data\n",
    "w_size = x_train.shape[1]\n",
    "\n",
    "x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train_dense = x_train[:, -w_size:, len(category_cols):].astype(np.float64) # split dense feature\n",
    "x_train_sparse = x_train[:, -w_size:, sparse_index].astype(np.int64) # split sparse feature\n",
    "\n",
    "x_train_dense = np.log1p(x_train_dense - x_train_dense.min(axis=0))\n",
    "x_train_dense = x_scaler.fit_transform(x_train_dense.reshape(-1, x_train_dense.shape[-1])).reshape(x_train_dense.shape)\n",
    "\n",
    "x_test_dense = x_test[:, -w_size:, len(category_cols):].astype(np.float64)\n",
    "x_test_sparse = x_test[:, -w_size:, sparse_index].astype(np.int64) \n",
    "\n",
    "x_test_dense = np.log1p(x_test_dense - x_test_dense.min(axis=0))\n",
    "x_test_dense = x_scaler.transform(x_test_dense.reshape(-1, x_test_dense.shape[-1])).reshape(x_test_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_dense.shape, x_train_sparse.shape)\n",
    "print(train_objsum.shape, train_objmean.shape, train_tscnt.shape, train_spcnt.shape, train_label_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test_dense.shape, x_test_sparse.shape)\n",
    "print(test_objsum.shape, test_objmean.shape, test_tscnt.shape, test_spcnt.shape, test_label_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8192\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(x_train_dense), torch.LongTensor(x_train_sparse), \n",
    "                              torch.FloatTensor(train_objmean), torch.FloatTensor(train_tscnt), \n",
    "                              torch.FloatTensor(train_spcnt), torch.LongTensor(train_label_0.flatten()))\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "test_dataset = TensorDataset(torch.FloatTensor(x_test_dense), torch.LongTensor(x_test_sparse), \n",
    "                              torch.FloatTensor(test_objmean), torch.FloatTensor(test_tscnt), \n",
    "                              torch.FloatTensor(test_spcnt), torch.LongTensor(test_label_0.flatten()))\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "out_dims = [1, 1, 1, 2]\n",
    "\n",
    "model = MultiTaskModel(dense_dims, sparse_dims, hidden_dims=64, out_dims=out_dims, n_layers=2, \n",
    "                       use_chid=USE_CHID, cell='GRU', bi=False, dropout=0.1, device=device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "\n",
    "print(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "trainer = Trainer(model, optimizer, device)\n",
    "history = trainer.fit(train_loader, test_loader, epoch=100, early_stop=20)\n",
    "t1 = time()\n",
    "\n",
    "print('cost: {:.2f}'.format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = pd.DataFrame(history['test'])\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    show_cols = ['tscnt_loss', 'spcnt_loss', 'label_0_loss']\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(data=pd.DataFrame(df_history[show_cols]))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    show_cols = ['objmean_loss']\n",
    "    sns.lineplot(data=pd.DataFrame(df_history[show_cols]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):   \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    show_cols = ['tscnt']\n",
    "    sns.lineplot(data=pd.DataFrame(df_history[show_cols]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):   \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    show_cols = ['spcnt']\n",
    "    sns.lineplot(data=pd.DataFrame(df_history[show_cols]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):   \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    show_cols = ['label_0']\n",
    "    sns.lineplot(data=pd.DataFrame(df_history[show_cols]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_result, train_true_list, train_pred_list = trainer.evaluate(train_loader)\n",
    "test_result, test_true_list, test_pred_list = trainer.evaluate(test_loader)\n",
    "\n",
    "print('train\\t'+' '.join(['{}:{:.2f}'.format(k, v) for k, v in train_result.items()]))\n",
    "print('test\\t'+' '.join(['{}:{:.2f}'.format(k, v) for k, v in test_result.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(test_true_list[-1].reshape(-1, 1), test_pred_list[-1].reshape(-1, 1), normalize='true')\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cf_matrix, linewidths=.01, annot=True)\n",
    "    ax.set_xlabel('Predict', fontsize=16)\n",
    "    ax.set_ylabel('True', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'AccuracyScore': accuracy_score(test_true_list[-1], test_pred_list[-1]), \n",
    "    'RecallScore': recall_score(test_true_list[-1], test_pred_list[-1], average='macro'),\n",
    "    'PrecisionScore': precision_score(test_true_list[-1], test_pred_list[-1], average='macro'),\n",
    "    'F1Score': f1_score(test_true_list[-1], test_pred_list[-1], average='macro'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'dense_dims': dense_dims,\n",
    "    'sparse_dims': sparse_dims,\n",
    "    'hidden_dims': 64,\n",
    "    'n_layers': 2,\n",
    "    'use_chid': True,\n",
    "    'cell': 'GRU',\n",
    "    'bi': False,\n",
    "    'dropout': 0.1,\n",
    "    'model_state_dict': model.rnn.cpu().state_dict()\n",
    "}, './models/rnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./models/rnn.pt')\n",
    "checkpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
