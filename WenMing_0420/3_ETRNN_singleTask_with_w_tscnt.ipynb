{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ET_Rnn(torch.nn.Module):\n",
    "    def __init__(self, dense_dims, sparse_dims, hidden_dims, n_layers=1, use_chid=True, cell='GRU', bi=False, dropout=0, device='cpu'):\n",
    "        super(ET_Rnn, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.cell = cell\n",
    "        self.use_chid = use_chid\n",
    "        self.device = device\n",
    "        self.bi = bi\n",
    "        \n",
    "        self.embedding_list = nn.ModuleList([nn.Embedding(fd, ed, padding_idx=0) for fd, ed in sparse_dims])\n",
    "        \n",
    "        if use_chid: \n",
    "            rnn_in_dim = dense_dims + sum([ed for fd, ed in sparse_dims[1:]])   \n",
    "            self.out_dim = hidden_dims*(bi+1) + sparse_dims[0][1] # chid embed dim\n",
    "            self.user_layer = nn.Linear(sparse_dims[0][1], sparse_dims[0][1]) \n",
    "            \n",
    "        else:\n",
    "            rnn_in_dim = dense_dims + sum([ed for fd, ed in sparse_dims[:]])\n",
    "            self.out_dim = hidden_dims*(bi+1)\n",
    "        \n",
    "        if self.cell == 'LSTM':\n",
    "            self.rnn = nn.LSTM(rnn_in_dim, hidden_dims, n_layers, batch_first=True, bidirectional=bi, dropout=dropout)\n",
    "        elif self.cell == 'GRU':\n",
    "            self.rnn = nn.GRU(rnn_in_dim, hidden_dims, n_layers, batch_first=True, bidirectional=bi, dropout=dropout)    \n",
    "        \n",
    "        self.init_embedding()\n",
    "        \n",
    "    def init_embedding(self):\n",
    "        for embed in self.embedding_list:\n",
    "            embed.reset_parameters()\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        if self.cell == 'LSTM':\n",
    "            hidden = Variable(torch.zeros(self.n_layers*(self.bi+1), x.size(0), self.hidden_dims)).to(self.device)\n",
    "            context = Variable(torch.zeros(self.n_layers*(self.bi+1), x.size(0), self.hidden_dims)).to(self.device)\n",
    "            ret = (hidden, context)\n",
    "        elif self.cell == 'GRU':\n",
    "            hidden = Variable(torch.zeros(self.n_layers*(self.bi+1), x.size(0), self.hidden_dims)).to(self.device)\n",
    "            ret = hidden\n",
    "        \n",
    "        return ret\n",
    "            \n",
    "    def forward(self, x_dense, x_sparse):\n",
    "        if self.use_chid:\n",
    "            x = torch.cat([x_dense]+[embed(x_sparse[:, :, i+1]) for i, embed in enumerate(self.embedding_list[1:])], dim=-1)            \n",
    "        else:\n",
    "            x = torch.cat([x_dense]+[embed(x_sparse[:, :, i]) for i, embed in enumerate(self.embedding_list[:])], dim=-1)\n",
    "        \n",
    "        self.hidden = self.init_hidden(x)\n",
    "        logits, self.hidden = self.rnn(x, self.hidden)\n",
    "        \n",
    "        if self.use_chid:\n",
    "            user_embed = self.user_layer(self.embedding_list[0](x_sparse[:,0,0]))\n",
    "            last_logits = torch.cat([logits[:, -1], user_embed], dim=-1)\n",
    "        else:\n",
    "            last_logits = logits[:, -1]\n",
    "        \n",
    "        return last_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims=[1], out_dim=1):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_dims = [input_dims] + hidden_dims\n",
    "        \n",
    "        self.layers = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Linear(idim, odim), \n",
    "                nn.ReLU()\n",
    "            ) for idim, odim in zip(hidden_dims[:-1], hidden_dims[1:])\n",
    "        ])\n",
    "        \n",
    "        self.out_layer = nn.Linear(hidden_dims[-1], out_dim)\n",
    "                   \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        out = self.out_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTaskModel(nn.Module):\n",
    "    def __init__(self, dense_dims, sparse_dims, hidden_dims, out_dim=1, n_layers=1, use_chid=True, cell='GRU', bi=False, dropout=0, device='cpu'):\n",
    "        super(SingleTaskModel, self).__init__()\n",
    "        self.rnn = ET_Rnn(dense_dims, sparse_dims, hidden_dims, n_layers=n_layers, use_chid=use_chid, \n",
    "                          cell=cell, bi=bi, dropout=dropout, device=device)\n",
    "        \n",
    "        self.mlp = MLP(self.rnn.out_dim, hidden_dims=[self.rnn.out_dim//2], out_dim=out_dim)\n",
    "        \n",
    "    def forward(self, x_dense, x_sparse):\n",
    "        logits = self.rnn(x_dense, x_sparse)\n",
    "        out = self.mlp(logits)\n",
    "        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        \n",
    "    def fit(self, train_loader, test_loader=None, epoch=1, early_stop=-1, eval_type='RMSE', scaler=None):\n",
    "        history = {\n",
    "            'train': [], \n",
    "            'test': []\n",
    "        }\n",
    "        \n",
    "        best_eval = 9e9\n",
    "        early_cnt = 0\n",
    "        best_model_params = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "        for ep in tqdm(range(epoch)):\n",
    "            #print('Epoch:{}'.format(ep+1))\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                x_dense, x_sparse, y = [b.to(self.device) for b in batch]\n",
    "                \n",
    "                output = self.model(x_dense, x_sparse)\n",
    "                loss = self.criterion(output, y)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "            train_result, _, _ = self.evaluate(train_loader, scaler)\n",
    "            history['train'].append(train_result)\n",
    "            #print('\\ttrain\\t'+' '.join(['{}:{:.2f}'.format(k, v) for k, v in train_result.items()]))\n",
    "            \n",
    "            if test_loader:\n",
    "                test_result, _, _ = self.evaluate(test_loader, scaler)\n",
    "                history['test'].append(test_result)\n",
    "                \n",
    "                if ep%5 == 0 or ep == epoch-1:\n",
    "                    print('Epoch:{}'.format(ep+1))\n",
    "                    print('\\ttest\\t'+' '.join(['{}:{:.3f}'.format(k, v) for k, v in test_result.items()]))                \n",
    "\n",
    "                if test_result[eval_type] < best_eval:\n",
    "                    early_cnt = 0\n",
    "                    best_eval = test_result[eval_type]\n",
    "                    best_model_params = copy.deepcopy(self.model.state_dict())\n",
    "                    #print('\\tbetter!')\n",
    "\n",
    "                elif early_stop > 0:\n",
    "                    early_cnt += 1\n",
    "\n",
    "            if early_stop > 0 and early_cnt >= early_stop:\n",
    "                break\n",
    "        \n",
    "        self.model.load_state_dict(best_model_params)\n",
    "        \n",
    "        return history\n",
    "        \n",
    "    def evaluate(self, loader, scaler=None):\n",
    "        true_list = []\n",
    "        pred_list = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        for batch in loader:\n",
    "            x_dense, x_sparse, y = [b.to(self.device) for b in batch]\n",
    "            output = self.model(x_dense, x_sparse)\n",
    "            \n",
    "            true_list.append(y.cpu().detach().numpy())\n",
    "            pred_list.append(output.cpu().detach().numpy())\n",
    "        \n",
    "        true_list = np.concatenate(true_list, axis=0)\n",
    "        pred_list = np.concatenate(pred_list, axis=0)\n",
    "        \n",
    "        true_list = np.expm1(scaler.inverse_transform(true_list) if scaler else true_list)\n",
    "        pred_list = np.expm1(scaler.inverse_transform(pred_list) if scaler else pred_list)\n",
    "        \n",
    "        result = {\n",
    "            'RMSE': mean_squared_error(true_list, pred_list, squared=False), \n",
    "            'MAE(mean)': mean_absolute_error(true_list, pred_list), \n",
    "            'MAE(median)': median_absolute_error(true_list, pred_list)\n",
    "        }\n",
    "        \n",
    "        return result, true_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1033871, 120, 14) (100000, 120, 14) (1033871, 7) (100000, 7) 50000\n",
      "[('bnsfg', 2), ('iterm', 15), ('mcc', 507), ('scity', 11074), ('stonc_tag', 49), ('stonc_label', 202387), ('stonm_label', 212342), ('stonc_6_label', 78560), ('stonc_10_label', 128075)] [('masts', 3), ('educd', 6), ('trdtp', 27), ('poscd', 9)]\n"
     ]
    }
   ],
   "source": [
    "sample_path = './data/sample_50k'\n",
    "\n",
    "x_train = np.load(os.path.join(sample_path, 'RNN', 'x_train.npy'), allow_pickle=True)\n",
    "x_test = np.load(os.path.join(sample_path, 'RNN', 'x_test.npy'), allow_pickle=True)\n",
    "\n",
    "#f_train = np.load(os.path.join(sample_path, 'RNN', 'f_train.npy'), allow_pickle=True)\n",
    "#f_test = np.load(os.path.join(sample_path, 'RNN', 'f_test.npy'), allow_pickle=True)\n",
    "\n",
    "Y_train = np.load(os.path.join(sample_path, 'RNN', 'y_train.npy'), allow_pickle=True)\n",
    "Y_test = np.load(os.path.join(sample_path, 'RNN', 'y_test.npy'), allow_pickle=True)\n",
    "\n",
    "chid_mapper = np.load(os.path.join(sample_path, 'sample_50k_chid_idx_map.npy'), allow_pickle=True).item()\n",
    "feat_mapper = np.load(os.path.join(sample_path, 'RNN', 'feature_map.npy'), allow_pickle=True).item()\n",
    "cust_feature_map = np.load(os.path.join(sample_path, 'RNN', 'cust_feature_map.npy'), allow_pickle=True).item()\n",
    "\n",
    "columns = np.load(os.path.join(sample_path, 'RNN', 'columns.npy'), allow_pickle=True).item()\n",
    "\n",
    "print(x_train.shape, x_test.shape, Y_train.shape, Y_test.shape, len(chid_mapper))\n",
    "print([(k, len(v)) for k, v in feat_mapper.items()], [(k, len(v)) for k, v in cust_feature_map.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chid', 'bnsfg', 'iterm', 'mcc', 'scity', 'stonc_tag', 'stonc_label', 'stonm_label', 'stonc_6_label', 'stonc_10_label', 'bnspt', 'timestamp_0', 'timestamp_1', 'objam']\n",
      "['chid', 'bnsfg', 'iterm', 'mcc', 'scity', 'stonc_tag', 'stonc_label', 'stonm_label', 'stonc_6_label', 'stonc_10_label'] ['bnspt', 'timestamp_0', 'timestamp_1', 'objam']\n"
     ]
    }
   ],
   "source": [
    "category_cols = columns['x_columns'][:-4]\n",
    "numeric_cols = columns['x_columns'][-4:]\n",
    "\n",
    "print(columns['x_columns'])\n",
    "print(category_cols, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chid', 'data_dt', 'objam_sum', 'objam_mean', 'trans_count', 'shop_count', 'objam_mean_M3_diff']\n"
     ]
    }
   ],
   "source": [
    "print(columns['y_columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regession\n",
    "index = columns['y_columns'].index('trans_count')\n",
    "train_tscnt = Y_train[:, [index]].astype(np.float64)\n",
    "test_tscnt = Y_test[:, [index]].astype(np.float64)\n",
    "\n",
    "# log transform\n",
    "train_tscnt = np.log1p(train_tscnt)\n",
    "test_tscnt = np.log1p(test_tscnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chid', 'bnsfg', 'iterm', 'mcc', 'scity'] [('bnsfg', 2), ('iterm', 15), ('mcc', 507), ('scity', 11074)]\n"
     ]
    }
   ],
   "source": [
    "sparse_feat = category_cols[:5]#+['stonc_tag', 'stonc_6_label']\n",
    "dense_feat = numeric_cols\n",
    "\n",
    "keys = list(feat_mapper.keys())\n",
    "for key in keys:\n",
    "    if key not in sparse_feat:\n",
    "        del feat_mapper[key]\n",
    "\n",
    "print(sparse_feat, [(k, len(v)) for k, v in feat_mapper.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, [(50001, 64), (3, 16), (16, 16), (508, 16), (11075, 16)], [0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CHID = True\n",
    "idx_start = 1-int(USE_CHID)\n",
    "sparse_index = [category_cols.index(feat) for feat in sparse_feat][idx_start:]\n",
    "\n",
    "chid_embed_dim = 64\n",
    "feat_embed_dim = 16\n",
    "\n",
    "dense_dims = len(dense_feat) # number of dense feature\n",
    "feat_dims = np.array([len(chid_mapper)] + [len(v) for v in feat_mapper.values()])+1 # 0 is padding index, so add 1 dims\n",
    "embed_dims = [chid_embed_dim]+[feat_embed_dim]*len(feat_mapper) # dims of chid and other sparse feature\n",
    "\n",
    "sparse_dims = [(fd, ed) for fd, ed in zip(feat_dims[idx_start:], embed_dims[idx_start:])]\n",
    "\n",
    "dense_dims, sparse_dims, sparse_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data\n",
    "w_size = x_train.shape[1]\n",
    "\n",
    "x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train_dense = x_train[:, -w_size:, len(category_cols):].astype(np.float64) # split dense feature\n",
    "x_train_sparse = x_train[:, -w_size:, sparse_index].astype(np.int64) # split sparse feature\n",
    "\n",
    "x_train_dense = np.log1p(x_train_dense - x_train_dense.min(axis=0))\n",
    "x_train_dense = x_scaler.fit_transform(x_train_dense.reshape(-1, x_train_dense.shape[-1])).reshape(x_train_dense.shape)\n",
    "\n",
    "x_test_dense = x_test[:, -w_size:, len(category_cols):].astype(np.float64)\n",
    "x_test_sparse = x_test[:, -w_size:, sparse_index].astype(np.int64) \n",
    "\n",
    "x_test_dense = np.log1p(x_test_dense - x_test_dense.min(axis=0))\n",
    "x_test_dense = x_scaler.transform(x_test_dense.reshape(-1, x_test_dense.shape[-1])).reshape(x_test_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1033871, 120, 4) (1033871, 120, 5)\n",
      "(100000, 120, 4) (100000, 120, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_dense.shape, x_train_sparse.shape)\n",
    "print(x_test_dense.shape, x_test_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(x_train_dense), torch.LongTensor(x_train_sparse), \n",
    "                              torch.FloatTensor(train_tscnt))\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "test_dataset = TensorDataset(torch.FloatTensor(x_test_dense), torch.LongTensor(x_test_sparse), \n",
    "                              torch.FloatTensor(test_tscnt))\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_dims': 4,\n",
       " 'sparse_dims': [(50001, 64), (3, 16), (16, 16), (508, 16), (11075, 16)],\n",
       " 'hidden_dims': 64,\n",
       " 'n_layers': 2,\n",
       " 'use_chid': True,\n",
       " 'cell': 'GRU',\n",
       " 'bi': False,\n",
       " 'dropout': 0.1,\n",
       " 'out_dim': 1,\n",
       " 'device': device(type='cuda', index=1)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('./models/rnn_w_tscnt.pt')\n",
    "params = checkpoint.copy()\n",
    "del params['model_state_dict']\n",
    "\n",
    "params['out_dim'] = 1\n",
    "params['device'] = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, optimizers, trainers, historys = [None]*3, [None]*3, [None]*3, [None]*3\n",
    "\n",
    "# 從頭訓練\n",
    "models[0] = SingleTaskModel(**params)\n",
    "optimizers[0] = torch.optim.AdamW(models[0].parameters(), lr=2e-3)\n",
    "\n",
    "# Freeze RNN MODEL，train MLP\n",
    "models[1] = SingleTaskModel(**params)\n",
    "models[1].rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizers[1] = torch.optim.AdamW(models[1].mlp.parameters(), lr=2e-3)\n",
    "\n",
    "# FineTune RNN MODEL，train MLP\n",
    "models[2] = SingleTaskModel(**params)\n",
    "models[2].rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizers[2] = torch.optim.AdamW(models[2].parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TYPE = [('RMSE', nn.MSELoss()), \n",
    "              ('MAE(mean)', nn.L1Loss())]\n",
    "eval_type, criterion = TRAIN_TYPE[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eb6fbce4694952a8a3aa28cab399a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "\ttest\tRMSE:14.783 MAE(mean):2.602 MAE(median):1.037\n",
      "Epoch:6\n",
      "\ttest\tRMSE:13.048 MAE(mean):2.640 MAE(median):1.003\n",
      "Epoch:11\n",
      "\ttest\tRMSE:12.584 MAE(mean):2.609 MAE(median):1.058\n",
      "Epoch:16\n",
      "\ttest\tRMSE:11.218 MAE(mean):2.647 MAE(median):1.036\n",
      "Epoch:21\n",
      "\ttest\tRMSE:10.065 MAE(mean):2.668 MAE(median):1.027\n",
      "Epoch:26\n",
      "\ttest\tRMSE:10.202 MAE(mean):2.691 MAE(median):1.071\n",
      "Epoch:31\n",
      "\ttest\tRMSE:10.665 MAE(mean):2.707 MAE(median):1.093\n",
      "Epoch:36\n",
      "\ttest\tRMSE:11.821 MAE(mean):2.760 MAE(median):1.056\n",
      "Epoch:41\n",
      "\ttest\tRMSE:12.037 MAE(mean):2.778 MAE(median):1.101\n",
      "Epoch:46\n",
      "\ttest\tRMSE:11.277 MAE(mean):2.764 MAE(median):1.062\n",
      "\n",
      "cost: 5287.30\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 3):\n",
    "    t0 = time()\n",
    "    \n",
    "    trainers[i] = Trainer(models[i], criterion, optimizers[i], params['device'])\n",
    "    historys[i] = trainers[i].fit(train_loader, test_loader, epoch=200, early_stop=20, eval_type=eval_type)\n",
    "    \n",
    "    t1 = time()\n",
    "\n",
    "    print('cost: {:.2f}'.format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 2\n",
      "train\tRMSE:4.35 MAE(mean):1.93 MAE(median):0.90\n",
      "test\tRMSE:9.04 MAE(mean):2.71 MAE(median):1.06 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result, train_true_list, train_pred_list = [None]*3, [None]*3, [None]*3\n",
    "test_result, test_true_list, test_pred_list = [None]*3, [None]*3, [None]*3\n",
    "\n",
    "for i in range(2,3):\n",
    "    print('model:', i)\n",
    "    \n",
    "    train_result[i], train_true_list[i], train_pred_list[i] = trainers[i].evaluate(train_loader)\n",
    "    test_result[i], test_true_list[i], test_pred_list[i] = trainers[i].evaluate(test_loader)    \n",
    "    \n",
    "    print('train\\t'+' '.join(['{}:{:.2f}'.format(k, v) for k, v in train_result[i].items()]))\n",
    "    print('test\\t'+' '.join(['{}:{:.2f}'.format(k, v) for k, v in test_result[i].items()]), '\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = [None]*1\n",
    "#df_history[0] = pd.DataFrame(historys[0]['test'])\n",
    "#df_history[1] = pd.DataFrame(historys[1]['test'])\n",
    "#df_history[2] = pd.DataFrame(historys[2]['test'])\n",
    "\n",
    "df_history[0] = pd.DataFrame(historys[2]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse = pd.DataFrame(data=np.vstack([df['RMSE'].values for df in df_history]).T, \n",
    "                       columns=[#'Re-training with Single task', \n",
    "                                #'Representation Transfer', \n",
    "                                'network Fine Tuning'])\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):   \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    axes = sns.lineplot(data=df_rmse)\n",
    "    axes.set_title('Learning Curve - RMSE', fontsize=16)\n",
    "    axes.set_xlabel('Epoch', fontsize=16)\n",
    "    axes.set_ylabel('Value', fontsize=16)\n",
    "    plt.legend(fontsize='12', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mae_mean = pd.DataFrame(data=np.vstack([df['MAE(mean)'].values for df in df_history]).T, \n",
    "                           columns=[#'Re-training with Single task', \n",
    "                                    #'Representation Transfer', \n",
    "                                    'network Fine Tuning'])\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):   \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    axes = sns.lineplot(data=df_mae_mean)\n",
    "    axes.set_title('Learning Curve - MAE(Mean)', fontsize=16)\n",
    "    axes.set_xlabel('Epoch', fontsize=16)\n",
    "    axes.set_ylabel('Value', fontsize=16)\n",
    "    plt.legend(fontsize='12', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mae_median = pd.DataFrame(data=np.vstack([df['MAE(median)'].values for df in df_history]).T, \n",
    "                             columns=[#'Re-training with Single task', \n",
    "                                      #'Representation Transfer', \n",
    "                                      'network Fine Tuning'])\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):   \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    axes = sns.lineplot(data=df_mae_median)\n",
    "    axes.set_title('Learning Curve - MAE(Median)', fontsize=16)\n",
    "    axes.set_xlabel('Epoch', fontsize=16)\n",
    "    axes.set_ylabel('Value', fontsize=16)\n",
    "    plt.legend(fontsize='12', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save({\n",
    "#    'dense_dims': dense_dims,\n",
    "#    'sparse_dims': sparse_dims,\n",
    "#    'hidden_dims': 64,\n",
    "#    'n_layers': 2,\n",
    "#    'use_chid': True,\n",
    "#    'cell': 'GRU',\n",
    "#    'bi': False,\n",
    "#    'dropout': 0.1,\n",
    "#    'out_dim': 1,\n",
    "#    'model_state_dict': models[0].to('cpu').state_dict()\n",
    "#}, './models/objamsum_retrain.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save({\n",
    "#    'dense_dims': dense_dims,\n",
    "#    'sparse_dims': sparse_dims,\n",
    "#    'hidden_dims': 64,\n",
    "#    'n_layers': 2,\n",
    "#    'use_chid': True,\n",
    "#    'cell': 'GRU',\n",
    "#    'bi': False,\n",
    "#    'dropout': 0.1,\n",
    "#    'out_dim': 1,\n",
    "#    'model_state_dict': models[1].to('cpu').state_dict()\n",
    "#}, './models/objamsum_freeze.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save({\n",
    "#    'dense_dims': dense_dims,\n",
    "#    'sparse_dims': sparse_dims,\n",
    "#    'hidden_dims': 64,\n",
    "#    'n_layers': 2,\n",
    "#    'use_chid': True,\n",
    "#    'cell': 'GRU',\n",
    "#    'bi': False,\n",
    "#    'dropout': 0.1,\n",
    "#    'out_dim': 1,\n",
    "#    'model_state_dict': models[2].to('cpu').state_dict()\n",
    "#}, './models/objamsum_finetune.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=[pd.DataFrame(data = np.hstack([test_true_list[i], test_pred_list[i]]), columns=['true', 'pred']) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    tmp = df_test[i]\n",
    "    tmp_asc = tmp.sort_values(by='true', ascending=True)[:10000]\n",
    "    tmp_dsc = tmp.sort_values(by='true', ascending=False)[:10000]\n",
    "    print(mean_squared_error(tmp_asc.true.values, tmp_asc.pred.values, squared=False), '\\t',\n",
    "          mean_squared_error(tmp_dsc.true.values, tmp_dsc.pred.values, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Re-training with Single task', 'Representation Transfer', 'network Fine Tuning']\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    for i in range(3):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        axes = sns.scatterplot(y=np.log1p(test_true_list[i]).flatten(), x=np.log1p(test_pred_list[i]).flatten())\n",
    "        axes.set_xlabel('Pred(log value)', fontsize=16)\n",
    "        axes.set_ylabel('True(log value)', fontsize=16)\n",
    "        axes.set_title('scatter plot of '+titles[i], fontsize=16)\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
